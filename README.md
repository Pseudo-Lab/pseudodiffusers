# Welcome to PseudoDiffusers!!

This is the repository of PseudoDiffusers team.

:bulb: Our aim is to review papers and code related to computer vision generation models, approach them theoretically, and conduct various experiments by fine-tuning diffusion based models.

[About Us - PseudoLab](https://www.linkedin.com/company/pseudolab/)

[About Us - PseudoDiffusers](https://chanrankim.notion.site/PseudoDiffusers-b666d39ea1924b4692796e442bebcd44)

참여 방법: 매주 수요일 오후 9시, 가짜연구소 Discord Room-DH 로 입장!

## Publications
**DiffInject: Revisiting Debias via Synthetic Data Generation using Diffusion-based Style Injection**  
Donggeun Ko*, Sangwoo Jo*, Dongjun Lee, Namjun Park, Jaekwang KIM  
CVPR 2024 Workshop  
[PDF](https://openreview.net/pdf?id=jSB5wlUU3p)

## Contributors 
- 조상우 [Sangwoo Jo] | [Github](https://github.com/jasonjo97) | [Linkedin](https://www.linkedin.com/in/sangwoojo/) | 
- 문광수 [Kwangsu Mun] | [Github](https://github.com/mksoo) | [Linkedin](https://www.linkedin.com/in/%EA%B4%91%EC%88%98-%EB%AC%B8-95681b229/) |
- 김지수 [Jisu Kim] | Github |  [Linkedin](https://www.linkedin.com/in/%EC%A7%80%EC%88%98-%EA%B9%80-5a0b2320a/) |
- 박범수 [Beomsoo Park] | [Github](https://github.com/hanlyang0522) | Linkedin |
- 지승환 [Seunghwan Ji] | [Github](https://github.com/hwansnaa) | [Linkedin](https://www.linkedin.com/in/%EC%8A%B9%ED%99%98-%EC%A7%80-0169b425a/) | 
- 고동근 [Donggeun Sean Ko] | [Github](https://github.com/seanko29) | [Linkedin](https://www.linkedin.com/in/sangwoojo/) | 
- 조남경 [Namkyeong Cho] | Github | Linkedin |
- 김선훈 [SeonHoon Kim] | [Github](https://github.com/egshkim) | [Linkedin](https://www.linkedin.com/in/seonhoonkim/) | 
- 이준형 [Junhyoung Lee] | [Github](https://github.com/jjuun0) | [Linkedin](https://www.linkedin.com/in/jjuun0) | 
- 조형서 [Hyoungseo Cho] | [Github](https://github.com/ChoHyoungSeo) | [Linkedin](https://www.linkedin.com/in/hyoungseo-cho/) |
- 유정화 [Jeonghwa Yoo] | [Github](https://github.com/jeongHwarr) | [Linkedin](https://www.linkedin.com/in/jeonghwa-yoo-8403a716b/) |
- 박세환 [Sehwan Park] | [Github](https://github.com/shp216) | Linkedin |
- 송건학 [Geonhak Song] | [Github](https://github.com/geonhak904) | Linkedin |
- 한동현 [Donghyun Han] | [GitHub](https://github.com/donghyun99) | [Linkedin](https://www.linkedin.com/in/donghyun99/) |
- 이창환 [ChangHwan Lee] | [Github](https://github.com/Hwan-I) | Linkedin |
- 유경민 [Kyeongmin Yu] | [Github](https://github.com/yukyeongmin) | Linkdedin |
- 이정인 [Jeongin Lee] | Github | Linkedin |
- 김현수 [Hyunsoo Kim] | [Github](https://github.com/gustn9609) | [Linkedin](https://www.linkedin.com/in/%ED%98%84%EC%88%98-%EA%B9%80-b28a67202/) |

## Reviewed Papers 
| idx | Date | Presenter | Paper / Code | 
| :--: | :--: | :--: | :--: |
| 1 | 2023.03.29 | Sangwoo Jo | [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (ICLR 2014) <br> [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (NIPS 2014)| 
| 2 | 2023.04.05 | Kwangsu Mun <br> Jisu Kim | [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593) (ICCV 2017) <br> [A Style-Based Generator Architecture for Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) (CVPR 2019)| 
| 3 | 2023.04.12 | Beomsoo Park <br> Seunghwan Ji | [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (NeurIPS 2020) <br> [Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502) (ICLR 2021)|
| 4 | 2023.05.10 | Donggeun Sean Ko | [Diffusion Models Beat GANs in Image Synthesis](https://arxiv.org/abs/2105.05233) (NeurIPS 2021) <br> [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092) (ICML 2021) |
| 5 | 2023.05.17 | Namkyeong Cho <br> Sangwoo Jo | [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (CVPR 2022) <br> [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) (CVPR 2023)|
| 6 | 2023.05.24 | Kwangsu Mun <br> Jisu Kim | [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618) <br> [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)|
| 7 | 2023.05.31 | Beomsoo Park <br> Seunghwan Ji | [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) <br> [Multi-Concept Customization of Text-to-Image Diffusion](https://arxiv.org/abs/2212.04488) (CVPR 2023) |
| 8 | 2023.08.30 | Donggeun Sean Ko  <br> Sangwoo Jo | [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487) <br> [Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting](https://arxiv.org/abs/2212.06909) (CVPR 2023) |
| 9 | 2023.09.06 | SeonHoon Kim <br> Seunghwan Ji | [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125) <br> [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](https://arxiv.org/abs/2108.01073) (ICLR 2022) |
| 10 | 2023.09.13 | Namkyeong Cho <br> Junhyoung Lee | [DeepFloyd IF](https://www.deepfloyd.ai/deepfloyd-if) <br> [SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952) |
| 11 | 2023.09.20 | HyoungSeo Cho <br> Sangwoo Jo | [HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models](https://arxiv.org/abs/2307.06949) <br> [T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453) |
| 12 | 2023.09.27 | Sehwan Park <br> Junhyoung Lee | [GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741) (ICML 2022) <br> [Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning](https://arxiv.org/abs/2309.02591) |
| 13 | 2023.10.11 | Jeonghwa Yoo <br> SeonHoon Kim | [Synthetic Data from Diffusion Models Improves ImageNet Classification](https://arxiv.org/abs/2304.08466) <br> [Your Diffusion Model is Secretly a Zero-Shot Classifier](https://arxiv.org/abs/2303.16203) (ICCV 2023) |
| 14 | 2023.10.18 | Seunghwan Ji | [A Study on the Evaluation of Generative Models](https://arxiv.org/abs/2206.10935) |
| 15 | 2023.10.25 | Sangwoo Jo <br> HyoungSeo Cho | [Progressive Distillation for Fast Sampling of Diffusion Models](https://arxiv.org/abs/2202.00512) (ICLR 2022) <br> [ConceptLab: Creative Generation using Diffusion Prior Constraints](https://arxiv.org/abs/2308.02669) |
| 16 | 2023.11.01 | SeonHoon Kim <br> Jeonghwa Yoo | [BBDM: Image-to-image Translation with Brownian Bridge Diffusion Models](https://arxiv.org/abs/2205.07680) (CVPR 2023) <br>  [Make-A-Video: Text-to-Video Generation without Text-Video Data](https://arxiv.org/abs/2209.14792) |
| 17 | 2023.11.15 | Sehwan Park <br> Junhyoung Lee | [Diffusion Models already have a Semantic Latent Space](https://arxiv.org/abs/2210.10960) (ICLR 2023) <br>  [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2304.08818) (CVPR 2023) |
| 18 | 2023.11.29 | Donggeun Sean Ko | [Video Diffusion Models](https://arxiv.org/abs/2204.03458) |
| 19 | 2024.03.13 | Geonhak Song | [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/abs/2311.17117) <br> [DreaMoving: A Human Video Generation Framework based on Diffusion Models](https://arxiv.org/abs/2312.05107) |
| 20 | 2024.03.20 | Junhyoung Lee | [Muse: Text-To-Image Generation via Masked Generative Transformers](https://arxiv.org/abs/2301.00704) (ICML 2023) |
| 21 | 2024.03.27 | Seunghwan Ji | [Scaling up GANs for Text-to-Image Synthesis](https://arxiv.org/abs/2303.05511) (CVPR 2023) |
| 22 | 2024.04.03 | Sangwoo Jo | [Consistency Models](https://arxiv.org/abs/2303.01469) (ICML 2023) |
| 23 | 2024.04.24 | Donghyun Han | [Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference](https://arxiv.org/pdf/2310.04378) |
| 24 | 2024.05.01 | Jeonghwa Yoo | [DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion](https://arxiv.org/abs/2304.06025) |
| 25 | 2024.05.08 | Sehwan Park | [LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models](https://arxiv.org/pdf/2305.13655) (CVPR2024)|
| 26 | 2024.05.15 | Kyeongmin Yu | [AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning](https://arxiv.org/abs/2307.04725) (ICLR 2024) |
| 27 | 2024.05.22 | Jeongin Lee | [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934) (CVPR 2020) |
| 28 | 2024.05.29 | Hyunsoo Kim | [3D Gaussian Splatting for Real-Time Radiance Field Rendering](https://arxiv.org/abs/2308.04079) (SIGGRAPH 2023) |
| 29 | 2024.06.12 | Donggeun Sean Ko | [DiffInject](https://arxiv.org/abs/2406.06134) (CVPR Workshop 2024) |
| 30 | 2024.06.26 | Jeonghwa Yoo  <br> Kyeongmin Yu | [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751) <br> [Shap-E: Generating Conditional 3D Implicit Function](https:arxiv.org/abs/2305.02463) |
| 31 | 2024.07.03 | Geonhak Song | [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) (ICLR 2023) |
| 32 | 2024.07.17 | Sangwoo Jo <br> Junhyoung Lee | [Magic3D: High-Resolution Text-to-3D Content Creation](https://arxiv.org/abs/2211.10440) (CVPR 2023) <br> [Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748) (ICCV 2023) |
| 33 | 2024.07.24 | Jeongin Lee <br> Hyunsoo Kim | [DreamBooth3D: Subject-Driven Text-to-3D Generation](https://arxiv.org/abs/2303.13508) (ICCV 2023) <br> [Style Aligned Image Generation via Shared Attention](https://arxiv.org/abs/2312.02133) (CVPR 2024) |


## Jupyter Book Update Procedure  
1. Clone the repo on your local computer  
```bash
git clone https://github.com/Pseudo-Lab/text-to-image-generation.git
```

2. Install required packages 
```bash
pip install jupyter-book==0.15.1
pip install ghp-import==2.1.0
```

3. Change the contents in ```book/docs``` folder with the following format and update ```_toc.yml``` file accordingly

    * reference: [https://github.com/Pseudo-Lab/SegCrew-Book](https://github.com/Pseudo-Lab/SegCrew-Book) 
    * default template: [https://github.com/Pseudo-Lab/Jupyter-Book-Template](https://github.com/Pseudo-Lab/Jupyter-Book-Template) 

- 3.1. Add information section on top of the markdown page 
```{admonition} Information
- **Title:** {논문 제목}, {학회/학술지명}

- **Reference**
    - Paper:  [{논문 링크}]({논문 링크})
    - Code: [{code 링크}]({code 링크})
    - Review: [{review 링크}]({review 링크})
    
- **Author:** {리뷰 작성자 기입}

- **Edited by:** {리뷰 편집자 기입}

- **Last updated on {최종 update 날짜 e.g. Apr. 12, 2023}**
```

- 3-2. Use the following template when displaying images 
```
:::{figure-md} 
<img src="{주소}" alt="{tag명}" class="bg-primary mb-1" width="{800px}">

{제목} \  (source: {출처})
:::
```

- 3-3. Update ```_toc.yml``` file accordingly
```
format: jb-book
root: intro
parts:
- caption: Paper/Code Review
  chapters:
  - file: docs/review/vae
  - file: docs/review/gan
```

4. Build the book using Jupyter Book command
```bash
jupyter-book build ./book
```

5. Sync your local and remote repositories
```bash
cd pseudodiffusers
git add .
git commit -m "adding my first book!"
git push
```

6. Publish your Jupyter Book with Github Pages
```
ghp-import -n -p -f book/_build/html -m "initial publishing"
```
