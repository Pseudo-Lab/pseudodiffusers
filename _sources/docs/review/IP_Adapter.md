``` {admonition} Information
- **Title:** IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models

- **Reference**
    - Paper: [https://arxiv.org/abs/2308.06721](https://arxiv.org/abs/2308.06721)
    - Code: [https://github.com/tencent-ailab/IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)
    - Project Page : [https://ip-adapter.github.io](https://ip-adapter.github.io)

- **Author:** Kyeongmin Yu

- **Last updated on Sep. 21, 2024**
```   

# IP-Adapter

> ğŸ“Œ ë¬¸ì œìƒí™© \
> text-to-image diffusion model(T2I diffusion model)ì´ ìƒì„±í•˜ëŠ” ì´ë¯¸ì§€ í’ˆì§ˆì€ í›Œë¥­í•˜ì§€ë§Œ text promptë¥¼ í†µí•´ ì›í•˜ëŠ” í˜•íƒœì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤. ë³µì¡í•œ prompt engineeringì„ ì‹œë„í•˜ê±°ë‚˜, image promptë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ fine-tuningí•˜ê²Œ ë˜ë©´ ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í•  ë¿ë§Œ ì•„ë‹ˆë¼ í•´ë‹¹ ë°©ì‹ì€ ë²”ìš©ì„±, í˜¸í™˜ì„±ë„ ë–¨ì–´ì§„ë‹¤. \
> \
> ğŸ“Œ í•´ê²°ë°©ì•ˆ \
> **cross-attentionì„ text featuresì™€ image featuresë¡œ decouplingí•œë‹¤.** ê¸°ì¡´ í•™ìŠµëœ diffusion modelì€ text featureì— ë§ì¶° í•™ìŠµëœ ìƒíƒœì´ë¯€ë¡œ ê¸°ì¡´ layerì— image featureë¥¼ ë„£ê²Œ ë˜ë©´ image featureì™€ text featureë¥¼ alignì„ ìˆ˜í–‰í•˜ê²Œ ë˜ë¯€ë¡œ ê¸°ì¡´ cross-attention layer í•˜ë‚˜ë¥¼ í†µí•´  image-featureì™€ text-featureë¥¼ ê²°í•©í•˜ëŠ” ê²ƒì€ ì ì ˆí•˜ì§€ ì•Šë‹¤. \
> \
> ğŸ“Œ  ë…¼ë¬¸ì˜ ê°•ì  
> - ì–´ë–¤ ëª¨ë¸ êµ¬ì¡°ì—ë„ í™œìš©ê°€ëŠ¥í•˜ë‹¤.
> - ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(22M)ë§Œ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ê°€ë³ë‹¤.
> - ê¸°ì¡´ controllable toolsì— ë§ë¶™ì—¬ ì“¸ ìˆ˜ë„ ìˆë‹¤.

:::{figure-md} 
<img src="../../pics/IP-Adapter/main.jpeg" alt="IP-Adapterë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ í•©ì„±" class="mb-1" width="800px">

IP-Adapterë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ í•©ì„± [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

# Introduction

**:image promptì˜ í•„ìš”ì„±ê³¼ ê¸°ì¡´ ì—°êµ¬ì—ì„œ image promptë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ëŠ” ì‹œë„ì˜ ì¢…ë¥˜ì™€ ì¥ë‹¨ì ì„ ë§í•œë‹¤.**

ë³µì¡í•œ sceneì´ë‚˜ conceptì„ ì…ë ¥í• ë•Œ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì´ ê°„í¸í•˜ê³  íš¨ê³¼ì ì´ë‹¤. 
image prompt + text prompt(â€œan image is worth a thousand wordsâ€)


:::{figure-md} 
<img src="../../pics/IP-Adapter/IMG_4891.png" alt="ì¹´í˜" class="mb-1" width="40%">

â€œë‚´ì¸„ëŸ´ í’ìœ¼ë¡œ ì¹´í˜ë¥¼ ê¾¸ë¯¸ê³  ì—¬ëŸ¬ ì‹ë¬¼ì„ ë‘ì–´ ì¥ì‹í•˜ê³  ì‹¶ì–´. ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ì‹ë¬¼ì€ ìŠ¤ë…¸ìš° ì‚¬íŒŒì´ì–´, í˜¸ì•¼, ìë¯¸ì˜¤ì¿¨ì¹´ìŠ¤ë“± ì´ê³ , ì˜ìì™€ í…Œì´ë¸”ì€ ì›ëª©ì„ ì„ í˜¸í•´.â€ 
:::


DALL-E2ëŠ” ì²˜ìŒìœ¼ë¡œ image promptë¥¼ ì§€ì›í•œ ëª¨ë¸ìœ¼ë¡œ, T2I prior modelì´ image embeddingì„ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í–ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ëŒ€ë¶€ë¶„ì˜ T2I ëª¨ë¸ì€ ì£¼ë¡œ textë¥¼ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ stable diffusion(SD) ëª¨ë¸ì˜ ê²½ìš° CLIP text encoderë¡œ ë¶€í„° text embeddingì„ ë½‘ì•„ë‚´ ì‚¬ìš©í–ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” â€œimage promptë¥¼ ê¸°ì¡´ T2I ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€â€, image promptë¥¼ ì‚¬ìš©í•œ T2I ì´ë¯¸ì§€ ìƒì„±ì„ ë‹¨ìˆœí•œ ë°©ì‹ìœ¼ë¡œ ê°€ëŠ¥ì¼€ í•œë‹¤.

:::{figure-md} 
<img src="../../pics/IP-Adapter/image.png" alt="ë¹„êµë¥¼ ìœ„í•œ DALL-E2(unCLIP) êµ¬ì¡°" class="mb-1" width="800px">

ë¹„êµë¥¼ ìœ„í•œ DALL-E2(unCLIP) êµ¬ì¡° [ì¶œì²˜](https://arxiv.org/abs/2204.06125)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image1.png" alt="ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°" class="mb-1" width="800px">

ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡° êµ¬ì¡° [ì¶œì²˜](https://arxiv.org/abs/2112.10752)
:::


SD Image Variationsì™€ Stable UnCLIPê³¼ ê°™ì€ ê¸°ì¡´ ì—°êµ¬ì—ì„œ image promptë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ text-conditioned diffusion modelsì„ image embeddingì„ ì‚¬ìš©í•´ ì§ì ‘ fine-tuningí•˜ë ¤ëŠ” ì‹œë„ë¥¼ í–ˆë‹¤. í•˜ì§€ë§Œ ë§ì€ ì–‘ì˜ ì»´í“¨í„° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ê³¼ ê¸°ì¡´ T2I ìƒì„±ëŠ¥ë ¥ ì €í•˜, ì¬ì‚¬ìš©ì„± ì €í•˜ë¼ëŠ” ë‹¨ì ì´ ìˆì—ˆë‹¤. ë˜í•œ í•´ë‹¹ ë°©ì‹ì€ ControlNetê³¼ ê°™ì€ ê¸°ì¡´ structural control toolsê³¼ í˜¸í™˜ë˜ì§€ ì•Šì•˜ë‹¤. ì´ëŠ” downstream applicationì— ì¹˜ëª…ì ì´ë‹¤.

ì´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ diffusion model ìì²´ë¥¼ fine-tuningí•˜ì§€ ì•Šê³  text encoderë¥¼ image encoderë¡œ êµì²´í•˜ëŠ” ë°©ì‹ë„ ìˆì—ˆì§€ë§Œ text promptë¥¼ ì§€ì›í•  ìˆ˜ ì—†ê²Œ ë˜ê³  ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆë‹¤.

ìµœê·¼ì—ëŠ” T2I base modelì„ ê±´ë“œë¦¬ì§€ ì•Šê³  ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•´ image promptë¥¼ ì§€ì›í•˜ëŠ” ì—°êµ¬ë“¤ì´ ìˆì—ˆë‹¤. ControlNet, T2I-Adapterì™€ ê°™ì€ ì—°êµ¬ë“¤ì€ ëŒ€ë¶€ë¶„ sketch, depth map, segmenation map ë“±ì˜ ì¶”ê°€ì ì¸ ì…ë ¥ì„ í™œìš©í–ˆë‹¤. ë˜í•œ T2I-Adapterë‚˜ Uni-ControlNet ê°™ì´reference imageë¥¼ ì…ë ¥í•´ style ì´ë‚˜ conceptì„ ì „ë‹¬í•˜ë ¤ëŠ” ì‹œë„ë„ ìˆì—ˆë‹¤. ì´ëŸ° íë¦„ì˜ ì—°êµ¬ë“¤ì€ CLIP image encoderì—ì„œ image embeddingì„ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€ trainable networkì— ìƒˆë¡œìš´ featureë“¤ì„ mappingí•˜ì—¬ text featureì™€ ìœµí•©í•˜ê³ ì í–ˆë‹¤. ê¸°ì¡´ text featureëŒ€ì‹  text feature+image featureë¥¼ ë””í“¨ì „ ëª¨ë¸ ë‚´ UNet êµ¬ì¡°ì— ë„£ì–´ promptì— ë„£ì€ ì´ë¯¸ì§€ì— ì í•©í•œ(faithful) ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³ ì í–ˆë‹¤. ì´ëŸ° ì—°êµ¬ë“¤ì„ í†µí•´ image promptì˜ ê°€ëŠ¥ì„±ì„ ë³¼ìˆ˜ ìˆì—ˆì§€ë§Œ ê·¸ ì¶©ì‹¤ë„ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•˜ë‹¤. ë˜í•œ ì´ë¯¸ì§€ í’ˆì§ˆì´ fine-tuningëœ image prompt modelë³´ë‹¤ ë‚˜ë¹´ë‹¤.


:::{figure-md} 
<img src="../../pics/IP-Adapter/compare_table.png" alt="ê¸°ì¡´ ëª¨ë¸ê³¼ IP-Adapter ë¹„êµ" class="mb-1" width="800px">

ê¸°ì¡´ ëª¨ë¸ê³¼ IP-Adapter ë¹„êµ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::


**image promptë¥¼ ì§€ì›í•˜ëŠ” ê¸°ì¡´ ë°©ì‹**

- input image embedding to T2I model
- base model fine-tuning
- text encoder â†’ image encoder
- additional network

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•ì„œ ì–¸ê¸‰í•œ ë¬¸ì œì ì˜ ì›ì¸ì„ T2I modelë‚´ì˜ cross-attentionì´ë¼ê³  ë³´ê³  ìˆë‹¤. **ì‚¬ì „í•™ìŠµëœ cross-attentionì—ì„œ key, value projection weightsì€ text featureì— ë§ê²Œ í›ˆë ¨ë˜ì–´ ì‡ëŠ” ìƒíƒœì´ë‹¤.** 
ê²°ê³¼ì ìœ¼ë¡œ image featureì™€ text featureë¥¼ cross-attention layerì—ì„œ í•©ì³ì§€ëŠ”ë° ì´ë•Œ image-specific íŠ¹ì„±ë“¤ì´ ë¬´ì‹œë˜ì–´ reference imageì— ì•„ì£¼ ì¶©ì‹¤í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ëª»í•˜ê³  coarse-grained controllable generation(e.g., image style)ë§Œ ë‹¬ì„± ê°€ëŠ¥í•´ì§„ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ì•ì„  ì—°êµ¬ì˜ ë¬¸ì œì ë“¤ì„ ê·¹ë³µí•œ íš¨ê³¼ì ì¸ image prompt adapter, IP-Adapterë¥¼ ì œì•ˆí•œë‹¤. íŠ¹íˆ IP-Adapterì˜ ê²½ìš° decoupled cross-attention mechanismì„ ì‚¬ìš©í•´ text featureì™€ image featureë¥¼ ë¶„ë¦¬í•œë‹¤. image featureë¥¼ ìœ„í•´ base modelë‚´ ëª¨ë“  UNet cross-attention layerì— cross-attention layer ë¥¼ ì¶”ê°€í•˜ì—¬ í›ˆë ¨ë‹¨ê³„ì—ì„œëŠ” ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(22M)ë§Œ í›ˆë ¨í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” IP-AdapterëŠ” ë§¤ìš° ê°€ë³ê³  íš¨ê³¼ì ì´ë‹¤. ë˜í•œ ì¼ë°˜í™” ëŠ¥ë ¥(generalization capability)ê°€ ë†’ê³  text promptì™€ë„ ì˜ ì–´ìš¸ë¦°ë‹¤(compatible). 

**IP-Adapterì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ì‹**

- additional cross-attention layer in UNet of diffusion model
- reusable and flexible (base + IP-Adapter + ControlNetê°€ëŠ¥)
- multimodal compatibility (image prompt + text prompt)

# Related Works

### Text-to-Image Diffusion Models

large T2I modelì€ í¬ê²Œ autoregressive models, diffusion models ë‘ ë¶€ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. DALLE, CogView, Make-A-Sceneê³¼ ê°™ì€ ì´ˆê¸° ì—°êµ¬ë“¤ì€ autoregressive model ì´ì—ˆë‹¤. autoregressive modelì€ VQ-VAEì™€ ê°™ì€ image tokenizerë¥¼ ì‚¬ìš©í•´ imageë“¤ì„ tokení™” í•˜ì—¬ autoregressive transformerì— text tokenì„ ì´ìš©í•´ image tokenì„ ì˜ˆì¸¡í•˜ê²Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆë‹¤. í•˜ì§€ë§Œ autoregressive modelì€ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ê³  ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í–ˆë‹¤.

ìµœê·¼ì—ëŠ” diffusion models(DM)ì´ ë“±ì¥í•˜ì—¬ T2I ìƒì„±ëª¨ë¸ì˜ state-of-the-artë¥¼ ë‹¬ì„±í–ˆë‹¤. ì´ì „ì— GLIDEëŠ” cascaded diffusion êµ¬ì¡°ë¥¼ í†µí•´ 64x64 â†’ 256x256 ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. 
DALL-E2ì˜ ê²½ìš°, text promptë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•´ image embeddingì„ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. DALL-E2ëŠ” text promptë¥¼ í†µí•œ ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•Šì•˜ë‹¤. text ì´í•´ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ Imagenì€ ê±°ëŒ€ transformer language modelì¸ T5ë¥¼ ë„ì…í–ˆë‹¤. Re-Imagenì˜ ê²½ìš° ë“œë¬¼ê±°ë‚˜ í•™ìŠµí•œì ì—†ëŠ” entityì— ëŒ€í•œ imageì— ëŒ€í•œ ì¶©ì„±ë„ë¥¼ ê°œì„ í–ˆë‹¤. 
SDëŠ” latent diffusion modelë¡œ pixel spaceê°€ ì•„ë‹Œ latent spaceìƒì—ì„œ ë™ì‘í•˜ê²Œ í•˜ì—¬ diffusion modelë§Œ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. text ì¼ì¹˜ë„(alignment)ë¥¼ ë†’ì´ê¸° ìœ„í•´ eDiff-Iì˜ ê²½ìš° T2I diffusion modelê³¼ ìœ ì‚¬í•œ ë””ìì¸ì„ ì±„íƒí•˜ì—¬ T5 text, CLIP text embedding, CLIP image embeddingë“± ë©€í‹°ëª¨ë‹¬ ì¡°ê±´ì„ í™œìš©í–ˆë‹¤. Versatile Diffusionì€ unified multi-flow diffusion frameworkë¥¼ ì´ìš©í•´ T2I, I2T, ë“± ë‹¤ì–‘í•œ ìƒì„±ë°©ì‹ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤. controllable image ìƒì„± ë©´ì—ì„œëŠ” Composerê°€ image embeddingì„ í™œìš©í•œ joint fine-tuningì„ ì‹œë„í–ˆì—ˆë‹¤. RAPHAELì€ mixture of experts(MoEs) ì „ëµì„ ì‚¬ìš©í•´ T2I modelì˜ ì´ë¯¸ì§€ í’ˆì§ˆì„ í–¥ìƒì‹œì¼°ë‹¤.

DALL-E2ëŠ” image promptë¥¼ í†µí•´ í•´ë‹¹ í’ì˜ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ë§¤ë ¥ì ì´ë‹¤. ë˜í•œ image promptë¥¼ T2I modelì—ì„œ ì§€ì›í•˜ê³ ì í•˜ëŠ” ì—°êµ¬ë“¤ì´ ìˆë‹¤. SD Image Variants modelì€ ë³€ê²½í•œ SDë¥¼ fine-tuningí•˜ì—¬ text featureë¥¼ CLIP image encoderì˜ image embeddingìœ¼ë¡œ êµì²´í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. Stable unCLIP ë˜í•œ SDë¥¼ fine-tuningí•˜ì—¬ time embeddingì— image embeddingì„ ì¶”ê°€í–ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì„ fine-tuningí•˜ëŠ” ë°©ì‹ì€ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ ë¹„êµì  training costê°€ ë†’ìœ¼ë©° ê¸°ì¡´ tools(e.g.,ControlNet)ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

### Adapters for Large Models

ê±°ëŒ€í•œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ì „ì²´ë¥¼ fine-tuningí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë‹¤. ì´ ëŒ€ì•ˆìœ¼ë¡œ ë– ì˜¤ë¥´ëŠ” ê²ƒì´ adapterë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì¸ë°, ê¸°ì¡´ ëª¨ë¸ì€ freezeì‹œì¼œ í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. adapterëŠ” NLPì—ì„œ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©ë˜ë˜ ë°©ì‹ì´ë‹¤. ìµœê·¼ì—ëŠ” LLMì˜ vision-language ì´í•´ë¥¼ ìœ„í•´ adapterë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤.

T2I modelì˜ ìµœê·¼ ì¸ê¸°ë¡œ ì¸í•´ adapterë“¤ë„ ì—¬ê¸°ì— ì¶”ê°€ì ì¸ controlì„ ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ControlNet(ì•„ë˜ ì‚¬ì§„ ì°¸ê³ )ì˜ ê²½ìš° ì‚¬ì „í•™ìŠµëœ T2I diffusion modelì— task-specificí•œ ì…ë ¥
(e.g.,canny edge)ì„ ì¶”ê°€ì ìœ¼ë¡œ ë„£ê¸°ìœ„í•´ adapterë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ìœ ì‚¬í•œ ì‹œê¸°ì— T2I-Adapter(ì•„ë˜ ì‚¬ì§„ ì°¸ê³ )ë„ ë“±ì¥í–ˆëŠ”ë° ë³´ë‹¤ ê°„ë‹¨í•˜ê³  ê°€ë²¼ìš´ í˜•íƒœë¡œ ìƒ‰ì´ë‚˜ êµ¬ì¡°ì ì¸ ë©´ì—ì„œ 
fine-grained controlì„ ì£¼ê³ ì í–ˆë‹¤. fine-tuningì— ì‚¬ìš©ë˜ëŠ” ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ Uni-ControlNetì€ multi-scale condition injectionì„ ì‚¬ìš©í–ˆë‹¤. 

structural controlì™¸ì— ì´ë¯¸ì§€ ì§‘í•©ì„ í†µí•´ contentë‚˜ styleì„ ì¡°ì ˆí•˜ê³ ì í•œ ì—°êµ¬ë„ ìˆë‹¤. ControlNet Shuffleì˜ ê²½ìš° ì´ë¯¸ì§€ë“¤ì„ recomposeí•˜ë„ë¡ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìê°€ ì œê³µí•œ ì´ë¯¸ì§€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆì—ˆë‹¤. ë˜í•œ ControlNet Reference-onlyì˜ ê²½ìš°, í•™ìŠµì—†ì´ SDì— feature injectionì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë³€í˜•í–ˆë‹¤. T2I-Adapterì˜ ìµœê·¼ ë²„ì „ì˜ ê²½ìš°, CLIP image encoderë¡œ ë¶€í„° reference imageì˜ image featureë¥¼ text featureì— ë”í•´ì¤Œìœ¼ë¡œì„œ style adapterë¡œì„œì˜ ì—­í• ë„ ê°€ëŠ¥í•˜ë‹¤. Uni-ControlNet(ì•„ë˜ ì‚¬ì§„ ì°¸ê³ )ì˜ global control adapter ë˜í•œ CLIP image encoderë¡œ ë¶€í„° ì¶”ì¶œí•œ image embeddingì„ ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ projectioní•˜ condition embeddingìœ¼ë¡œ projectioní•˜ì—¬ ì‚¬ìš©í•œë‹¤. SeeCoder(ì•„ë˜ ì‚¬ì§„ ì°¸ê³ )ëŠ” ê¸°ì¡´ text encoderë¥¼ semantic context encoderë¡œ êµì²´í•˜ì—¬ image variantsë¥¼ ìƒì„±í•˜ê³ ì í–ˆë‹¤.


**ControlNet**

:::{figure-md} 
<img src="../../pics/IP-Adapter/image2.png" alt="ë¹„êµë¥¼ ìœ„í•œ ControlNetì˜ ì‘ë™ ë°©ì‹" class="mb-1" width="40%">

ë¹„êµë¥¼ ìœ„í•œ ControlNetì˜ ì‘ë™ ë°©ì‹ [ì¶œì²˜](https://arxiv.org/abs/2302.05543)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image3.png" alt="" class="mb-1" width="40%">

ControlNet preprocessor[ContentShuffleDetector](https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/pixelshuffle.py)
:::

**Uni-ControlNet**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image4.png" alt="Uni-ControlNet Architecture" class="mb-1" width="40%">

Uni-ControlNet Architecture [ì¶œì²˜](https://arxiv.org/abs/2305.16322)
:::

**T2I-Adapter**

:::{figure-md} 
<img src="../../pics/IP-Adapter/image6.png" alt="ë¹„êµë¥¼ ìœ„í•œ T2I Adapterì˜ ì‘ë™ ë°©ì‹" class="mb-1" width="800px">

ë¹„êµë¥¼ ìœ„í•œ T2I Adapterì˜ ì‘ë™ ë°©ì‹ [ì¶œì²˜](https://arxiv.org/abs/2302.08453) [PixelUnshuffle](https://pytorch.org/docs/stable/generated/torch.nn.PixelUnshuffle.html)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image5.png" alt="ë¹„êµë¥¼ ìœ„í•œ T2I Adapterì˜ ì‘ë™ ë°©ì‹" class="mb-1" width="40%">

T2I Adapterì˜ ì¥ì  [ì¶œì²˜](https://arxiv.org/abs/2302.08453)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image7.png" alt="ë¹„êµë¥¼ ìœ„í•œ T2I Adapterì˜ ì‘ë™ ë°©ì‹" class="mb-1" width="40%">

T2I-adapterì˜ ê²°ê³¼ëŠ” ê¸°ì¡´ ëª¨ë¸ì˜ ê° ì¸µì˜ feature map í¬ê¸°ê°€ ë§ë„ë¡ ë”í•´ì§ [ì¶œì²˜](https://arxiv.org/abs/2302.08453)
:::

**SeeCoder**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image8.png" alt="SeeCoder Architecture" class="mb-1" width="40%">

SeeCoder Architecture [ì¶œì²˜](https://arxiv.org/abs/2305.16223)
:::

# Method

### Preliminaries


> ğŸ“Œ ìƒì„±ëª¨ë¸ì˜ ì¼ì¢…ì¸ diffusion modelì˜ ì´ë¯¸ì§€ ìƒì„±ë‹¨ê³„ 
> 1. **diffusion process (forward process)** \
    T stepì˜ fixed Markov chainì„ í†µí•´ë°ì´í„°ì— gaussian noiseë¥¼ ì ì°¨ ì¶”ê°€.  
> 2. **denoising process** \
    gaussian noiseë¡œ ë¶€í„° learnable modelì„ í†µí•´ sampleì„ ìƒì„±.
    

ì¼ë°˜ì ìœ¼ë¡œ noise ì˜ˆì¸¡ì„ ìœ„í•œ diffusion model($\epsilon_\theta$)ì˜ training objectiveëŠ” ì•„ë˜ì™€ ê°™ì´ ë‹¨ìˆœí•œ variant of variational bound ë¡œ í‘œí˜„ëœë‹¤. 

$$
L_{\text{simple}}=\Bbb E_{x_0, \epsilon\sim \mathcal N(0,I),c,t}\|\epsilon-\epsilon_\theta(x_t, \mathbf c,t)\|^2 \tag{1}
$$

$x_0$ ëŠ” real data, $\mathbf c$ ëŠ” ì¶”ê°€ì¡°ê±´, $t$ ëŠ” time stepì„ ë§í•˜ë©° $[0,T]$ ë‚´ì— ì†í•œë‹¤.  $x_t=\alpha_t x_0+\sigma_t\epsilon$ì€ step tì— í•´ë‹¹í•˜ëŠ” noisy dataë¥¼ ë§í•˜ê³ , $\alpha_t, \sigma_t$ëŠ” diffusino processë¥¼ ê²°ì •í•˜ëŠ” predefined functionì´ë‹¤. $\epsilon_\theta$ê°€ í•œë²ˆ í•™ìŠµë˜ê³  ë‚˜ë©´ ëœë¤ ë…¸ì´ì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìƒì„± ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ DDIM, PNDM, DPM-solverì™€ ê°™ì€ fast samplerë¥¼ inferenceì‹œ ì‚¬ìš©í•œë‹¤.

conditional diffusion modelì—ì„œ classifier guidanceë¥¼ í†µí•´ ì´ë¯¸ì§€ ì •í™•ë„(fidelity)ì™€ ë‹¤ì–‘ì„±(sample diversity)ë¥¼ ë°¸ëŸ°ì‹±í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ë”°ë¡œ í•™ìŠµëœ classifierì˜ gradientë¥¼ í™œìš©í•˜ëŠ”ë°, classifierë¥¼ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ë²ˆê±°ë¡œì›€ì„ ì§€ìš°ê¸° ìœ„í•´ classifier-free guidanceë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. ì´ëŸ° ì ‘ê·¼ì—ì„œ conditional, unconditional diffusion modelsëŠ” í•™ìŠµì‹œ ëœë¤í•˜ê²Œ ì¡°ê±´ $c$ ë¥¼ ë°°ì œí•˜ì—¬ í•©ë™ í•™ìŠµ(joint training)ëœë‹¤. samplingë‹¨ê³„ ì—ì„œëŠ” conditional modelê³¼ unconditional modelì˜ predictionì„ ëª¨ë‘ ì´ìš©í•˜ì—¬ noiseë¥¼ ê³„ì‚°í•œë‹¤. 

$$
\hat \epsilon_\theta(x_t,\mathbf c,t)=\mathcal w \epsilon_\theta(x_t,\mathbf c, t)+(1-\mathcal w)\epsilon_\theta(x_t,t) \tag{2}
$$

$\mathcal w$ì€ guidance scale í˜¹ì€ guidance weightë¡œ ë¶ˆë¦¬ëŠ”ë° condition $c$ì˜ ì˜í–¥ë ¥ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•œ ìƒìˆ˜ê°’ì´ë‹¤. T2I diffusion modelì˜ ê²½ìš° image-text ì¼ì¹˜ì„±ì„ ë†’ì´ëŠ”ë° classifier-free guidanceê°€ í° ì—­í• ì„ í•œë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” open-source SDì— IP-Adapterë¥¼ ë§ë¶™ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. SDëŠ” latent diffusion modelë¡œ frozen CLIP text encoderë¡œ ë½‘ì•„ë‚¸ text featureë¥¼ conditionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. diffusion modelì€ Unetì— attention layerê°€ ì¶”ê°€ëœ í˜•íƒœì´ë‹¤. Imagenê³¼ ê°™ì€ pixel-based diffusion modelê³¼ ë¹„êµí•´ SDëŠ” ì‚¬ì „í•™ìŠµëœ auto-encoder modelì„ í™œìš©í•´ latent spaceì—ì„œ ë™ì‘í•˜ë¯€ë¡œ íš¨ìœ¨ì ì´ë‹¤. 

### Image Prompt Adapter

:::{figure-md} 
<img src="../../pics/IP-Adapter/image9.png" alt="S" class="mb-1" width="800px">

IP-Adapterì˜ ë™ì‘ë°©ì‹ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image1.png" alt="ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°" class="mb-1" width="800px">

ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡° [ì¶œì²˜](https://arxiv.org/abs/2112.10752)
:::

**Image Encoder**

pretained CLIP image encoderë¥¼ ì‚¬ìš©í•´ image promptì—ì„œ image featureë¥¼ ë½‘ì•„ëƒˆë‹¤. CLIPì€ multimodal modelë¡œ ê±°ëŒ€ image-text pair ë°ì´í„°ì…‹ìœ¼ë¡œ contrastive learningì‹œí‚¨ ëª¨ë¸ì´ë‹¤. CLIP image encoderë¥¼ í†µí•´ global image embeddingì„ ì–»ì—ˆë‹¤. ì´ëŠ” imageë¡œ ë¶€í„° í’ë¶€í•œ ë‚´ìš©(content)ì™€ ìŠ¤íƒ€ì¼ì„ ë‹´ì€ image captionê³¼ ì˜ ì¡°ì •ë˜ì–´(well-aligned) ìˆë‹¤. í•™ìŠµë‹¨ê³„ì—ì„œ CLIP image encoderëŠ” frozenë˜ì–´ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤.

**Decoupled Cross-Attention**

image featureëŠ” ì‚¬ì „í•™ìŠµëœ UNetì— decoupled cross-attentionì„ í†µí•´ ê²°í•©ëœë‹¤. ì´ˆê¸° SD modelì—ì„œëŠ” CLIP text encoderë¥¼ í†µí•´ ë½‘ì•„ë‚¸ text featureë¥¼ UNetì˜ cross-attention layerì— ë„£ì—ˆë‹¤. 

$$
\mathbf Z'=\text{Attention}(\mathbf{Q,K,V})=\text{Softmax}(\frac{\mathbf {QK}^T}{\sqrt{d}})\mathbf V, \tag{3}
$$

query featureëŠ” $Z$, text featureëŠ” $c_t$, cross-attentionì˜ ê²°ê³¼ëŠ” $Zâ€™$ì´ê³ , $\mathbf{Q=ZW_q, K=c_t W_k, V=c_t W_v}$ëŠ” attention ì—°ì‚°ì˜ ê°ê° query, key, value í–‰ë ¬ì´ë‹¤. $\mathbf{W_q, W_k, W_v}$ëŠ” linear projection layersì˜ í•™ìŠµê°€ëŠ¥í•œ weigth matricesë‹¤.

image featureë¥¼ ì´ë¯¸ì§€ ìƒì„±ì— ë°˜ì˜í•˜ëŠ” ì§ê´€ì ì¸ ë°©ë²•ì€ cross-attentionì‹œ text feature+image featureë¡œ ê²°í•©(concatenate)í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì—¬ decoupled cross-attentionì„ ì œì•ˆí•œë‹¤. ì´ëŠ” cross-attention ì—ì„œ image featureì™€ text featureë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ê¸°ì¡´ cross-attention layerê°€ ì¡´ì¬í•˜ë˜ ê³³ì— ìƒˆë¡œìš´ cross-attention layerë¥¼ ì¶”ê°€í•˜ì—¬ image featureë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í–ˆë‹¤. image feature $c_i$ê°€ ì£¼ì–´ì§ˆë•Œ ìƒˆë¡œìš´ attention layerì˜ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$
\mathbf Z''=\text{Attention}(\mathbf{Q,K',V'})=\text{Softmax}(\frac{\mathbf{Q(K')}^T}{\sqrt{d}})\mathbf V', \tag{4}
$$

$\mathbf{Q=ZW_q}$, $\mathbf{K'=c_t W'_k}$ , $\mathbf{V'=c_t W'_v}$ ëŠ” image featureë¥¼ ìœ„í•œ query, key, value í–‰ë ¬ì´ë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ text cross-attentionê³¼ image cross-attentionì—ì„œ ë™ì¼í•œ qeuryë¥¼ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ê° cross-attention layer ë§ˆë‹¤ 2ê°œì˜ íŒŒë¼ë¯¸í„° $\mathbf{W'_k,W'_v}$ ë¥¼ ì¶”ê°€í•˜ê²Œ ëœë‹¤. ìˆ˜ë ´ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ $\mathbf{W'_k,W'_v}$ëŠ” $\mathbf{W_k,W_v}$ë¡œ ì´ˆê¸°í™”í–ˆë‹¤. ê·¸ëŸ¬ë©´ ë‘ cross-attention layerì˜ ê²°ê³¼ë¥¼ ë”í•¨ìœ¼ë¡œì¨ ìµœì¢… ê²°ê³¼ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. decoupled cross-attentionì˜ ìµœì¢…ì ì¸ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\mathbf Z^\text{new}=\text{Softmax}(\frac{\mathbf {QK}^T}{\sqrt{d}})\mathbf V + \text{Softmax}(\frac{\mathbf {Q(K')}^T}{\sqrt{d}})\mathbf V' \tag{5}
$$

$$ \text{where} \space \mathbf{Q=ZW}_q,\space \mathbf{k=c}_t\mathbf W_k,\space \mathbf{K'=c}_i\mathbf W'_k, \space \mathbf{V'=c}_i\mathbf W'_v 
$$

ì‚¬ì „í•™ìŠµí•œ UNetì€ freezeì‹œí‚¤ê³  í›ˆë ¨ì„ ì§„í–‰í•˜ë¯€ë¡œ $\mathbf{W'_k,W'_v}$ **ë§Œ** í•™ìŠµëœë‹¤.

 

**Training and Inference**

í•™ìŠµì‹œ IP-Adapterë§Œ ìµœì í™”í•˜ê³  ê¸°ì¡´ ì‚¬ì „í•™ìŠµëœ diffusion modelì€ ê³ ì •í•œë‹¤. IP-AdapterëŠ” image-text pair datasetìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©° original SDì™€ ë™ì¼í•œ objectiveë¥¼ ì‚¬ìš©í•œë‹¤.

$$
L_{\text{simple}}=\Bbb E_{x_0, \epsilon\sim \mathcal N(0,I),c_t,c_i,t}\|\epsilon-\epsilon_\theta(x_t,\mathbf {c_t,c_i},t)\|^2 \tag{6}
$$

ë˜ randomí•˜ê²Œ image conditionì„ dropí•˜ì—¬ inference ë‹¨ê³„ì—ì„œ classifier-free guidanceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

$$
\hat \epsilon_\theta(x_t,\mathbf {c_t,c_i},t)=\mathcal w \epsilon_\theta(x_t,\mathbf {c_t,c_i}, t)+(1-\mathcal w)\epsilon_\theta(x_t,t) \tag{7}
$$

image conditionì´ dropë˜ë©´  CLIP image embeddingì€ 0ìœ¼ë¡œ ì²˜ë¦¬í–ˆë‹¤. text cross-attentionê³¼ image cross-attentionì„ detachë˜ë©° inferenceì‹œ image conditionì˜ ê°€ì¤‘ì¹˜ë„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. $\lambda$ ê°€ 0ì´ ë˜ë©´ ê¸°ì¡´ T2I ëª¨ë¸ì´ ëœë‹¤.

$$
\mathbf Z^\text{new}=\text{Softmax}(\mathbf {Q,K,V})+ \lambda\cdot\text{Softmax}(\mathbf {Q,K',V'})\tag{8}
$$

# Experiments

### Experimental Setup

|   í•­ëª©  |   ê°’    |
|--------|:------:|
| base model | SD v1.5 |
| image encoder | OpenCLIP ViT-H/14 |
| resolution | 512x512 (resized and center crop) |
| optimizer | AdamW |
| learning rate | 0.0001 |
| weight decay | 0.01 |
| libraries | Hugging Face diffusers,\\ DeepSpeed SeRO-2 |
| GPU | 8 V100 |
| training step | 1M |
| batch size | 8 per GPU |
| classifier-free guidance | 0.05 |
| training data | LAION-2B, COYO-700M |
| sampler for inference | DDIM (50steps) |
| guidance scale | 7.5 |
| $\lambda$ | 1.0 for only image prompt |

### Comparison with Existing Methods

**Quantitative Comparison**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image10.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**Qualitative Comparison**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image11.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

(ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ê³  IP-Adapterë¥¼ í™œìš©í•´ ìƒì„±í•œ ì´ë¯¸ì§€ê°€ referenceì™€ ì§€ë‚˜ì¹˜ê²Œ ìœ ì‚¬í•˜ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. ëª‡ëª‡ì€ ê·¸ëƒ¥ ì¢Œìš°ë°˜ì „ì„ í•œê²ƒì²˜ëŸ¼ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. í”íˆ GANì—ì„œ ë§í•˜ëŠ” Model Collapseì™€ ê°™ì€ í˜„ìƒì´ ì•„ë‹Œê°€ ì‹¶ì–´ ë‹¤ì–‘ì„±ì´ ë‚®ì•„ë³´ì´ëŠ” ê²°ê³¼ê°€ ì˜ì•„í–ˆìœ¼ë‚˜, conclusionì—ì„œ ì´ ë‹¨ì ì„ ì–¸ê¸‰í•©ë‹ˆë‹¤.)

### More Results

**Generalizable to Custom Models**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image13.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**Structure Control**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image14.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**Image-to-Image Inpainting**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image15.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**Multimodal Prompts**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image16.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

### Ablation Study

**Importance of Decoupled Cross-Attention**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image17.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**Comparison of Fine-grained Features and Global Features**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image18.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="800px">

ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

IP-AdapterëŠ” CLIP image encoderë¡œ ë¶€í„° ì¶”ì¶œí•œ global image embeddingë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— reference imageì˜ ì¼ë¶€ íŠ¹ì„±ì„ ìƒì–´ë²„ë¦´ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ fine-grained featureë¥¼ ìœ„í•œ IP-Adapterë¥¼ ë””ìì¸í–ˆë‹¤. ì²«ë²ˆì§¸ë¡œ CLIP image encoderì—ì„œ penultimate layerì—ì„œ grid featureë¥¼ ë½‘ì•„ë‚¸ë‹¤. ì´í›„ ì‘ì€ query networkë¥¼ ì´ìš©í•´ featureë¥¼ í•™ìŠµí•œë‹¤. grid featureë¡œ ë¶€í„° ì •ë³´ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ lightweight transformerë¥¼ ì‚¬ìš©í•´ learnable 16 tokenë“¤ì„ ì •ì˜í•œë‹¤. ì´ token featureë“¤ì„ query networkì˜ cross-attention layerì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤€ë‹¤.

ë‘ adapterì˜ ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµí•˜ë©´ finer-grained featureë¥¼ ì´ìš©í•˜ë©´ ë³´ë‹¤ image promptì™€ ê°€ê¹Œìš´ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. finer-grained featureëŠ” spatial structure informationì„ í•™ìŠµí•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ diversityë¥¼ ë‚®ì¶”ëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¶”ê°€ì ì¸ ì¡°ê±´(text prompt, structure map)ì„ í™œìš©í•˜ë©´ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‚¬ì§„+poseë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆë‹¤.

# Conclusion

ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì „ í•™ìŠµëœ T2I diffusion modelì— image prompt capabilityë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ IP-Adapterë¥¼ ì œì•ˆí•œë‹¤. IP-Adapterì˜ í•µì‹¬ ë””ìì¸ì€ decoupled cross-attentionìœ¼ë¡œ image featureë¥¼ ë¶„ë¦¬í•˜ì—¬ cross-attentionì„ ìˆ˜í–‰í•œë‹¤. ê³ ì‘ 22M parameterê°€ ì¶”ê°€ëœ IP-AdapterëŠ” qualitative, quantitative experimental results ëª¨ë‘ì—ì„œ ë¹„ë“±í•˜ê±°ë‚˜ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë˜í•œ IP-AdapterëŠ” í™•ì¥ì„±ì´ ì¢‹ì•„ í•œë²ˆ í›ˆë ¨ëœ ë’¤, ë‹¤ë¥¸ custom model, structural controllable toolsì— ê³§ë°”ë¡œ ë§ë¶™ì—¬ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ë”ìš± ì¤‘ìš”í•œ ì ì€ image promptë¥¼ text promptì™€ ë”ì•  ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ìƒì„±ì„ ê°€ëŠ¥ì¼€í•œë‹¤ëŠ” ì ì´ë‹¤.

IP-AdapterëŠ” íš¨ê³¼ì ì´ì§€ë§Œ reference imageì™€ content, styleì´ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë§Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆë‹¤. ë•Œë¬¸ì— Textual Inversionì´ë‚˜ DreamBoothì™€ ê°™ì´ íŠ¹ì • ì´ë¯¸ì§€ ì§‘í•© í’ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ëŠ” ëª»í•œë‹¤. ë¯¸ë˜ì— consistencyë¥¼ í–¥ìƒì‹œí‚¨ ë” ê°•ë ¥í•œ Image prompt adapterë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.

**Textural Inversion**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image19.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="40%">

Textural Inversion ë™ì‘ë°©ì‹ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image20.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="40%">

Textural Inversion ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

**DreamBooth**
:::{figure-md} 
<img src="../../pics/IP-Adapter/image21.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="40%">

DreamBooth ë™ì‘ ë°©ì‹ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::

:::{figure-md} 
<img src="../../pics/IP-Adapter/image22.png" alt="ì‹¤í—˜ê²°ê³¼" class="mb-1" width="40%">

DreamBooth ì‹¤í—˜ê²°ê³¼ [ì¶œì²˜](https://arxiv.org/abs/2308.06721)
:::