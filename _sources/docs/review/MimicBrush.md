```{admonition} Information
- **Title:** MimicBrush: Zero-shot Image Editing with Reference Imitation

- **Reference**
    - Paper: [https://arxiv.org/pdf/2406.07547](https://arxiv.org/pdf/2406.07547)
    - Code: [Official](https://github.com/ali-vilab/MimicBrush)
    
- **Author:** Chanyeong Shin

- **Last updated on Nov. 05, 2024**
```

# MimicBrush: Zero-shot Image Editing with Reference Imitation 

## Three Lines Summary

1. Edited Image ë¡œ ì–´ë–»ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í•´ì•¼ í• ì§€ì— ê´€í•œ **â€œimitative editingâ€ ì— ê´€í•œ ì•„ì´ë””ì–´**
2. **Source Image ì™€ Reference Image ê°„ì˜ correspondence** ë¥¼ ì´ìš©í•´ ë°”ë€Œì–´ì•¼ í•  ë¶€ë¶„ì„ ì˜ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì¸ MimicBrush ë¼ê³  ëª…ëª…í•œ generative training framework ì œì•ˆ
3. **SOTA ì¸ ë™ì‹œì— ì•ìœ¼ë¡œì˜ imitative editing ì—°êµ¬ë¥¼ ìœ„í•œ evaluation benchmark** ì œì•ˆ

## 1. Introduction

- Image editing ë¶„ì•¼ ìì²´ê°€ êµ‰ì¥íˆ ìˆ˜ë§ì€ ìš”êµ¬ ì¡°ê±´ê³¼ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë§Œì¡±í•˜ë©´ì„œ ìˆ˜í–‰ë˜ì–´ì•¼ í–ˆê¸°ì— ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì´ ì—¬ì „íˆ challenging í•œ ë¬¸ì œë¥¼ í’€ê³  ìˆìŒ
- ê¸°ì¡´ì˜ ë°©ë²•ë“¤ì€ í˜„ì¬ source image ì™€ í•¨ê»˜ í•´ë‹¹ mask ë¥¼ input ìœ¼ë¡œ ë„£ì–´ì£¼ê³  ìˆìŒ (ì´ê±´ ê³µí†µ)
    - Inpainting method
        - editing ì´ë¼ëŠ” task ë¥¼ **â€œText Promptâ€** í•˜ë‚˜ë¡œë§Œ í•´ê²°í•˜ê³ ì í–ˆëŠ”ë° ì´ê±´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ë½‘ì•„ë‚´ê¸°ì—ëŠ” ì ì ˆí•˜ì§€ ì•ŠìŒ
        - My thoughts : ì‹¤ì œë¡œ inpainting ì€ refining ê³¼ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒ ì™¸ì—ëŠ” ì‹¤ë¬´ì—ì„œ ì˜ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šì•˜ë˜ ê²ƒ ê°™ìŒ
    - Composition method
        - ref image ì™€ ref mask/box ë¥¼ ì´ìš©í•´ ì´ë¥¼ í•´ê²°í•˜ê³ ì í•˜ì˜€ëŠ”ë°, ì•„ë¬´ë˜ë„ **â€œindividual objectâ€ ë¥¼ insertion** í•˜ëŠ” ì‘ì—…ì²˜ëŸ¼ ëŠê»´ ëª¨ë¸ì´ ì–´ë ¤ì›Œ í•  ë§Œ í–ˆìŒ
            - shoe soles ì´ë‚˜ hair ì™€ ê°™ì€ local components ë‚˜ ë¡œê³ ë‚˜ texture ê°™ì€ local patterns
        - ë˜í•œ, Image ë¡œë¶€í„° reference area ë¥¼ ì™„ë²½í•˜ê²Œ ì˜ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì´ í•„ìš”í–ˆìŒ
        - Local components ë“¤ì€ ë˜ ì „ì²´ image ì— ì˜ ì–´ìš°ëŸ¬ì§€ê²Œ í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•´ì•¼ í–ˆê³ , í•™ìŠµ ê³¼ì •ì—ì„œ ê°™ì€ object ì¸ë° frame ì— ë”°ë¼ ëª¨ì–‘ë„ ì¡°ê¸ˆì”© ë‹¬ë¼ì„œ ì´ëŸ° ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì„ í’€ì–´ì•¼ í–ˆìŒ
- ì•ì„  ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, editing ì„ í•  ìˆ˜ ìˆëŠ” novel pipeline ì¸ **imitative editing** ì„ ì œì•ˆ
    - Ref image ì—ì„œ mask ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  source image ì˜ mask area ë¶€ë¶„ì„ ref image ì—ì„œ ì–´ë””ì— í•´ë‹¹í•˜ëŠ”ì§€ ìë™ìœ¼ë¡œ ì°¾ê³  ëª¨ë°©í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ìŒ

- Imitative editing ì„ í•˜ê¸° ìœ„í•´ì„œ **MimicBrush ë¼ê³  í•˜ëŠ” dual diffusion U-Nets network framework** ë¥¼ ì œì•ˆ
    - self-supervised manner ë¡œ í•™ìŠµí•¨ â†’ ê°™ì€ video ì—ì„œ source / ref image ë¥¼ ë½‘ì•„ì„œ í•™ìŠµì— í™œìš©í•˜ëŠ” ë°©ì‹
    - ë‹¹ì—°íˆ ê°™ì€ video ì—ì„œ ì¶”ì¶œí–ˆê¸° ë•Œë¬¸ì— semantic correspondence ì™€ visual variations ë¥¼ ê°€ì§€ê³  ìˆì„ ê²ƒ
    - masked source image ëŠ” imitative U-Net ìœ¼ë¡œ / reference image ëŠ” reference U-Net ìœ¼ë¡œ í†µê³¼ â†’ ì´í›„  reference U-Net ì˜ attention K,V ë¥¼ imitative U-Net ì— injection ì‹œí‚¤ëŠ” ë°©ì‹
- ì´ Image editing ë°©ì‹ì„ ì´ìš©í•˜ì—¬ í•™ìŠµëœ model ì€ êµ¬ë„, ì¡°ëª…, ì¹´í…Œê³ ë¦¬ ìì²´ê°€ ë‹¬ë¼ë„ ì˜ ë³€í™˜ì´ ë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, ref image ì˜ visual concepts ì˜ detail ë„ ì˜ ìœ ì§€í•œ ì±„ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŒ
- ì¡°ê¸ˆ ë” comprehensive í•œ evaluation ì„ ìœ„í•´ì„œ imitative editing ì˜ benchmark ê¹Œì§€ ì œì•ˆ
    - part composition : ì–¼ë§ˆë‚˜ part êµ¬ì„±ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì˜ ë˜ì—ˆëŠ”ê°€?
    - texture transfer : ëŠë‚Œì„ ì–¼ë§ˆë‚˜ ì˜ ìœ ì§€í•˜ë©´ì„œ texture ê°€ ë³€í™˜ëëŠ”ê°€?

## 2. Method

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_1.png" alt="The training process of MimicBrush" class="bg-primary mb-1" width="800px">

The training process of MimicBrush
:::

### Overall Pipeline

- **Dual diffusion models architecture + self-supervised manner**
- Video data ì—ëŠ” consistent content ë¥¼ ê°€ì§€ê³  ìˆê³  visual variations ê¹Œì§€ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ í™œìš©
    - ëœë¤í•˜ê²Œ video clip ì—ì„œ two frames ë¥¼ ë½‘ì•„ì„œ í•™ìŠµ sample ë¡œ í™œìš©
- source image ëŠ” masking ì‹œí‚¤ê³ , ref image ëŠ” masked source image ë¥¼ recover í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ì„ ì£¼ë„ë¡ ë„£ìŒ
- ê²°êµ­ MimicBrush ëŠ” dogâ€™s face ì™€ ê°™ì€ corresponding visual information ì´ ìœ„ì¹˜í•˜ëŠ” ê²ƒì„ í•™ìŠµí•˜ê³  source image ì˜ masked area ë¥¼ repaint ì‹œí‚´
- ë˜í•œ source image ì˜ ë¹ˆ ë¶€ë¶„ì„ ì±„ìš°ëŠ” ê³¼ì •ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ blending ì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì— visual content ë¥¼ ê°™ì€ í™˜ê²½ì˜ í¬ì¦ˆ, ì¡°ëª…, ì‹œì ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒë„ í•™ìŠµ

- ì•ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒì²˜ëŸ¼ dual branch ì˜ U-Nets êµ¬ì¡°ë¥¼ í™œìš© â†’ imitative and reference U-Net
    - attention layers ì˜ K,V ëŠ” ì„œë¡œ share í•˜ê³ (ì‹¤ì§ˆì ìœ¼ë¡œëŠ” concat) reference image ë¡œë¶€í„° indications ì„ ì°¾ì•„ masked source image ë¥¼ ë§Œë“¤ë„ë¡ í•¨
- ì¶”ê°€ë¡œ, source & ref image ì— variation ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ augmentation ë„ ì ìš©
- ë˜í•œ, optional condition ìœ¼ë¡œ imitative U-Net ì— depth map ë„ ì¤Œ
    - inference ë‹¨ê³„ì—ì„œ object ì˜ shape ê°€ ì˜ ìœ ì§€ëëŠ”ì§€ depth map ì„ í™œìš©í• ì§€ ë§ì§€ë¥¼ ê²°ì •í•  ìˆ˜ë„ ìˆê²Œ í•˜ì˜€ìŒ


### Model Structure

- Imitative U-Net
    - Base : SD-1.5 inpainting model
    - Input : 13 channels tensor
        - image latent â†’ 4 channels
        - binary mask â†’ 1 channel
        - background latent â†’ 4 channels
        - depth latent â†’ 4 channels
    - (Figure ì—ëŠ” ë‚˜ì™€ìˆì§€ ì•Šì§€ë§Œ) ê¸°ì¡´ original U-Net ì€ CLIP ì„ text embedding ìœ¼ë¡œ í™œìš©í•˜ëŠ”ë°, ë³¸ ë…¼ë¬¸ì€ CLIP ì„ reference image ìœ¼ë¡œë¶€í„° ë½‘ì•„ë‚¸ image embedding ì„ cross-attention ì— í™œìš©
        - image embedding ì´í›„ì— projection layer ì„ ê±°ì³ ë“¤ì–´ê°
    - í•™ìŠµì€ imitative U-Net ê³¼ CLIP projection layer ì˜ parameters ë“¤ì´ í•™ìŠµ..

- Reference U-Net
    - ìµœê·¼ì— êµ‰ì¥íˆ ë§ì€ ì—°êµ¬ë“¤ì´ reference image ë¡œë¶€í„° fine-grained features ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ additional U-Net ì„ í™œìš©í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë” íš¨ìœ¨ì ì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•˜ì˜€ìŒ
    - Base : SD-1.5
    - reference features ë¥¼ imitative U-Net ì˜ middle & upper stages ì— K,V ë¥¼ injection (concat) ì‹œí‚´
        - ì´ë¥¼ í†µí•´, imitative U-Net ì´ reference image ì˜ content ë¥¼ í™œìš©í•´ source image ì˜ masking ëœ ë¶€ë¶„ì„ ì™„ì„±ì‹œí‚´

- Depth Model
    - Depth Anything ìœ¼ë¡œ unmasked source image ì˜ depth map ì„ ë½‘ë„ë¡ í–ˆìŒ
    - Depth model ìì²´ëŠ” freeze ì‹œí‚¤ê³ , trainable projector ë¥¼ ë„£ì–´ depth map ì„ depth latent ì¸ 4-channel ë¡œ projection ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•¨
    - í•™ìŠµ ì¤‘ì—ëŠ” depth model ì˜ input ì„ 0.5 í™•ë¥ ë¡œ drop ì‹œí‚´ìœ¼ë¡œì¨ inference ì¤‘ì— optional í•˜ê²Œ shape control ì„ ê°€ëŠ¥í•˜ë„ë¡ í•¨
        - My thoughts : ì•„ë§ˆ texture transfer task ë¥¼ ìœ„í•¨ ì¼ë“¯

### Training Strategy

- MimicBrush ì˜ cross-image imitation ability (ì„œë¡œ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì—ì„œ ì˜ ëª¨ë°©í•´ì˜¤ëŠ” ëŠ¥ë ¥ ì •ë„ë¡œ ì´í•´í•˜ë©´ ë¨) ë¥¼ ëŒì–´ì˜¬ë¦¬ê¸° ìœ„í•´ì„œëŠ” ì¡°ê¸ˆ ë” ì í•©í•œ training sample ì„ ëª¨ì•„ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ í•„ìš”í–ˆìŒ
- Training data ë¥¼ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì—ì„œ ë‘ ê°€ì§€ì˜ ì² í•™ì„ ì§€í‚¤ë ¤ í•˜ì˜€ìŒ
    1. source / reference images ë“¤ ê°„ì˜ correspondence relation ì´ ì¡´ì¬í•´ì•¼ í•œë‹¤.
    2. robustness ë¥¼ ìœ„í•´ source / reference image ì‚¬ì´ì˜ large variations ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
- Data selection
    - í•™ìŠµ ì¤‘ì—ëŠ” ê°™ì€ ë¹„ë””ì˜¤ë¡œë¶€í„° frame 2ê°œë¥¼ sampling í•´ì„œ ë½‘ì•˜ìŒ
    - SSIM ì„ ì´ìš©í•´ video frames ê°„ì˜ similarity ë¥¼ ì¸¡ì •í–ˆê³  ë„ˆë¬´ ê·¸ ê°’ì´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ filtering í–ˆìŒ
- Data augmentation
    - source & reference image ì˜ variation ì„ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ì„œ, ê°•ë ¥í•œ data augmentation ì„ í™œìš©
        - color jitter, rotation, resizing, flipping, ì‹¬ì§€ì–´ëŠ” random projection transformation ìœ¼ë¡œ ë”ìš± ê°•í•œ deformation ë„ ìˆ˜í–‰
- Masking strategy
    - ê°€ì¥ ê°„ë‹¨í•˜ê²ŒëŠ” image ë¥¼ N x N grid ë¡œ ë‚˜ëˆ„ê³  ëœë¤í•˜ê²Œ masking ì‹œí‚¤ëŠ” ë°©ë²•ì¸ë° ì €ìë“¤ì€ ì´ë ‡ê²Œ ëœë¤í•˜ê²Œ ê°€ì ¸ê°€ë©´ easy cases ë“¤ì´ ë§ì€ portion ì„ ì°¨ì§€í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬
        - ex. ë°°ê²½ ê°™ì€ í° area ë¥¼ ì°¨ì§€í•˜ëŠ” ê²ƒë“¤ì€ ê³„ì† ë°˜ë³µë˜ëŠ” content/textures ì´ê¸° ë•Œë¬¸ì— ë„ì›€ì´ ì•ˆë¨
    - SIFT matching ì„ ì´ìš©í•´ì„œ source & ref image ì˜ matching points ë¥¼ ì–»ê³  ê·¸ matched feature points ì˜ grids ë“¤ì„ ì¢€ ë” masking í•˜ë„ë¡ í•˜ì˜€ìŒ
    - video ë³´ë‹¤ high-quality image ë¥¼ ì°¾ëŠ” ê²ƒì´ ë” ì‰¬ì› ê¸° ë•Œë¬¸ì— static image í•œ ì¥ì„ ê°€ì§€ê³  augmentation ì‹œí‚¨ ë‹¤ìŒ, seg map ê°€ì§€ê³  masking ì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œë„ í™œìš© â†’ robustness ë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ë¶ˆëŸ¬ì¼ìœ¼í‚´

### Evaluation Benchmark

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_2.png" alt="Evaluation Benchmark" class="bg-primary mb-1" width="800px">

Evaluation Benchmark
:::

- Imitative editing ì€ êµ‰ì¥íˆ novel í•œ task ì´ê¸° ë•Œë¬¸ì— ë³¸ ë…¼ë¬¸ì—ì„œ ì„±ëŠ¥ì„ evaluation í•  ìˆ˜ ìˆëŠ” benchmark ê¹Œì§€ ì œê³µ
- Part composition
    - source / ref image ê°„ì˜ semantic correspondence ë¥¼ ì°¾ê³  composition ì‹œí‚¤ëŠ” task
    - Inter-ID track
        - Fashion, animal, product, scenario ë“± ë‹¤ì–‘í•œ data ë¥¼ ê° topic ë§ˆë‹¤ 30 ì¥ì”© ëª¨ì•˜ìŒ
        - ìˆ˜ë™ìœ¼ë¡œ source mask ë¥¼ ë‹¤ ê·¸ë¦¬ê³ , ìƒì„±ëœ ê²°ê³¼ GT ë„ ì—†ê¸° ë•Œë¬¸ì— ì§ì ‘ reference regions ê³¼ text prompt ê¹Œì§€ ë‹¤ annotation ì‹œí‚´ = ë…¸ê°€ë‹¤ í–ˆìŒ
        - reference region ê³¼ ìƒì„±ëœ region ì‚¬ì´ì˜ similarity ë„ DINO ì™€ CLIP image sim score ë¥¼ ê³„ì‚°í•˜ë„ë¡ í–ˆê³  edited image ì™€ text prompt ì‚¬ì´ì˜ CLIP text similarity ë„ report í•´ë†“ìŒ
    - Inner-ID track
        - DreamBooth ë¡œë¶€í„° 30 image paris ê°€ì ¸ì™€ì„œ source image masking ë‹¤ í•¨
        - GT ì™€ëŠ” SSIM, PSNR, LPIPS score í™œìš©
- Texture transfer
    - ref image ì˜ texture ë‚˜ pattern ì„ ì–¼ë§ˆë‚˜ ì˜ transfer í–ˆëŠ”ì§€ì— ê´€í•œ task
    - Additional condition ìœ¼ë¡œ depth map ì„ í™œìš©
    - Part composition ì€ semantic correspondence ë¥¼ ì°¾ë„ë¡ í•˜ì§€ë§Œ, ì´ task ëŠ” source shape ëŠ” ìœ ì§€í•˜ë©´ì„œ reference ì˜ texture ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ê°€ì ¸ì˜¤ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ë„ë¡ objects ì „ì²´ë¥¼ masking


## Experiments

### Implementation Details

- Hyperparameters
    - 512x512 image ë¡œ resolution ë‹¤ ë§ì¶”ë„ë¡ í•´ì„œ í•™ìŠµ
    - Adam optimizer & lr : 1e-5
    - grid number N : 3~10 ì—ì„œ randomly choose
    - masking ì€ SIFT-matched features points ëŠ” 75% ê³ ë¥´ë„ë¡ í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” 50% ê³ ë¥´ë„ë¡ í•˜ì˜€ìŒ
    - Reference U-Net ì—ëŠ” CFG 10% í™•ë¥ ë¡œ drop í•˜ë„ë¡ í•˜ì˜€ê³ , inference ì‹œì— guidance scale ì€ 5
- Training data
    - Pexels ê°™ì€ websites ì—ì„œ 100k video
    - diversity ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•´ì„œ SAM dataset ë„ í™œìš© â†’ ì—¬ê¸°ì„œ static image augmentation ì ìš©
    - í•™ìŠµ ì¤‘ì—ëŠ” Pexels 70% , SAM 30%

### Comparisons with Other Works

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_3.png" alt="Qualitative Result" class="bg-primary mb-1" width="800px">

Qualitative Result
:::

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_4.png" alt="Quantitative Result" class="bg-primary mb-1" width="800px">

Quantitative Result
:::

### Ablation Studies

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_5.png" alt="Ablation Studies" class="bg-primary mb-1" width="800px">

Ablation Studies
:::

### Qualitative Analysis

:::{figure-md} 
<img src="../../pics/MimicBrush/MimicBrush_6.png" alt="Qualitative Analysis" class="bg-primary mb-1" width="800px">

Qualitative Analysis
:::

## Limitations

- Robust performance.. But, region ì´ ë„ˆë¬´ ì‘ê±°ë‚˜ multiple candidates ê°€ ìˆê²Œ ë˜ë©´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ìƒê¸°ê¸´ í–ˆìŒ
    - ì´ëŸ´ ë•ŒëŠ” zoom in ì„ í•´ì„œ í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤ê³  í•¨
- ì—¬ì „íˆ ìœ í•´í•œ content ë“¤ì„ editing í•˜ëŠ” ê²½ìš°ë„ ìˆê¸° ë•Œë¬¸ì— ì´ë¥¼ í•„í„°ë§ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ê³ ì•ˆë˜ì–´ì•¼ í•œë‹¤ê³  ì£¼ì¥

## Review

<aside>
ğŸ”–

**Three Line Review**

1. ë…¼ë¬¸ ìì²´ì˜ ìƒ‰ê¹”ì´ ì—­ì‹œ ì‹¤ë¬´ì— ì í•©í•œ í˜ì´í¼ë‹¤ !
2. Evaluation benchmark ê¹Œì§€ ì œì•ˆí•˜ë©´ì„œ ìƒˆë¡œìš´ field ë¥¼ ì—´ë ¤ê³  í•˜ëŠ” ì‹œë„ê°€ ì¸ìƒ ê¹Šì—ˆë‹¤
3. paper ìì²´ëŠ” 24.06 ìœ¼ë¡œ ë”°ëˆë”°ëˆí•œë° ì™œ SD 1.5 ì¼ëŠ”ì§€ ì´í•´ê°€ ì˜ ì•ˆ ëë˜.. ë‹¤ë¥¸ baseline ë„ ì¨ë´¤ìœ¼ë©´ ë” ì¢‹ì•˜ì„ ê²ƒ ê°™ë‹¤

</aside>
