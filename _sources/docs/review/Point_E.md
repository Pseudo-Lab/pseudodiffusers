```{admonition} Information
- **Title:** Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)

- **Reference**
    - Paper:  [https://arxiv.org/abs/2212.08751](https://arxiv.org/abs/2212.08751)
    - Project: [https://openai.com/index/point-e/](https://openai.com/index/point-e/)

- **Author:** [Jeonghwa Yoo](https://www.linkedin.com/in/jeonghwa-yoo-8403a716b)

- **Last updated on Sep. 11, 2024**
```

# Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)


<aside>
💡 핵심 요약

- 관련 태스크: text-to-3D, point cloud generation
- 본 논문의 접근 방식
    - Text-to-image 모델과 image-to-3D 모델을 결합하여 두 방법의 장점을 합쳤다.
        - Text-to-image: 대규모 데이터가 존재하여 복잡하고 다양한 텍스트 프롬프트에 대해 적용 가능하다.
        - Image-to-3D: 소규모의 이미지 데이터와 3D 데이터 쌍에 대해 학습하여 3D 포인트 클라우드를 생성한다.
    - 세 단계의 프로세스로 구성됨
        - Text → 합성 뷰: GLIDE 모델
        - 합성 뷰 → 저해상도 포인트 클라우드: 트랜스포머 기반의 디퓨전 모델
        - 저해상도 포인트 클라우드 → 고해상도 포인트 클라우드: 트랜스포머 기반의 디퓨전 모델
- 결과
    - 샘플 품질 측면에서 SOTA 성능이 아니지만, 샘플링 속도가 1~2배 더 빠르다.
    - 텍스트 프롬프트에 의해 조건화된 다양하고 복잡한 3D 형상을 효율적으로 생성할 수 있다.
</aside>

# Abstract

- 제안 배경
    - 최근 텍스트 조건부 3D 객체 생성 기술(text-conditional 3D object generation)이 놀라운 발전을 보이고 있다.
    - 하지만, SOTA 모델들은 여전히 하나의 샘플을 만들기 위해 여러 GPU 시간을 요구하고 있다.
    - 본 논문에서는 단일 GPU에서 1~2분만에 3D 모델을 생성하는 3D 객체 생성을 위한 방법을 탐색한다.
- 접근법
    - 텍스트-이미지 디퓨전 모델을 사용하여 단일 합성 뷰를 생성한 다음 두 번째 디퓨전 모델을 사용하여 3D 포인트 클라우드를 생성한다.
- 결과
    - 샘플 품질 측면에서 SOTA 성능이 아니지만, 샘플링 속도가 1~2배 더 빠르다.

# 1. Introduction

- Text-to-image 생성 모델에서 text-to-vide/3D로의 발전
    - 최근 text-to-image 생성 모델이 폭발적으로 증가함에 따라 몇 초만에 자연어에서 고품질 이미지를 생성하고 수정할 수 있게 되었다.
    - 이러한 결과에 영감을 받아 최근 연구에서는 비디오나 3D 객체와 같은 다른 도메인에서의 텍스트 조건부 생성을 탐색하고 있다.
    - 본 논문도 text-to-3D 생성 문제에 중점을 둔다.
- 최근 text-to-3D 합성의 분류
    - 최근 text-to-3D 합성은 일반적으로 다음의 두 카테고리 중 하나에 속한다.
        1. 쌍을 이룬(paired)(ex: text, 3D) 데이터 또는 레이블이 없는(unlabeld) 3D 데이터에서 생성 모델을 직접(directly) 학습 시키는 방법
            1. 장점: 기존 생성 모델링 접근 방식을 활용하여 샘플을 효율적으로 생성할 수 있다.
            2. 단점: 대규모 3D 데이터셋이 없기 때문에 다양하고 복잡한 텍스트 프롬프트로 확장하기 어렵다. → 데이터셋의 한계, 확장성의 어려움
        2. 사전 학습된 text-to-image 모델을 활용하여 미분가능한(differentiable) 3D 표현법들(representations)을 최적화하는 방법
            1. 장점: 복잡하고 다양한 텍스트 프롬프트를 처리할 수 있다.
            2. 단점: 
                1. 각 샘플에 대해 최적화 과정을 거쳐야 하기 때문에 계산 비용이 많이 들고 시간이 오래 걸릴 수 있다.  샘플을 생성하는 데 비용이 많이 드는 최적화 프로세스가 필요하다. 
                2. 강력한 3D prior가 없기 때문에 의미 있거나 일관된 3D 개체에 해당하지 않는 local minima에 빠질 수 있다.
- 본 논문의 접근법
    
    :::{figure-md} 
    <img src="../../pics/Point_E/01.png" alt="Point_E_01" class="bg-primary mb-1">

    Point-E 파이프라인 개요 
    :::
    
    - Text-to-image 모델과 image-to-3D 모델을 결합하여  두카테고리의 장점을 합치는 것을 목표로 한다.
        - 본 논문의 text-to-image 모델
            - 대규모(텍스트, 이미지)쌍 데이터를 활용하여 다양하고 복잡한 프롬프트를 따를 수 있게 한다.
            - 3D 렌더링에 대해 파인튜닝된 GLIDE 버전을 사용한다.
        - 본 논문의 image-to-3D 모델
            - 소규모의(이미지,3D)쌍 데이터로 학습된다.
            - RGB 포인트 클라우드를 생성하는 디퓨전 모델의 스택을 사용한다. (새로운 transformer 기반 아키텍처 사용)
            - 생성된 포인트 클라우드에서 메쉬를 생성하기 위해 회귀 기반(regression-based) 접근 방식을 사용한다.
    - 먼저 text-to-image 모델을 사용하여 이미지를 샘플링하고, 샘플링된 이미지를 조건으로 넣어 3D 객체를 샘플링한다.
    - 이 두 단계 모두 몇 초 내에 수행될 수 있으며, 비용이 많이 드는 최적화 과정을 필요로 하지 않는다.
- 본 논문의 결과
    - 간단한 텍스트 프롬프트뿐만 아니라 복잡한 텍스트 프롬프트와도 일치하는 컬러 3D 포인트 클라우드를 생성할 수 있었다.
    
    → 포인트 클라우드를 효율적으로 생성한다는 의미에서 본 논문의 시스템을 Point E라고 명칭하였다.
    

# 2. Background

- 디퓨전 모델 개요:
    - 점진적으로 Gaussian 노이즈를 추가하는 과정을 통해 데이터를 변형한다.
    - 본 논문에서는 Ho et al. (2020)의 Gaussian 확산 설정을 따른다.
- 노이즈 프로세스
    - 노이즈 프로세스는 시간 단계 t마다 신호에 Gaussian 노이즈를 추가한다.
    - 최종 단계에서는 샘플이 거의 정보를 포함하지 않게 된다.
- 역 노이즈 프로세스
    - 랜던 가우시안 노이즈 $x_T$에서 시작하여 점진적으로 노이즈 프로세스를 역으로 진행하여 잡음이 없는 샘플 $x_0$에 도달할 수 있다.
- 모델 학습
    - q(xt−1|xt)를 신경망 pθ(xt−1|xt)로 근사하여 학습한다.
    - Nichol & Dhariwal (2021)은 평균뿐만 아니라 분산도 예측하여 더 나은 성능을 얻었다.
- 샘플링
    - 디퓨전 샘플링은 미분 방정식 관점에서 설명될 수 있으며, 이를 통해 다양한 SDE 및 ODE 해석기를 사용하여 이러한 모델에서 샘플링할 수 있다.
    - 본 논문에서는 Karras et al. (2022)의 2차 ODE 해석기를 사용한다.
- 가이드 전략
    - Dhariwal & Nichol (2021)은 분류기 가이던스(classifier guidance)를 도입하여 생성 충실도를 높였다.
    - Ho & Salimans (2021)은 분류기 없는 가이던스(classifier-free guidance)를 도입하여 조건부 정보를 무작위로 삭제한다.
    - 본 논문에서는 학습 시 드롭 확률 0.1을 사용하여 이 기술을 적용한다.

# 3. Related Work

- 포인트 클라우드 생성 모델:
    - 다양한 연구들이 GAN, VAE, 플로우 모델, 디퓨전 모델 등을 사용하여 포인트 클라우드를 생성한다.
    - 본 논문의 연구와 가장 유사한 PVD 모델은 단일 디퓨전 모델을 사용하여 포인트 클라우드 직접 생성한다.
    - 본 논문에서는 더 간단한 트랜스포머 기반 아키텍처를 사용하며, RGB 채널을 함께 생성하는 점에서 차별점을 가진다.
- 다른 3D 표현을 사용한 모델 생성:
    - 2D 이미지 데이터셋에서 3D 인식 GAN을 학습하거나, 직접 3D 메쉬를 생성하는 연구들이  있다.
    - 이러한 연구들은 주로 새로운 뷰 합성 문제에 초점을 맞추지만, 전체 360도 뷰를 재구성하려고 하지는 않는다.
- 텍스트 조건부 3D 생성:
    - 몇몇 연구들은 텍스트-이미지 매칭 목표에 따라 3D 표현을 최적화하는 접근 방식을 탐구한다.
    - 이러한 접근 방식들은 다양한 복잡한 객체나 장면을 생성할 수 있지만, 최적화 절차가 매우 시간이 많이 걸린다.
- 텍스트-3D 데이터 활용:
    - 텍스트-3D 쌍 데이터를 사용하여 텍스트 조건부 3D 모델을 생성하는 연구들도 있다.
    - 많은 연구들이 단순한 프롬프트나 좁은 객체 카테고리에 한정되지만, 본 논문에서는 사전 학습된 텍스트-이미지 모델을 활용하여 이러한 문제를 해결한다.
- 이미지 기반 3D 재구성:
    - 단일 또는 소수의 이미지에서 3D 모델을 재구성하는 회귀 기반 및 생성 접근 방식이 있.
    - 이들 접근 방식은 불충분한 문제를 다루면서도 유망한 결과를 보여주고 있다.

# 4. Method

- 텍스트를 조건으로 받아 단일 생성 모델로 포인트 클라우드를 직접 생성하는 대신 생성 프로세스를 세 단계로 나눈다.
    1. ‘텍스트 캡션’을 조건으로 받아 ‘합성 뷰’를 생성한다.  → *4.2 내용*
        1. GLIDE 모델 사용
        2. 렌더링된 3D 모델로 파인튜닝
    2. ‘합성 뷰’를 조건으로 받아 ‘대략적인(coarse) 포인트 클라우드(1024개의 포인트)’를 생성한다 → *4.3 내용*
        1. 조건부 순열 불변 디퓨전 모델(conditional, permutation invariant diffusion model) 사용
    3. ‘저해상도 포인트 클라우드와 합성 뷰’를 조건으로 받아 ‘고해상도 포인트 클라우드(4096 포인트)’를 생성한다. → *4.4 내용*
        1. 2에서 사용된 모델과 유사하지만 저해상도 포인트 클라우드를 조건으로 하는 더 작은 디퓨전 모델을 사용 
- 수백만 개의 3D 모델과 관련 메타데이터로 구성된 데이터셋에서 모델을 훈련시킨다.
- 데이터셋을 렌더링된 뷰, 텍스트 설명, 그리고 각 점에 대한 RGB 색상을 포함하는 3D 포인트 클라우드로 처리한다.

## 4-1. Dataset

- 수백만 개의 3D 모델은 데이터 형식과 품질이 데이터셋 전반에 걸쳐 매우 다양했고, 더 높은 데이터 품질을 보장하기 위해 다양한 후처리 단계가 필요했다.
- 후처리 단계
    1. Blender를 사용하여 모든 데이터를 하나의 일반적인 형식(RGBAD 이미지)으로 변환한다.
        1. Blender: 다양한 3D 형식을 지원하며 최적화된 렌더링 엔진을 제공하는 프로그램
        2. RGBAD 이미지: RGB 이미지에 깊이(Depth)와 알파(Alpha) 채널이 추가된 형식의 이미지
        3. 20개의 랜덤한 카메라 각도에서 각 3D 모델을 경계 상자(bounding cube)로 정규화하고 표준 조명 설정을 구성한 후, blender의 내장된 실시간 렌더링 엔진을 사용하여 RGBAD 이미지를 내보냈다. 
    2. 각 객체를 렌더링을 사용해 색상이 있는 포인트 클라우드로 변환한다.
        1. 각 RGBAD 이미지의 각 픽셀에 대한 점을 계산하여 각 객체에 대한 밀집된(dense) 포인트 클라우드를 구성한다.  
        2. 이러한 포인트 클라우드는 일반적으로 고르게 분포되어 있지 않으므로, 가장 먼 점 샘플링을 사용하여 4K 점의 균일한 클라우드를 생성한다. 
        3. 렌더링에서 직접 포인트 클라우드를 구성함으로써, 3D 메쉬에서 직접 점을 샘플링할 때 발생할 수 있는 여러 가지 문제를 피할 수 있었다. (모델 내부에 포함된 점을 샘플링하는 문제, 이상한 파일 형식의 3D 모델로 인한 문제)
    3. 저품질 모델을 제거하기 위해 다양한 휴리스틱을 사용한다. 
        1. 각 포인트 클라우드의 SVD를 계산하고, 가장 작은 특이값이 일정 임계값(threshold) 이상인 경우에만 유지함으로써 평평한 객체를 제거했다.
        2. 다음으로, CLIP 특성에 따라 데이터셋을 클러스터링 했다. (일부 클러스터는 많은 저품질 모델 카테고리를 포함하는 반면, 다른 클러스터는 더 다양하거나 해석 가능한 것으로 나타났음)
        3. 클러스터를 여러 가지 품질의 버킷으로 나누고, 최종 데이터셋으로서 결과 버킷의 가중치 혼합을 사용했다. 

## 4.2 View Synthesis GLIDE Model

- ‘텍스트 캡션’을 조건으로 받아 ‘합성 뷰’를 생성하는 모델
- 4.3에서 설명할 포인트 클라우드 모델은 모두 동일한 렌더러와 동일한 조명 설정을 사용하여 생성된 데이터셋의 렌더링된 뷰를 조건으로 받는다.
- 따라서 해당 파트에서는 데이터셋의 분포와 일치하는 3D 렌더를 명시적으로 생성하고자 하였다.
- 이를 위해 GLIDE를 원래의 데이터셋과 저자들의 3D 렌더링 데이터셋을 혼합하여 파인튜닝 하였다.
    - 저자들의 3D 렌더링 데이터셋이 원래 GLIDE 학습셋에 비해 작기 때문에 3D 렌더링 데이터셋에서 이미지를 샘플링하는 비율을 5%로만 설정하고, 나머지 95%는 원래의 데이터셋을 사용했다.
    - 반복(iterations) 횟수는 100,000번의 설정 하였으며, 이는 모델이 3D 데이터셋을 여러 번 거치는 학습을 진행했음을 의미한다. (단, 동일한 렌더링된 시점을 두 번 사용하지 않았다.)
- 테스트 시간에는 항상 분포 내 렌더를 샘플링하기 위해, 모든 3D 렌더의 텍스트 프롬프트에 특별한 토큰을 추가하여 이 토큰을 사용하여 샘플링을 수행하였다.

## 4.3  **Point Cloud Diffusion**

- ‘합성 뷰’를 조건으로 받아 ‘대략적인(coarse) 포인트 클라우드(1024개의 포인트)’를 생성하는 모델
- 디퓨전을 이용해 포인트 클라우드를 사용하기 위해 [3D Shape Generation and Completion through Point-Voxel Diffusion](https://arxiv.org/abs/2104.03670)에서 사용한 프레임워크를 확장하여 포인트 클라우드의 각 포인트에 RGB 색상을 포함시켰다.
- 포인트 클라우드를 K x 6 형태의 텐서로 나타내며 (K: 포인트 수), 내부 차원은 (x,y,z) 좌표와 (R,G,B) 색상을 포함한다.
- 모든 좌표와 색상은 [-1, 1] 범위로 정규화 된다.
- K x 6 형태의 랜덤한 노이즈에서 시작하여 이를 점진적으로 디노이징하여 텐서를 직접 생성한다.
- 기존 3D 전용 구조를 활용하던 이전 방법들과 달리, 본 논문에서는 트랜스포머 기반 모델을 사용한다. 모델은
이미지, 타임 스텝 t, 노이즈가 있는 포인트 클라우드 $x_t$를 조건으로 받아 $\epsilon$과 $\sum$을 예측한다.
- 모델 구조
  
    :::{figure-md} 
    <img src="../../pics/Point_E/02.png" alt="Point_E_02" class="bg-primary mb-1">

    Point-E 모델 구조 
    :::
    
    - 포인트 클라우드의 각 포인트를 출력 차원이 D인 선형 레이어(linear layer)에 넣어 K×D 입력 텐서를 얻고 모델에 입력 컨텍스트로 사용한다. 또한 작은 MLP에 타임스텝 t를 넣어 컨텍스트 앞에 추가할 다른 D차원 벡터를 얻는다.
    - 이미지를 조건으로 입력 받기 위해, 사전 학습된 ViT-L/14 CLIP 모델에 이미지를 입력하고 이 CLIP 모델의 마지막 레이어의 임베딩을 가져온다. (shape: 256xD’), 이를 선형 투사(lienarly project)하여 256xD shape의 또 다른 텐서를 얻고 이를 트랜스포머 컨텍스트 앞에 추가한다. → 이 방법이 단일 CLIP 이미지 또는 텍스트 임베딩을 사용하는 것보다 우수했다.
    - 최종 입력 컨텍스트는 (K+257) x D의 shape가 된다. 길이 K의 최종 출력 시퀀스를 얻기 위해 최종 토큰 K개를 가져오고 이를 프로젝션하여 입력 포인트 K개에 대한 ε와 Σ 예측을 얻는다.

<aside>
💡 정리

- 입력 컨텍스트 구성:
    - 포인트 클라우드의 각 점: K×D
    - CLIP 이미지 임베딩: 256×D
    - 타임스텝 임베딩: 1×D
    
    → 최종 입력 컨텍스트: (K+257)×D
    
- 트랜스포머 모델의 출력: (K+257)개의 토큰 (각 토큰의 차원은 D)
- 최종 출력 시퀀스 선택: 최종 K개의 토큰을 가져온다.
- ε와 Σ 예측**:** 최종 K개의 토큰을 ε와 Σ 예측을 위한 입력 포인트로 사용한다.
- 예측된 ε와 Σ을 통해 노이즈를 제거하여 포인트 클라우드를 복원
</aside>

- 이 모델에서는 positional encoding을 사용하지 않는다. 따라서 모델 자체는  입력 포인트 클라우드에 대해 순열 분별(permutation-invariant)하다.

## 4.4 **Point Cloud Upsampler**

- 이미지 디퓨전 모델에서의 계층 구조
    - 이미지 디퓨전 모델의 경우 가장 좋은 품질은 일반적으로 계층 구조를 사용하는 방식으로 달성된다.
    - 이 방식에서는 저해상도의 기본 모델이 출력을 생성한 후, 이를 다른 모델이 업샘플한다.
    
    → 포인트 클라우드 생성에 이 접근 방식을 사용
    
- 포인트 클라우드 생성에서의 계층 구조
    - 큰 베이스 모델로 1K 포인트를 생성한 후, 작은 업샘플링 모델을 사용하여 4K 포인트로 업샘플링 한다.
    - 모델 크기가 같을 때, 4K 포인트를 생성하는 데에는 1K 포인트를 생성할 때보다 네 배 더 많은 연산을 필요로 한다.
- 업샘플러
    - 업샘플러는 베이스 모델과 동일한 아키텍처를 사용한다.
    - 모델은 저해상도 포인트 클라우드 모델과 동일한 아키텍처를 사용한다.
    - 저해상도 포인트 클라우드를 입력 받기 위한 추가 컨디셔닝 토큰이 있다.
    - 1K 포인트를 조건으로 입력 받아 추가로 3K 포인트를 생성하여 저해상도 포인트 클라우드에 추가한다.
    - $x_t$에 사용된 레이어가 아닌 별도의 선형 임베딩 레이어를 통해 저해상도 포인트를 전달하여, 모델이 positional encoding을 사용할 필요 없이 조건부 정보와 새로운 포인트를 구별할 수 있도록 한다.

## 4.5 Producing Meshes

- 렌더링 기반 평가를 위해 생성된 포인트 클라우드를 직접 렌더링하지 않는다.
- 대신, 포인트 클라우드를 텍스처가 입혀진 메쉬로 변환하고 Blender를 사용해 이러한 메쉬를 렌더링한다.
- 포인트 클라우드에서 메쉬를 생성하는 것은 때때로 어렵고, 본 논문의 모델이 생성한 포인트 클라우드는 종종 균열, 이상치 또는 기타 유형의 노이즈를 가지고 있어 더욱 어렵다.
- 포인트 클라우드에서 메쉬를 생성하기 위해 사전 학습된 SAP모델을 사용해봣으나 포인트 클라우드에 존재했던 큰 부분이나 중요한 세부 사항을 잃어버리는 경우가 있었다.
- 따라서 본 논문에서는 회귀(regression) 기반 모델을 사용하여 signed distance field를 예측하고, 이를 merching cube 알고리즘을 적용하여 메쉬를 추출했다.
- 그런 다음 원래 포인트 클라우드에서 가장 가까운 점의 색을 사용하여 메쉬의 각 버텍스에 색을 할당했다.

<aside>
💡

- 렌더링 과정 요약
    1. 포인트 클라우드에서 SDF 예측: 회귀 기반 모델을 사용하여 포인트 클라우드로부터 객체의 SDF를 예측한다.
    2. 메쉬 생성: 예측된 SDF를 기반으로 merching cube 알고리즘을 적용하여 메쉬를 생성합니다.
    3. 색상 할당: 생성된 메쉬의 각 버텍스에 원래 포인트 클라우드의 색상을 할당하여 텍스처가 입혀진 메쉬를 만든다.
    4. Blender를 통한 렌더링: 최종적으로 텍스처가 입혀진 메쉬를 Blender를 사용하여 렌더링한다. 
</aside>

# 5. Results

- 평가 지표: CLIP R-Precision, P-IS, P-FID
    - CLIP R-Precision
        - 특정 객체를 기준으로 하여 모델이 텍스트 설명과 얼마나 잘 일치하는지를 평가하는 지표
        - 계산하는 과정
            - 생성된 이미지와 텍스트 프롬프트를 기반으로 CLIP 모델을 사용하여 각 이미지의 텍스트 임베딩을 계산한다.
            - CLIP 모델에서 계산된 텍스트 임베딩과 이미지 임베딩 간의 유사도를 계산한다.
            - 유사도가 가장 높은 상위 R개의 이미지 중 실제로 맞는 이미지의 비율을 계산한다.
    - P-IS, P-FID
        - 포인트 클라우드의 Inception Score와 FID를 평가하기 위해 본 논문에서 도입한 지표
        - 수정된 PointNet++ 모델을 사용하여 포인트 클라우드에서 특징을 추출하고 클래스 확률을 에측

## 5.1 **Model Scaling and Ablations**

저자들은 다음과 같은 베이스 모델에 대하여 학습 중에 생성한 샘플들로 평가하였다.

- 40M (uncond.): 어떠한 조건 정보도 없는 작은 모델
- 40M (text vec.): 텍스트 캡션에만 의존하는 작은 모델 (이미지 사용 x), 파인튜닝된 GLIDE 모델 활용 x
- 40M (image vec.): 렌더링된 이미지의 CLIP 이미지 임베딩에 의존하는 작은 모델, 단일 CLIP 임베딩 사용
- 40M: CLIP 잠재 그리드(latent grid)를 통한 전체 이미지 조건을 사용하는 작은 모델
- 300M: CLIP 잠재 그리드를 통한 전체 이미지 조건을 사용하는 중간 모델
- 1B: CLIP 잠재 그리드를 통한 전체 이미지 조건을 사용하는 큰 모델

평가 결과는 아래 그래프와 같다.

:::{figure-md} 
<img src="../../pics/Point_E/03.png" alt="Point_E_03" class="bg-primary mb-1">

평가 결과
:::

- 결과
    - 텍스트 조건만 사용하고 텍스트에서 이미지로의 단계가 없는 경우 CLIP R-Precision이 매우 나쁘게 나오는 것을 발견
    - 이미지를 조건으로 사용할 때 단일 CLIP 임베딩보다 임베딩 그리드를 사용하는 것이 성능이 더 나은 것을 발견 →  조건 이미지에 대해 더 많은 (공간적인) 정보를 보는 것이 포인트 클라우드 모델에 이점이 있음을 시사
    - 모델의 스케일을 증가시키면 P-FID 수렴 속도가 향상되고 최종 CLIP R-Precision이 증가하는 것을 발견

## 5.2 Qualitative Results

- 포인트 클라우드 생성 결과
    :::{figure-md} 
    <img src="../../pics/Point_E/04.png" alt="Point_E_04" class="bg-primary mb-1">

    포인트클라우드 생성 결과
    :::

    
- Point·E 모델이 복잡한 프롬프트에 대해 종종 일관된 고품질의 3D 형상을 생성할 수 있다는 것을 발견했다.
- 때때로 포인트 클라우드 디퓨전 모델은 조건화된 이미지를 이해하지 못하거나 예측할 수 없는 경우가 있다.  이는 주로 두 가지 문제 중 하나로 인해 발생한다.
    
    :::{figure-md} 
    <img src="../../pics/Point_E/05.png" alt="Point_E_05" class="bg-primary mb-1">

    잘못 추론한 예시
    :::

    
    1. 모델이 이미지에 나타난 객체의 모양을 잘못 해석하는 경우
    2. 모델이 이미지에서 가려진 형상의 일부를 잘못 추론하는 경우

## **5.3 Comparison to Other Methods**

- CLIP-R-Precision 지표를 이용하여 Point·E를 다른 3D 생성 기술과 비교했다.
    
    :::{figure-md} 
    <img src="../../pics/Point_E/06.png" alt="Point_E_06" class="bg-primary mb-1">

    CLIP-R-Precision 성능
    :::
    
- Point·E는 state-of-the-art 기술(DreamFusion)보다 성능이 좋지 않지만, 이 불일치의 일부를 설명할 수 있는 이 평가의 두 가지 미묘한 점에 주목해야 한다.
    1. DreamFusion과 같은 멀티뷰 최적화 기반 방법과 달리 Point·E는 텍스트 프롬프트와 일치하도록 모든뷰를 명시적으로 최적화하지 않는다. 특정 객체가 모든 각도에서 쉽게 식별되지 않을 수 있기 때문에 CLIP R-Precision이 낮아질 수 있다.
    2. 본 논문의 방법은 렌더링 전에 포인트 클라우드를 전처리해야 하는데, 포인트 클라우드를 메쉬로 변환하는 것은 어려운 문제다. 본 논문이 사용하는 접근 방식은 때때로 포인트 클라우드 자체에 있는 정보를 잃을 수 있다.
- Point·E는 최신 테크닉보다 이 평가에서 성능이 좋지 않지만 짧은 시간 내에 샘플을 생성한다.
- 이를 통해 보다 실용적으로 응용 프로그램을 만들거나 많은 개체를 샘플링하고 최상의 개체를 휴리스틱을 따라 선택하여 고품질 3D 개체를 찾을 수 있다.

# **6. Limitations and Future Work**

- 합성 렌더링을 필요로 한다. → 향후 실제 세계 이미지를 조건으로 하는 3D 생성기를 훈련시켜 해결할 수 있을 것
- 색상이 있는 3D 형태를 생성하지만, 이 과정은 비교적 낮은 해상도의 3D 형식(포인트 클라우드)로 이루어진다. 형상이나 질감의 세부 사항을 캡처하지 못한다. → 메쉬나 NeRF와 같은 고해상도 3D 표현을 생성하도록 확장하면 해결할 수 있을 것
- 최적화 기반 기술(optimization-based techniques)을 초기화하여 초기 수렴 속도를 높이는 데 사용할 수 있다.
- 이 모델이 DALL·E 2 시스템과 많은 제한 사항을 공유할 것으로 예상한다. (데이터셋에서 야기된 많은 편향을 포함할 수 있다)
- 모델이 생성한 3D 모델이 실제로 물리적으로 제작될 때, 그 제품이 위험할 수 있는 물체의 청사진을 생성할 수 있다.
    
    :::{figure-md} 
    <img src="../../pics/Point_E/07.png" alt="Point_E_07" class="bg-primary mb-1">

    Figure 6 
    :::
    

# 7. Conclusion

- Point·E는 합성된 뷰를 생성하고 이를 기반으로 조건화된 색상 포인트 클라우드를 생성하는 텍스트 조건 합성 시스템이다.
- Point·E가 텍스트 프롬프트에 의해 조건화된 다양하고 복잡한 3D 형상을 효율적으로 생성할 수 있는 능력을 갖추고 있다는 것을 발견했다.
- 본 논문의 방식이 텍스트에서 3D로의 합성 분야에서의 추가적인 연구의 시작점으로 기여할 수 있기를 희망한다.