```{admonition} Information
- **Title:** {Zero-shot text-to-image generation}, {ICML 2021}

- **Reference**
    - Paper:  [https://arxiv.org/abs/2102.12092](https://arxiv.org/abs/2102.12092)
    - Code: [Unofficial-PyTorch](https://github.com/lucidrains/DALLE-pytorch)
    - Code: [Official](https://github.com/openai/DALL-E)
    
- **Author:** Donggeun "Sean" Ko

- **Last updated on June 22 2023**
```

# DALL-E

## 1. Introduction

- GPT-3 ê¸°ë°˜ ëª¨ë¸ì´ë©° 120ì–µê°œ parameter ìˆ˜ì™€ 2.5ì–µ ë°ì´í„° (text,image) setìœ¼ë¡œ í•™ìŠµ
- Autoregressive í•œ ëª¨ë¸ë§ì„ í†µí•˜ì—¬ imageì™€ textë¥¼ ì´ìš©í•˜ì—¬ text-to-image generation taskë¥¼ ìˆ˜í–‰
- 2021ë…„ ê¸°ì¤€ zero-shot SOTA performance ë‹¬ì„±
- ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ text inputì— ë”°ë¼ diverseí•œ ì´ë¯¸ì§€ ìƒì„±


:::{figure-md} 
<img src="../../pics/dalle/fig1.png" alt="fig1" class="bg-primary mb-1" width="700px">

Images generated using DALL-E
:::

:::{figure-md} 
<img src="../../pics/dalle/fig2.png" alt="fig2" class="bg-primary mb-1" width="700px">

Images generated using DALL-E
:::


## 2. Background
- GPT-3ì™€ VQ-VAEë¥¼ í™œìš©í•˜ì—¬ ë‚˜ì˜¨ ë…¼ë¬¸. 
- VQ-VAEë¥¼ ë¨¼ì € í•™ìŠµí•˜ê³ , Autoregressive Transformerì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ zero-shot architectureì„ êµ¬ì¶•.

### GPT-3
- Autoregressive Language Modelë©° few-shot learningì„ í†µí•´ fine-tuning ì—†ì´ ë†’ì€ ì„±ëŠ¥ì„ ëƒ„ *(fine-tuning ì„ í•  ìˆ˜ëŠ” ìˆì§€ë§Œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” task-agnostic performance ì— ì¤‘ì ì„ ë§ì¶° Few shotì„ í•¨) 
- GPT-3 ëŠ” transformerì—ì„œ decoder ë¶€ë¶„ë§Œ ì‚¬ìš© (GPT-2 ì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŒ )
- ì•½ 1750ì–µ parameter ê°œìˆ˜ì˜ ëª¨ë¸


:::{figure-md} 
<img src="../../pics/dalle/fig3.png" alt="fig3" class="bg-primary mb-1" width="700px">

Transformer ì•„í‚¤í…ì³ \ (source: https://arxiv.org/pdf/2005.14165.pdf)

:::

:::{figure-md} 
![GPT-3 GIF](../../pics/dalle/fig4.gif)

GPT 3 Animation \ (source: https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
:::


### VQ-VAE
- Encoderì—ì„œ ë‚˜ì˜¨ outputì€ discrete í•˜ë©° posterior ê³¼ prior ì´ categorical distributionì„ ê°–ëŠ”ë‹¤ê³  ê°€ì •í•¨.
- CNN (encoder) ì„ ê±°ì¹œ ê° Dì°¨ì›ì˜ ìœ„ì¹˜ì— $H \times W$ ê·¸ë¦¬ë“œë¡œ ì´ë¯¸ì§€ë¥¼ ë‚˜ëˆ„ê³  embedding space (Codebook) ì—ì„œ $ğ‘’_1$ë¶€í„° $ğ‘’_ğ‘˜$ ì¤‘ì—ì„œ ê°€ê¹Œìš´ 1ê°œ embedding codeë¡œ ë³€í™˜. 
- Quantization: Encoding output $z_{e}(x)$ representation ê³¼ ìœ ì‚¬í•œ codebook embedding $e_j$ ë¥¼ ì°¾ì•„ì„œ $k$ ê°’ì„ ë¶€ì—¬í•¨.

:::{figure-md} 
<img src="../../pics/dalle/fig5.png" alt="fig5" class="bg-primary mb-1" width="700px">

VQ-VAE ì•„í‚¤í…ì³, Loss í•¨ìˆ˜ \ (source: https://velog.io/@p2yeong/Understanding-VQ-VAE-DALL-E-Explained-Pt.-1)

:::



:::{figure-md} 
<img src="../../pics/dalle/fig6.png" alt="fig6" class="bg-primary mb-1" width="700px">

Quantization of VQ-VAE
:::



## 3. Methodology

## Limitation of Previous Works

1. Memory/Bottleneck Issue
- ê° Imageì—ì„œ ë‚˜ì˜¤ëŠ” pixelì„ ì§ì ‘ì ìœ¼ë¡œ image tokenì„ ì‚¬ìš©í•˜ë©´ ê³ í™”ì§ˆ ì´ë¯¸ì§€ì¼ìˆ˜ë¡ ë„ˆë¬´ ë§ì€ ë©”ëª¨ë¦¬ëŸ‰ì´ í•„ìš”í•´ì„œ â€œë¹„íš¨ìœ¨ì â€


2. Short-range dependence modeling between pixels
- Modelë“¤ ì¤‘ Likelihood functionì„ objective functionìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ short-range dependencyë¥¼ ìš°ì„ ì ìœ¼ë¡œ ë³¼ ê²ƒì´ë©° low-frequency ë³´ë‹¤ high-frequency detailì— ë”ìš± ì§‘ì¤‘í•˜ê²Œ ë¨.
- Low frequency ëŠ” visually recognizableí•´ì„œ ì‹œê°ì ìœ¼ë¡œ ë” ë„ì›€ì´ ë˜ëŠ” ë¶€ë¶„

ì´ 2ê°€ì§€ ë¬¸ì œì ì„ ê·¹ë³µí•˜ê³ ì Two-stage training process ì œì•ˆ


## DALL-E Overview
### Stage 1: Training VQ-VAE
- \textbf{Discrete VAE}ë¥¼ ì´ìš©í•˜ì—¬ $256 \times 256$ RGB image \rightarrow  $32 \times 32$ ì´ë¯¸ì§€ í† í°ìœ¼ë¡œ ì••ì¶• 
- ê° ì´ë¯¸ì§€ í† í°ì€ 8,192ê°œì˜ code ê°’ ì¤‘ì— í•˜ë‚˜ ë°°ì •
- ì´ë¯¸ì§€ì˜ \textbf{quality ì†ì‹¤ ì—†ì´} $8 \times 8 \times 3$ ë°° ë§Œí¼ context sizeë¥¼ ì ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŒ.


### Stage 2: Training an Autoregressive Transformer
- \textbf{ìµœëŒ€ 256 BPE-Encoded text tokens}ë“¤ê³¼ 1024 image tokens ($32 \times 32$) ë¥¼ ì—°ì†ì ìœ¼ë¡œ ì…ë ¥í•¨ (concatenate)
- Text tokenê³¼ Image Tokens ë“¤ì˜ joint distribution (ê²°í•© ë¶„í¬)ë¥¼ ëª¨ë¸ë§í•˜ì—¬ autoregressive transformerì„ í•™ìŠµ


## DALL-E Pipeline ì˜ˆì‹œ


:::{figure-md} 
<img src="../../pics/dalle/fig7.png" alt="fig7" class="bg-primary mb-1" width="700px">

DALL-E ì‹œê°í™” \ (source:https://jiho-ml.com/weekly-nlp-40/)
:::

:::{figure-md} 
<img src="../../pics/dalle/fig8.png" alt="fig8" class="bg-primary mb-1" width="700px">

DALL-E íŒŒì´í”„ë¼ì¸ \ (source:https://www.youtube.com/watch?v=CQoM0r2kMvI&t=1729s)
:::


## Methodology Details

### DALL-E Equations

:::{figure-md} 
<img src="../../pics/dalle/fig9.png" alt="fig9" class="bg-primary mb-1" width="700px">

equation 1
:::

:::{figure-md} 
<img src="../../pics/dalle/fig10.png" alt="fig10" class="bg-primary mb-1" width="700px">

equation 2: Maximizing ELBO
:::

x: images, y: captions , z: encoded RGB image tokens

**<span style="color: red;">ğ‘<sub>Î¦ (red)</sub></span>** : input imageì—ì„œ dVAE encoderì—ì„œ ìƒì„±í•œ 32 x 32 image tokenë¥¼ ì˜ˆì¸¡

**<span style="color: blue;">ğ‘<sub>ğœƒ (blue)</sub></span>**: image tokenì—ì„œ dVAE decoderì—ì„œ ìƒì„±í•œ RGB imageë¥¼ ì˜ˆì¸¡

**<span style="color: purple;">ğ‘<sub>Ïˆ (purple)</sub></span>**: transformer ëª¨ë¸ë¡œ ëª¨ë¸ë§í•œ textì™€ image tokenë“¤ì˜ ê²°í•© ë¶„í¬ (joint distribution)

### DALL-E í•™ìŠµê³¼ì • Stage 1: Learning the VIsual Codebook
- Transformerì„ ê³ ì •í•˜ê³  dVAE encoder & decoder (ğ‘_Î¦ , ğ‘_ğœƒ) ì„ í•™ìŠµí•¨ 
  - ì¦‰, ELB (Evidence Lower Boundë¥¼ maximize í•¨) 
  - K = 8,192 codebook (embedding space)ë¡œ ì„¤ì •


- \textbf{ELBë¥¼ optimize} í•˜ê¸° ìœ„í•´ì„œëŠ” discrete distributionì„ continuousë¥¼ ë°”ê¿”ì•¼ í•¨ 
  - í•™ìŠµì‹œì—ëŠ” ê²°êµ­, argmaxë¥¼ ì‚¬ìš©í•´ì„œ codebook vector ì¸ë±ìŠ¤ë¥¼ ì„ íƒí•˜ì—¬ ê³„ì‚°í•˜ë©´ Reparameterization gradientë¥¼ ì—°ì‚° X 
  - argmax ëŒ€ì‹  \textbf{gumbel softmax}ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ê²°

  - í‰ê°€ë¥¼ ì§„í–‰í•  ë•Œì—ëŠ” $z = codebook[\underset{i}{argmax}[g_i+log(q(e_i|x))]]$

- Gumbel Softmax Relaxationë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ê²°! $q_\phi \rightarrow q_{\phi}^{\tau}$, temperature $\tau \rightarrow 0$, relaxationì„ tightí•˜ê²Œ ì¡ì•„ì¤Œ.


### DALL-E í•™ìŠµê³¼ì • Stage 2: Learning the Prior
- Transformerì„ ê³ ì •í•˜ê³  dVAE encoder & decoder ($q_{phi}$ , $p_{\theta}$) transformerì˜ prior distribution $p_{\psi}$ë¥¼ í•™ìŠµí•¨. 
- ì´ë•Œ, $p_{\psi}$ì˜ ELBë¥¼ maximize í•˜ë©° 120ì–µê°œì˜ parameterë¥¼ ê°€ì§„ sparse transformer êµ¬ì¡°ë¥¼ ì‚¬ìš©í•¨

- Image tokenì€ dVAE Encoder logitì—ì„œ Argmax samplingì„ í†µí•´ ìƒì„±
- Text tokenì€ ì†Œë¬¸ìí™” í›„ 16,384 ê°œì˜ vocabularyë¥¼ BPE-encoding í†µí•´ í•œë²ˆì— ìµœëŒ€ 256 tokenì„ í™œìš©

:::{figure-md} 
<img src="../../pics/dalle/fig11.png" alt="fig11" class="bg-primary mb-1" width="700px">

Text-to-text attention: causal attention mask
Image-to-image attention: row/column/convolutional attention mask ì ìš©
:::


## Results
- ì¶”ë¡  ì‹œì—ëŠ” textì— ëŒ€í•˜ì—¬ Nê°œì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±. 
- Best of Nê°œëŠ” \textbf{Nê°œ ìƒì„± í›„ best}ë¥¼ ê³¨ë¼ì„œ ì„ íƒ í•¨.

- ìš°ìˆ˜í•œ ì´ë¯¸ì§€ë¥¼ ê³ ë¥´ê¸° ìœ„í•´ CLIP (Contrastive Language-Image Pretraining, 2021) ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ text ì™€ k ë²ˆì§¸ë¡œ similarity ì ìˆ˜ê°€ ë†’ì€ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•¨ (k=1)

:::{figure-md} 
<img src="../../pics/dalle/fig12.png" alt="fig12" class="bg-primary mb-1" width="700px">

DALL-E ê²°ê³¼ë¬¼. Bestë¥¼ ê³ ë¥¼ë•Œ N ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ì£¼ì–´ì§„ text promptë‘ ë” ìœ ì‚¬í•œ ê²°ê³¼ë¬¼ì´ ë‚˜ì˜´. 
:::

- ìƒì„±í•œ 512ê°œ ì´ë¯¸ì§€ ì¤‘ CLIP ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ similarity scoreì´ ì œì¼ ë†’ì€ ì´ë¯¸ì§€ë¥¼ ë½‘ìŒ.
- Ours (DALL-E) vs ë‹¤ë¥¸ baseline method ì™€ ë¹„êµ ì‹œ textì— ë”ìš± ì•Œë§ì€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆìŒ.


:::{figure-md} 
<img src="../../pics/dalle/fig13.png" alt="fig13" class="bg-primary mb-1" width="700px">

ì„ íƒí•˜ëŠ” ì´ë¯¸ì§€ ê°œìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ í–¥ìƒ
:::


- DF-GAN ì´ë‘ ë¹„êµí•´ì„œ MS-COCO datasetì— ëŒ€í•˜ì—¬ ì •ì„±ì  í‰ê°€ë¥¼ ì§„í–‰.
- Best-of-Five votes ì¤‘ì— DF-GANë³´ë‹¤ ë§¤ë²ˆ ì••ë„ì ì¸ ì°¨ì´ë¡œ íˆ¬í‘œ ìˆ˜ë¥¼ ë°›ì•˜ìŒ.


:::{figure-md} 
<img src="../../pics/dalle/fig14.png" alt="fig14" class="bg-primary mb-1" width="700px">

DF-GAN ì´ë‘ Qualitative Results ë¹„êµ
:::




- FID (Frechet Inception Distance)ëŠ” ê°’ì´ ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë©° / IS (Inception Score)ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
- MS-COCO ë‘ CUB (ìƒˆ íŠ¹í™” ë°ì´í„°ì…‹) ê¸°ì¤€, DALL-EëŠ” MS-COCOì—ì„œëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ìŒ. 
- CUBì—ì„œëŠ” SOTAë¥¼ ì°ì§€ ëª»í•˜ì˜€ê³  Inception scoreì—ì„œëŠ” ë‚®ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í•¨.
- ì €ìë“¤ì€ Fine-tuning ìœ¼ë¡œ CUBì— ì„±ëŠ¥ ê³„ì„ ì„ í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•¨.

:::{figure-md} 
<img src="../../pics/dalle/fig15.png" alt="fig15" class="bg-primary mb-1" width="700px">

MS-COCO ì™€ CUB datasetì—ì„œ FID/IS ê²°ê³¼ê°’ ë¹„êµ
:::

## Conclusion
- GPT-3ì˜ í™•ì¥ ëª¨ë¸ë¡œ 120ì–µê°œì˜ parameterê³¼ autoregressive Transformer (Decoder only) ê¸°ë°˜ ëª¨ë¸ë§ì„ í†µí•´ text-to-image generation taskë¥¼ ë›°ì–´ë‚˜ê²Œ í•´ê²°í•¨.
- Zero-shot learningì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ í›Œë¥­í•œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì„
- ì •ëŸ‰ì  / ì •ì„±ì  í‰ê°€ì—ì„œ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìœ¼ë©° ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ìƒì„±ì´ ê°€ëŠ¥í•¨.

** Limitations: **
- ìƒì„±í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ì— ë‹¤ì–‘í•œ ê°ì²´ê°€ í¬í•¨ë˜ë©´ ì–´ë ¤ì›€ì„ ê²ªìŒ 
- (b)ì— ë³´ë©´ ê³ ìŠ´ë„ì¹˜ê°€ 2ë§ˆë¦¬ê±°ë‚˜ ê°•ì•„ì§€ì™€ ê³ ìŠ´ë„ì¹˜ ë‘˜ë‹¤ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ìŠ¤ì›¨í„°ë¥¼ ì…ê³  ìˆìŒ.

- CUB dataset ì²˜ëŸ¼ ë‹¤ì†Œ ì•„ì‰¬ìš´ ì„±ëŠ¥ì„ ë³´ì¸ ë°ì´í„°ì…‹ì´ ìˆì§€ë§Œ fine-tuningìœ¼ë¡œ í•´ê²°


:::{figure-md} 
<img src="../../pics/dalle/fig16.png" alt="fig16" class="bg-primary mb-1" width="700px">

Limitationì„ ë³´ì—¬ì£¼ëŠ” ê²°ê³¼ë¬¼. 
:::
