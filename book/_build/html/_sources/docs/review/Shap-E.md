```{admonition} Information
- **Title:** Shap-E: Generating Conditional 3D Implicit Function

- **Reference**
    - Paper: [https:arxiv.org/abs/2305.02463](https:arxiv.org/abs/2305.02463)
    - Code: [https:github.com/openai/shap-e](https:github.com/openai/shap-e)

- **Author:** Kyeongmin Yu

- **Last updated on July. 18. 2024**
```

# Shap-E

:::{figure-md} 
<img src="../../pics/Shap_E/figure1.png" alt="figure1" class="bg-light mb-1" width="600px">

Shap-Eë¥¼ í†µí•´ ìƒì„±í•œ 3D assets
:::

# 0. Abstract
>ğŸ“Œ **ë…¼ë¬¸ìš”ì•½**\
> 2023ë…„ openaiì˜ [Heewoo Jun](https:arxiv.org/search/cs?searchtype=author&query=Jun,+H),Â [Alex Nichol](https:arxiv.org/search/cs?searchtype=author&query=Nichol,+A) ê°€ ë°œí‘œí•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. official codeëŠ” [github](https:github.com/openai/shap-e/tree/main)ì—ì„œ, diffusersë¥¼ í™œìš©í•œ ì½”ë“œëŠ” [huggingface](https:huggingface.co/docs/diffusers/en/api/pipelines/shap_e)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \
>**ëª©ì  -** ì¡°ê±´ë¶€ 3D assets ìƒì„± \
>**ìƒì„±ë°©ì‹ -** encoderë¥¼ í†µí•´ implicit functionì˜ parameter í˜•íƒœë¡œ í‘œí˜„í•œ í›„, ì´ë¥¼ diffusion modelì˜ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ conditional 3D assetsì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤.\
>**ì°¨ë³„ì  -** texture mesh ë‚˜ NeRF ëª¨ë‘ ìƒì„± ê°€ëŠ¥í•œ implicit functionì˜ parametersë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. (ë‹¤ë¥¸ 3D ìƒì„± ëª¨ë¸ì˜ ê²½ìš° ë‹¨ì¼ í‘œí˜„ë§Œ ê°€ëŠ¥í•œ ê²½ìš°ê°€ ë§ë‹¤ê³  í•©ë‹ˆë‹¤.)

# 1. Introduction

implicit neural representations (INRs)ëŠ” 3D assetsì„ ì¸ì½”ë”©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ëœë‹¤. 3D assetì„ í‘œí˜„í•˜ê¸° ìœ„í•´ INRsëŠ” ì£¼ë¡œ 3D coordinateë¥¼ location specific info(density, color)ë¡œ ë§µí•‘í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ INRsëŠ” í™”ì§ˆì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë° ì´ëŠ” ê³ ì •ëœ gridë‚˜ sequenceê°€ ì•„ë‹Œ arbitrary input pointsë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë•ë¶„ì— end-to-end ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë‹¤. INRsì€ ì´í›„ ë‹¤ì–‘í•œ downstream applicationsë„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 2ê°€ì§€ íƒ€ì…ì˜ INRsì„ ë‹¤ë£¬ë‹¤.

- **Neural Radiamce Field (NeRF)** - 3D sceneì„ function mappingìœ¼ë¡œ í‘œí˜„.
    - coordinate, viewing direction $\rightarrow$ density, colors along camera rays
- **textured 3D mesh** (DMTet, GET3D)
    - coordinate $\rightarrow$ colors, signed distances, vertex offsets
    - INRsëŠ” ì‚¼ê°ë©”ì‰¬ë¥¼ ìƒì„±í•  ë•Œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, 3D assets ìƒì„±ì— ê´€í•œ ë‹¤ì–‘í•œ ì—°êµ¬ê°€ ìˆì§€ë§Œ downstream applicationì—ì„œ ì‚¬ìš©í•˜ê¸° í¸í•œ í˜•íƒœë¡œ 3D assetsì„ í‘œí˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ëŠ” ë¶€ì¡±í•˜ë‹¤. ë³¸ ë…¼ë¬¸ì€ ë‹¨ì¼ representationìœ¼ë¡œ ë¶€í„° ë‘ê°€ì§€ í˜•íƒœë¡œ rendering ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.

# 2. Background

## 2.1 Neural Radiance Fields (NeRF)

Mildenhall et al. ëŠ” ì•„ë˜ì™€ ê°™ì´ NeRF(3D sceneì„ implicit functionìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•)ë¥¼ ì œì•ˆí–ˆë‹¤.

$$
F_{\Theta} : (\mathbf{x},\mathbf d)â†¦(\mathbf c,\sigma) \tag{1}
$$ 

$x$ ëŠ” 3D ê³µê°„ ì¢Œí‘œ, $d$ ëŠ” 3D ì‹œì•¼ ê°ë„, $c$ ëŠ” RGB, $\sigma$ ëŠ” density($\ge 0$) ì´ë‹¤. $F_\Theta$ ëŠ” í¸ì˜ë¥¼ ìœ„í•´ $\sigma(x)$ ì™€ $c(x,d)$ ë‘ê°œì˜ ì‹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‘œí˜„í–ˆë‹¤. 

ìƒˆë¡œìš´ ì‹œì•¼ì—ì„œ ë°”ë¼ë³¸ scene ì„ ë Œë”ë§í•˜ê¸° ìœ„í•´ì„œ, ì•„ë˜ì™€ ê°™ì´ ê° rayì— ë§ëŠ” colorê°’ì„ ê³„ì‚°í•œë‹¤.

$$
\hat C(\mathbf r)=\int^\infty_0 T(t)\sigma(\mathbf R(t))\mathbf c(\mathbf r(t),\mathbf d)dt, \space \text{where} \space T(t)=\text{exp}\Big(-\int^\infty_0 \sigma(\mathbf r(s))ds\Big) \tag{2}
$$

- ìˆ˜ì‹(2) ì„¤ëª…
    
    :::{figure-md}
    <img src="../../pics/Shap_E/IMG_4859.png" alt="figure1" class="bg-light mb-1" width="800px"> 

    ìˆ˜ì‹ (2) ë³´ì¶©ì„¤ëª…
    :::

ìœ„ì˜ ì ë¶„ì‹ì„ ì•„ë˜ì™€ ê°™ì´ discrete sumìœ¼ë¡œ ê°„ëµí™” í•  ìˆ˜ ìˆë‹¤. 

$$
\hat C(\mathbf r)=\sum^N_{i=1} T_i(1-\text{exp}(-\sigma(\mathbf r(t_i))\delta_t))\mathbf c (\mathbf r(t_i),\mathbf d), \space \text{where} \space T_i=\text{exp}\Big(-\sum^{i-1}_{j=1} \sigma(\mathbf r(t_j))\delta_j\Big) \tag{3}
$$

êµ¬ê°„ì„ ë‚˜ëˆ„ëŠ” ë°©ì‹ì€ ì¤‘ìš”í•œ ë¶€ë¶„ìœ¼ë¡œ coarseì™€ fine ë‘ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ë” ì„¸ë¶€ì ìœ¼ë¡œ sequenceë¥¼ ë‚˜ëˆˆë‹¤. 2ê°œì˜ NeRF ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ 2ë²ˆì˜ samplingì„ í•œë‹¤. 

$$
w_i \sim T_i(1-\text{exp}(-\sigma(\mathbf r(t_i))\delta_i))\tag{4}
$$

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” rayì˜ transmittanceë¥¼ ì•„ë˜ì™€ ê°™ì´ ì¶”ê°€ì ìœ¼ë¡œ ì •ì˜í•˜ì˜€ë‹¤. ì´ëŠ” ì§ê´€ì ìœ¼ë¡œ rayì˜ alphaê°’ì´ë‚˜ opacityì˜ ì´í•©ì— í•´ë‹¹í•œë‹¤.

$$
\hat T(\mathbf r)=1-\text{exp}\Big(-\sum^N_{i=1}\sigma(\mathbf r(t_i))\delta_i\Big)\tag{5}
$$

- ìˆ˜ì‹(5) ì„¤ëª…

    :::{figure-md} 
    <img src="../../pics/Shap_E/IMG_4860.png" alt="figure2" class="bg-light mb-1" width="400px">

    ìˆ˜ì‹ (5) ë³´ì¶© ì„¤ëª…
    :::
    
    
    

## 2.2 Signed Distance Functions and Texture Field (STF)

ë³¸ ë…¼ë¬¸ì—ì„œ STFëŠ” signed distancesì™€ texture colors ë‘ê°€ì§€ ëª¨ë‘ë¥¼ ìƒì„±í•˜ëŠ” implicit functionì„ ì˜ë¯¸í•œë‹¤. ì´ë²ˆ ì„¹ì…˜ì—ì„œëŠ” ì´ëŸ¬í•œ implicit functionì´ meshesë¥¼ êµ¬ì„±í•˜ê³  renderingì„ ë§Œë“œëŠ” ë°©ì‹ì„ ì„¤ëª…í•œë‹¤.

:::{figure-md} 
<img src="../../pics/Shap_E/IMG_4872.png" alt="figure3" class="bg-light mb-1" width="800px">

point cloud, voxel, polygon meshì˜ ë¹„êµ \
source - 3D Vision with Transformers: A Survey
:::

**Signed Distance Functions (SDFs)**ëŠ” 3D shapeì„ scaler fieldì—ì„œ í‘œí˜„í•˜ëŠ” ì „í†µì ì¸ ë°©ë²•ì¤‘ í•˜ë‚˜ë‹¤. íŠ¹íˆ SDF $f$ëŠ” coordinate $x$ë¥¼ scaler ë¡œ mappingí•œë‹¤. ($f(\mathbf x)=d$) ì—¬ê¸°ì„œ $d$ëŠ” íŠ¹ì • ìœ„ì¹˜ $x$ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ì˜ í‘œë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ë§í•œë‹¤. $d$ê°€ 0ë³´ë‹¤ ì‘ìœ¼ë©´ í•´ë‹¹ ë¬¼ì²´ ì™¸ë¶€ì„ì„ ì˜ë¯¸í•œë‹¤. ì´ëŸ¬í•œ ì •ì˜ì— ë”°ë¼ $f(\mathbf x)=0$ ì¼ë•ŒëŠ” ë¬¼ì²´ì˜ í‘œë©´ì„ ì˜ë¯¸í•œë‹¤. $\text{sign}(d)$ëŠ” í‘œë©´ì— ë”°ë¥¸ normal orientationì„ ì˜ë¯¸í•œë‹¤. 

- DMTet : SDFsë¥¼ í™œìš©í•˜ì—¬ 3D shapeì„ ìƒì„±í•˜ëŠ” ëª¨ë¸. coarse voxelì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ synthesized shape(SDF, tetrahedral)ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤.  DMTetì˜ ì¶œë ¥ì€ dense spatial gridì—ì„œì˜ ê° vertex $v_i$ë³„ SDF ê°’ $s_i$ì™€ displacement $\vartriangle v_i$ ì´ë‹¤. ì´í›„ ì„¤ëª… ìƒëµ

- GET3D : DMTetì— ì¶”ê°€ì ì¸ texture ì •ë³´ê¹Œì§€ë„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ë¬¼ì²´ì˜ í‘œë©´ì˜ ì§€ì  $p$ ë§ˆë‹¤ RGB colorë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë”°ë¡œ í•™ìŠµì‹œì¼œ textureë¥¼ ë§Œë“¤ì—ˆë‹¤. ì´í›„ ì„¤ëª… ìƒëµ

:::{figure-md} 
<img src="../../pics/Shap_E/IMG_4874.png" alt="figure4" class="bg-light mb-1" width="800px">

texture, bump, displacementì˜ ë¹„êµ \
source - [tutorials in grabcad](https:grabcad.com/tutorials/adding-textures-to-3d-models-texture-bump-and-displacement-mapping-how-to-make-photo-realistic-models)
:::

bumpëŠ” lighting ì„ ê³ ë ¤í•˜ì—¬ textureê°€ ë” ìì—°ìŠ¤ëŸ¬ì›Œ ì¡Œì§€ë§Œ êµ¬ì˜ í‘œë©´ì„ ë³´ë©´ ë¬¼ì²´ì˜ í˜•íƒœê°€ ì‹¤ì œë¡œ ë°”ë€ê²ƒì€ ì•„ë‹˜ì„ ì•Œìˆ˜ ìˆë‹¤.displacementë¥¼ ë³´ë©´ textureë¥¼ ë”°ë¼ ë¬¼ì²´ì˜ í‘œë©´ì´ ë³€í™”ëœê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

## 2.3 Diffusion Models

ë³¸ ë…¼ë¬¸ì—ì„œ í™œìš©í•œ diffusion modelì€ DDPMìœ¼ë¡œ diffusion process(noising process)ë¥¼ data sample $x_0$ ì— gaussian noiseë¥¼ ì„œì„œíˆ ì¶”ê°€í•˜ì—¬ ì™„ì „í•œ ë…¸ì´ì¦ˆê°€ ë˜ì–´ê°€ëŠ” ê³¼ì • $(x_1,x_2,â€¦x_T)$ ìœ¼ë¡œ í‘œí˜„í–ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ $x_T$ëŠ” gaussian noiseì™€ êµ¬ë¶„ë¶ˆê°€ëŠ¥í•œ ìƒíƒœë¡œ ìƒì •í•œë‹¤. í•´ë‹¹ ê³¼ì •ì€ sequentialí•˜ê²Œ ì§„í–‰ë˜ì§€ë§Œ í™œìš©ì‹œì—ëŠ” ì•„ë˜ì˜ ì‹ê³¼ ê°™ì´ íŠ¹ì • ë‹¨ê³„ë¡œ ë°”ë¡œ â€œjumpâ€í•˜ëŠ” ë°©ì‹ì„ ì´ìš©í•œë‹¤.

$$
x_t=\sqrt{\bar\alpha_t}x_0+\sqrt{1-\bar\alpha_t}\epsilon\tag{6}
$$

$\epsilon$ ì€ ëœë¤í•œ ë…¸ì´ì¦ˆë¥¼ ì˜ë¯¸í•˜ê³ , $\bar\alpha_t$ëŠ” ë‹¨ì¡°ê°ì†Œí•˜ëŠ” ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ì„ ì˜ë¯¸í•œë‹¤. ($t=0$ ì¼ë•ŒëŠ” sample dataê°€ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ $\bar\alpha_0=1$)

ëª¨ë¸ $\epsilon_\theta$ë¥¼ í•™ìŠµí• ë•ŒëŠ” ì•„ë˜ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. 

$$
L_{\text{simple}}=E_{x_0\sim q(x_0),\epsilon\sim\mathcal N(0,\mathbf I),t\sim U[1,T]}\|\epsilon -\epsilon_\theta (x_t,t)\|^2_2\tag{7}
$$

ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ë„ ìˆëŠ”ë° Shap-E ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì˜ ì‹ì„ í™œìš©í•˜ì˜€ë‹¤. ìœ„ëŠ” (ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ëŠ” ë…¸ì´ì¦ˆ, diffusion processì—ì„œ ë”í•´ì§„ ë…¸ì´ì¦ˆ)ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì•„ë˜ëŠ” (data sample $x_0$, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ë§Œë“  ì´ë¯¸ì§€)ì˜ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.

$$
L_{x_0}=E_{x_0\sim q(x_0),\epsilon\sim\mathcal N(0,\mathbf I),t\sim U[1,T]}\|x_\theta (x_t,t)-x_0\|^2_2\tag{8}
$$

denosingì‹œì—ëŠ” ë†’ì€ í€„ë¦¬í‹°ì™€ ì ë‹¹í•œ latencyë¥¼ ìœ„í•´ Heun samplerì™€ classifier-free guidanceë¥¼ ì‚¬ìš©í–ˆë‹¤. 

$$
\hat x_\theta(x_t,t|y)=x_\theta(x_T,t)+s\space\cdot\space (x_\theta(x_t,t|y)-x_\theta(x_t,t)) \tag{9}
$$

$s$ ëŠ” guidance scaleì´ê³  $s=0, s=1$ ì¼ë•ŒëŠ” regular unconditional, conditional samplingì„ ëœ»í•œë‹¤. $s$ ë¥¼ ë” í‚¤ìš°ë©´ ì¼ê´€ì„±(coherence)ì€ ì»¤ì§€ì§€ë§Œ ë‹¤ì–‘ì„±(diversity)ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ ë‚˜ì€ ê²°ê³¼ë¬¼ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” guidanceê°€ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œì•„ëƒˆë‹¤. (section 5ì˜ figure 4 ì°¸ê³ )

## 2.4 Latent Diffusion

continuous latent spaceì—ì„œë„ diffusionì„ í™œìš©í•˜ì—¬ ìƒ˜í”Œë“¤ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” Stable Diffusion(LDM)ì—ì„œ ì œì•ˆëœ ê²ƒìœ¼ë¡œ, pixel spaceì™€ latent spaceê°„ì˜ ë³€í™˜ì„ ë‹´ë‹¹í•˜ëŠ” encoderì™€ decoderë¥¼ ì¶”ê°€í•˜ì—¬ two-stageë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©´ ëœë‹¤. ì•ì„œ ë´¤ë˜ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ $\epsilon_\theta$ëŠ” latent spaceì—ì„œ ì¶”ê°€ëœ ë…¸ì´ì¦ˆ(latent noise)ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. original LDMì—ì„œëŠ” latent noiseë¥¼ ì›ë³¸ ì´ë¯¸ì§€ ë³´ë‹¤ ë‚®ì€ ë³µì¡ë„(lower-dimensional distribution)ë¥¼ ê°€ì§€ë„ë¡ KL penaltyë‚˜ vector quantization layerë¥¼ ì‚¬ìš©í–ˆë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œë„ ìœ„ì™€ ìœ ì‚¬í•œ ë°©ì‹ì„ ì‚¬ìš©í–ˆìœ¼ë‚˜ GAN-based objectiveì™€ perceptual lossë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ $L_1$, $L_2$ reconstruction lossë¥¼ ì‚¬ìš©í–ˆë‹¤. ë˜í•œ KL regularizationê³¼ vector quantizationì€ bottleneckì´ ë˜ë¯€ë¡œ ê³ ì •ëœ numerical rangeë¥¼ ê°€ì§€ë„ë¡ í•˜ê³  diffusion styleì˜ noiseë¥¼ ì¶”ê°€ í–ˆë‹¤.

# 3. Related Work

- Point-E
- 3D auto-encoder + implicit decoder
    - Fu et al. [16] - SDF sample gridë¥¼ encode, implicit decoderì˜ conditionìœ¼ë¡œ ì‚¬ìš©.
    - Sanghi et al. [54] - voxel gridë¥¼ encode, implicit occupancy networkì˜ conditionìœ¼ë¡œ ì‚¬ìš©.
    - Liu et al. [34] - voxel-based encoderì™€ implicit occupancy, color decoderë¥¼ í•™ìŠµ.
    - Kosiorek et al. [30] - rendered viewì„ encode, encodingëœ latent vectorë¥¼ NeRFì˜ conditionìœ¼ë¡œ ì‚¬ìš©.
    - Chen and Wang [6] - transformerê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ rendered viewì—ì„œ MLP parameterë¥¼ ê³§ë°”ë¡œ ìƒì„±.
- í•™ìŠµëœ encoder ì—†ì´ implicit 3D representationì„ ìƒì„±í•˜ëŠ”ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ëª¨ë¸ë“¤
    - Park et al. [43] - auto decoderë¥¼ í•™ìŠµ. ë°ì´í„°ì…‹ ë‚´ì˜ ê° ìƒ˜í”Œì˜ embedding vector tableì„ í•™ìŠµ.
    - Bautista et al. [4] - NeRF decoderë¥¼ ì¡°ê±´ìœ¼ë¡œ scene ë³„ latent codeë¥¼ í•™ìŠµ.
    - Dupont et al. [12] - implicit functionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ meta learning í™œìš©.
    - ErkocÌ§ et al. [14] - implicit MLP weightë¥¼ ê³§ë°”ë¡œ ìƒì„±í•˜ê¸° ìœ„í•´ diffusionì„ í™œìš©.
    - akin to [12] - NeRF parameter fittingì„ í•„ìš”ë¡œ í•¨.
    - Wang et al. [66] - ë°ì´í„°ì…‹ ë‚´ì˜ ê° ìƒ˜í”Œì˜ ê°œë³„ NeRFë¥¼ joint í•™ìŠµ.

# 4. Method


>ğŸ“Œ í›ˆë ¨ ë°©ë²• \
>two stage ë°©ì‹ìœ¼ë¡œ Shap-Eë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.\
>**Stage 1. train an encoder** \
>**Stage 2. train a conditional diffusion model on outputs of the encoder**


## 4.1 3D Encoder

:::{figure-md} 
<img src="../../pics/Shap_E/IMG_4861.png" alt="figure5" class="bg-light mb-1" width="800px">

3D Encoderì˜ êµ¬ì¡°
:::

**encoderì˜ input :**  (point clouds, rendered views) \
**encoderì˜ output :** MLPì˜ parameter

> ì…ë ¥ representationì˜ ì„¸ë¶€ íŠ¹ì„± \
> Point-Eì™€ ë¹„êµí•˜ì˜€ì„ë•Œ, post-processing ë°©ì‹ì„ ë³€ê²½í•˜ì—¬ 3D Assetë³„ ì‚¬ìš©í•˜ëŠ” RGB point cloudì˜ point ê°œìˆ˜ë¥¼ ëŠ˜ì´ê³ , ë” ë§ì€ viewë¥¼ 256x256 í¬ê¸°ë¡œ ë Œë”ë§ í•˜ì—¬ ì‚¬ìš©í–ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
> - Point Clouds: ê¸°ì¡´ 4K -> 16K
> - Multiview point clouds: ê¸°ì¡´ 20 views -> 60 views (20ê°œì˜ viewë¥¼ ì‚¬ìš©í•œ ê²½ìš° ìƒì„±ëœ pointcloudì— crackì´ ë°œìƒí–ˆë‹¤ê³  í•¨)\ view ë Œë”ë§ì‹œ ì¡°ëª…ê³¼ ë¬¼ì²´í‘œë©´ì˜ íŠ¹ì„±ì„ ê°„ëµí™”í–ˆë‹¤.


encoderì—ì„œ ì–»ì€ parameterëŠ” implicit functionì—ì„œ assetì˜ representationì„ ì˜ë¯¸í•œë‹¤. (+ì˜ë¯¸ìƒ ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì…ë ¥ë°›ì€ 3D assetì˜ íŠ¹ì„±ì„ ìœµí•©í•˜ì—¬ í•˜ë‚˜ë¡œ í‘œí˜„í•œ ê²ƒ, ë…¼ë¬¸ì˜ ì¥ì ìœ¼ë¡œ NeRFì™€ point cloud ëª¨ë‘ë¥¼ ì–»ì„ìˆ˜ ìˆë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ ìƒë‹¹íˆ ì˜ë„ê°€ ëŠê»´ì§€ëŠ” ì…ë ¥ìœ¼ë¡œ ë³´ì¸ë‹¤. )

:::{figure-md} 
<img src="../../pics/Shap_E/IMG_4869.png" alt="figure7" class="bg-light mb-1" width="800px">

pseudocode
:::

encoderì— ì…ë ¥ëœ point cloudsì™€ viewsëŠ” cross-attentionê³¼ transformer backboneì— ì˜í•´ ì²˜ë¦¬ë˜ì–´ sequence of vectorsê°€ ëœë‹¤. ì´í›„ latent bottleneckê³¼ projection layerë¥¼ í†µê³¼í•˜ì—¬ MLP weight matricesë¥¼ ë§Œë“ ë‹¤.

encoderëŠ” NeRF rendering objectiveë¥¼ ì‚¬ìš©(Section 4.1.1 ì°¸ê³ )í•˜ì—¬ ì‚¬ì „ í•™ìŠµí•œë‹¤. mesh-based objectiveë¥¼ ì´ìš©í•œ ì‚¬ì „í•™ìŠµì‹œ ë³´ë‹¤ ë” ì•ˆì •ì ì¸ ê²°ê³¼ë¬¼ì„ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤ê³  í•œë‹¤. ì´í›„ì—ëŠ” SDFì™€ texture color predictionì„ ìœ„í•´ ì¶”ê°€ì ì¸ output headë¥¼ ë„£ì–´ Section 4.1.2ì™€ ê°™ì´ two-stage ë°©ì‹ìœ¼ë¡œ headë“¤ì„ í•™ìŠµì‹œí‚¨ë‹¤. 

### 4.1.1 Decoding with NeRF Rendering

original NeRFì˜ ì‹ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ coarse netê³¼ fine netì´ parameterë“¤ì„ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ í•˜ì§€ëŠ” ì•Šì•˜ë‹¤. ëœë¤í•œ 4096ê°œì˜ rayë¥¼ ê° í•™ìŠµ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§í•˜ì˜€ìœ¼ë©°, $L_1$ lossê°€ ìµœì†Œê°€ ë˜ë„ë¡ í–ˆë‹¤. (original NeRFì—ì„œëŠ” $L_2$ lossë¥¼ ì‚¬ìš©)

$$
L_{\mathbf{RGB}}=E_{\mathbf r\in R}[\|\hat C_c(\mathbf r)-C(\mathbf r)\|_1+\|\hat C_f(\mathbf r)-C(\mathbf r)\|_1] \tag{10}
$$

ì—¬ê¸°ì— ì¶”ê°€ì ìœ¼ë¡œ ê° rayì˜ transmittanceì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì¶”ê°€í–ˆë‹¤. íŠ¹íˆ, í•œ rayì˜ density ì ë¶„ê°’(integrated density)ì„ í†µí•´ ì–»ì€transmittanceë¡œ coarse renderingê³¼ fine renderingì‹œ $\hat T_c(r)$ ì™€ $\hat T_f(r)$ë¥¼ ì˜ˆì¸¡í•˜ì˜€ë‹¤. ground truthë¡œëŠ” gt renderingê²°ê³¼ì˜ alpha channelì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì´ ì†ì‹¤í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. (+NeRFì˜ ê²½ìš° novel viewë¥¼ ë§Œë“œëŠ” ê²ƒì´ ëª©ì ì´ì—ˆìœ¼ë‚˜ ë³¸ ë…¼ë¬¸ì€ meshë„ ìƒì„±í•´ì•¼ í•˜ë¯€ë¡œ ë…¸ì´ì¦ˆ ì œê±°ê°€ ë”ìš± ì¤‘ìš”í•˜ì˜€ì„ ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤.)

$$
L_T=E_{\mathbf r\in R}[\|\hat T_c(\mathbf r)-T(\mathbf r)\|_1 +\|\hat T_f(\mathbf r)-T(\mathbf r)\|_1]\tag{11}
$$

ìµœì¢…ì ìœ¼ë¡œëŠ” ë‘ ì†ì‹¤í•¨ìˆ˜ë¥¼ í•©í•˜ì—¬ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤.

$$
L_\text{NeRF}=L_\text{RGB}+L_T \tag{12}
$$

### 4.1.2 Decoding with STF Rendering

:::{figure-md} 
<img src="../../pics/Shap_E/IMG_4874.png" alt="figure8" class="bg-light mb-1" width="800px">

texture, bump, displacementì˜ ë¹„êµ \
source - https:grabcad.com/tutorials/adding-textures-to-3d-models-texture-bump-and-displacement-mapping-how-to-make-photo-realistic-models
:::

NeRF ë°©ì‹ì„ í†µí•´ ì‚¬ì „í•™ìŠµí•œ í›„, MLPsì— STF output headsë¥¼ ì¶”ê°€í•œë‹¤. ì´ëŸ¬í•œ MLPsëŠ” SDFì™€ texture colorë¥¼ ì˜ˆì¸¡í•œë‹¤. triangle meshë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ê° vertexì˜ SDFë¥¼ regular $128^3$ gridë¡œ ì˜®ê²¨ ë¯¸ë¶„ê°€ëŠ¥í•œ í˜•íƒœì˜ Marching Cubeë¥¼ ì§„í–‰í•´ì•¼ í•œë‹¤. ì´í›„ texture colorëŠ” ìµœì¢… meshì˜ ê° vertex texture color headë¥¼ í†µí•´ ì–»ëŠ”ë‹¤. Pytorch 3Dë¥¼ í™œìš©í•˜ë©´ ë¯¸ë¶„ê°€ëŠ¥í•œ renderingì„ í†µí•´ textured meshë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ë Œë”ë§ ì‹œì—ëŠ” ë°ì´í„°ì…‹ êµ¬ì¶•ì‹œ preprocessingì— ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ lighting ì¡°ê±´ì„ ì‚¬ìš©í–ˆë‹¤.

ì‚¬ì „ ì‹¤í—˜ì‹œ ëœë¤ ì´ˆê¸°í™”ëœ STF output headsë¥¼ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” ê²°ê³¼ê°€ ë¶ˆì•ˆì • í–ˆìœ¼ë©°, rendering based objectiveë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì–´ë ¤ì› ë‹¤. í•´ë‹¹ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ SDFì™€ texture colorë¥¼ í•´ë‹¹ output headsë¥¼ ì§ì ‘ í•™ìŠµì‹œí‚¤ê¸° ì „ì— distill ì ‘ê·¼ë²•ì„ ì‚¬ìš©í–ˆë‹¤. Point-Eì˜ regression modelì„ í™œìš©í•˜ì—¬ ì…ë ¥ ì¢Œí‘œë¥¼ ëœë¤í•˜ê²Œ ìƒ˜í”Œë§í•˜ê³ , SDF distillation targetì„ êµ¬í–ˆë‹¤. ê·¸ë¦¬ê³  RGB targetë¡œëŠ” asset RGB point cloudì—ì„œ íŠ¹ì •ìœ„ì¹˜ $x$ì™€ ê°€ì¥ ê°€ê¹Œìš´(nearest neighbor) pointì˜ ìƒ‰ì„ ì‚¬ìš©í–ˆë‹¤. distillation training ì‹œ distillation lossì™€ NeRF lossë¥¼ ë”í•˜ì—¬ ì‚¬ìš©í–ˆë‹¤. 

$$
L_\text{distill}=L_\text{NeRF}+E_{\mathbf x\sim U[-1,1]^3}[\|\text{SDF}_\theta(\mathbf x)-\text{SDF}_\text{regression}(\mathbf x)\|_1+\|\text{RGB}_\theta(\mathbf x)-\text{RGB}_\text{NN}(\mathbf x)\|_1]
\tag{13}
$$

STF output headsê°€ distillationì„ í†µí•´ ì ì ˆí•œ ì´ˆê¸°ê°’ì„ ê°–ê²Œëœ í›„, NeRF encoderì™€ STF rendering ì „ì²´ë¥¼ end-to-endë¡œ fine-tuneí•œë‹¤. ì‹¤í—˜ì ìœ¼ë¡œ STF renderingì—ëŠ” $L_1$ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¶ˆì•ˆì •í–ˆìœ¼ë¯€ë¡œ $L_2$ ì†ì‹¤í•¨ìˆ˜ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì´ëŸ¬í•œ rendering ë°©ì‹ì— ì ì ˆí•¨ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. STF renderingì— ì‚¬ìš©í•œ lossëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

$$
L_\text{STF}=\frac{1}{N\space \cdot\space s^2}\sum^N_{i=1}\|\text{Render}(\text{Mesh}_i)-\text{Image}_i\|^2_2\tag{14}
$$

+ meshë¥¼ ë Œë”ë§í•œ ì´ë¯¸ì§€ì™€ target ì´ë¯¸ì§€ì˜ L2 reconstruction lossì˜ í‰ê· 

Nì€ ì´ë¯¸ì§€ ê°œìˆ˜, sëŠ” ì´ë¯¸ì§€ì˜ í™”ì§ˆ, $\text{Mesh}_i$ëŠ” $\text{sample}_i$ì˜ constructed meshë¥¼ ë§í•œë‹¤. $\text{Image}_i$ëŠ” RGBA renderingëœ ê²°ê³¼ë¬¼ë¡œ alphaì±„ë„ì„ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì— transmittanceì— ëŒ€í•œ lossë¥¼ ë”°ë¡œ ì¶”ê°€í•˜ì§€ ì•Šì•˜ë‹¤.

ìµœì¢… fine-tuning ë‹¨ê³„ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ë”í•œ objective functionì„ ì‚¬ìš©í•œë‹¤.

$$
L_\text{FT}=L_\text{NeRF}+L_\text{STF}\tag{15}
$$

## 4.2 Latent Diffusion

Point-Eì˜ transformer ê¸°ë°˜ diffusion êµ¬ì¡°ë¥¼ ì±„íƒí–ˆë‹¤. í•˜ì§€ë§Œ point cloudë¥¼ latent vectorì˜ sequenceë¡œ ë°”ê¾¸ì—ˆë‹¤. latent sequencesì˜ í¬ê¸°ëŠ” $1024\times1024$ ë¡œ ì´ë¥¼ ê¸¸ì´ê°€ 1024ì¸ 1024ê°œì˜ tokenì²˜ëŸ¼ transformerì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ê° tokenì€ MLP weight matricesì˜ ê° rowì™€ ì¼ì¹˜í•œë‹¤. Shap-Eì˜ ëª¨ë¸ì€ Point-E base ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ë¶€ë¶„ì´ ë§ë‹¤.(context lengthì™€ widthê°€ ë™ì¼) í•˜ì§€ë§Œ ë” ê³ ì°¨ì›ì˜ ìƒ˜í”Œ(samples in higher-dimensional)ì„ ìƒì„±í•˜ëŠ”ë° ì´ëŠ” ì…ì¶œë ¥ ì±„ë„ì˜ ë³µì¡ë„(dimension)ê°€ ì¦ê°€í•˜ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.

Point-Eì˜ conditioning ë°©ì‹ì„ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì˜€ë‹¤. ì´ë¯¸ì§€ ì¡°ê±´ë¶€ 3d ìƒì„±ì‹œ 256-token CLIP embedding sequenceë¥¼ transformer contextë¡œ ì‚¬ìš©í–ˆìœ¼ë©°, í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ 3d ìƒì„±ì‹œ single tokenì„ ì‚¬ìš©í–ˆë‹¤. 

Point-Eì™€ì˜ ì°¨ì´ì ìœ¼ë¡œëŠ” diffusion modelì˜ ì¶œë ¥ì„ $\epsilon$ predictionìœ¼ë¡œ parameterizeí•˜ì§€ ì•Šì•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ëŒ€ì‹  ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê³§ë°”ë¡œ sampleì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆë‹¤. ëŒ€ìˆ˜ì ìœ¼ë¡œëŠ” ë™ì¼í•œ ì˜ë¯¸ì´ë‚˜ ì´ˆê¸° ì‹¤í—˜ì—ì„œ ë” ì¼ê´€ëœ ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ì—¬ í•´ë‹¹ ë°©ì‹ì„ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•¨. 

## 4.3 Dataset

ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ ëŒ€ë¶€ë¶„ì˜ ì‹¤í—˜ì—ì„œ Point-Eì™€ ë™ì¼í•œ 3D assetsì„ ì‚¬ìš©í–ˆë‹¤. í•˜ì§€ë§Œ post-processingë¶€ë¶„ì—ì„œëŠ” ì°¨ì´ê°€ ìˆë‹¤. 

- point cloud ê³„ì‚°ì‹œ, 20ê°œê°€ ì•„ë‹Œ 60ê°œì˜ viewë¥¼ renderingí–ˆë‹¤. 20ê°œë§Œ ì‚¬ìš©í–ˆì„ë•Œ ì£¼ì–´ì§„ viewì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ì˜ì—­ë•Œë¬¸ì— crack ë°œìƒ (+NeRF ë•Œë¬¸ìœ¼ë¡œ ì¶”ì •)
- point cloudë¥¼ 4K ê°€ì•„ë‹Œ 16Kì˜ pointë¡œ ë§Œë“¤ì—ˆë‹¤.
- encoderí•™ìŠµì„ ìœ„í•œ viewë¥¼ ë Œë”ë§ í• ë•Œ ë‹¨ìˆœí•œ ì†Œì¬ì™€ ë¼ì´íŒ…ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. íŠ¹íˆ ëª¨ë“  ëª¨ë¸ì€ ë™ì¼í•œ ê³ ì •ëœ ë¼ì´íŒ… ì¡°ê±´ë‚´ì—ì„œ ë Œë”ë§ ë˜ì—ˆë‹¤. ambientì™€ diffuse shadingë§Œ ì‚¬ìš© (+ë°˜ì‚¬ê´‘ì´ ê³ ë ¤ë˜ì§€ ì•Šì•„ í‘œë©´ì´ ë§¤ëˆí•œ ë¬¼ì²´ëŠ” ìƒì„±í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒìœ¼ë¡œ ì¶”ì •ë¨)

    :::{figure-md}
    <img src="../../pics/Shap_E/untitled.png" alt="figure7" class="bg-light mb-1" width="800px">

    Phong model \
    ê¸°ë³¸ì ì¸ shadingë°©ì‹ìœ¼ë¡œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” specularë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ \
    source - [Realistic_Visualisation_of_Endoscopic_Surgery_in_a_Virtual_Training_Environment](https:www.researchgate.net/publication/265514880_Realistic_Visualisation_of_Endoscopic_Surgery_in_a_Virtual_Training_Environment)
    :::

text ì¡°ê±´ë¶€ ëª¨ë¸ê³¼ í•´ë‹¹ Point-E baselineì„ ìœ„í•´ ë°ì´í„° ì…‹ì„ ë”ìš± í™•ì¥í–ˆë‹¤. ì´ ë°ì´í„° ì…‹ì„ ìœ„í•´ ëŒ€ëµ 100ë§Œê°œì˜ 3D assetsê³¼ 12ë§Œê°œì˜ (human labeled)captionì„ ì¶”ê°€ë¡œ ìˆ˜ì§‘í–ˆë‹¤. 

# 5. Result

## 5.1 Encoder Evaluation

:::{figure-md}
<img src="../../pics/Shap_E/table1.png" alt="figure8" class="bg-light mb-1" width="800px">

ê° ìŠ¤í…Œì´ì§€ ë³„ í›ˆë ¨ ì´í›„ encoder ì„±ëŠ¥í‰ê°€
:::


distillationì—ì„œ rendering ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ê°€ ë–¨ì–´ì§€ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ë‚˜ finetuningì‹œ í€„ë¦¬í‹°ê°€ ë”ìš± ì¢‹ì•„ì§„ë‹¤. ë˜í•œ STFì˜ í€„ë¦¬í‹° ë˜í•œ í¬ê²Œ ìƒìŠ¹í•œë‹¤. 

## 5.2 Comparison to Point-E

:::{figure-md}
<img src="../../pics/Shap_E/figure4.png" alt="figure9" class="bg-light mb-1" width="800px">

Shap-Eì™€ Point-Eë¹„êµ\
ì„¸ëª¨ ë§ˆí¬ê°€ Point-E, ì›í˜• ë§ˆí¬ê°€ Shap-Eì´ë‹¤.
:::

point-Eë³´ë‹¤ Shap-Eì˜ CLIP scoreê°€ ë” ë†’ë‹¤. ë” ë§ì€ìˆ˜ì˜ parameterë¥¼ ê°€ì§„ point-Eë¥¼ ì‚¬ìš©í•˜ì—¬ë„ Shap-Eì˜ ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨. 

ë‘ í‰ê°€ ì§€í‘œ ëª¨ë‘ OpenAIì˜ CLIP (Contrastive Language-Image Pretraining) ëª¨ë¸ì„ í™œìš©í•œ í‰ê°€ ì§€í‘œë¡œ CLIP scoreì˜ ê²½ìš° ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì™€ ìƒì„±ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ê²ƒì´ê³ , CLIP R precisionì˜ ê²½ìš° ìƒì„±ê²°ê³¼ì™€ ì°¸ì¡° ì´ë¯¸ì§€ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ í‰ê°€í•˜ê¸° ìœ„í•œ ê²ƒì´ë‹¤.

:::{figure-md}
<img src="../../pics/Shap_E/figure5.png" alt="figure10" class="bg-light mb-1" width="800px">

Shap-Eì™€ Point-Eë¹„êµ
:::
ë™ì¼í•œ base modelì˜ í¬ê¸° ë™ì¼í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ê²°ê³¼. í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ ìƒì„±ì‹œì—ëŠ” í€„ë¦¬í‹° ì°¨ì´ê°€ í¬ì§€ ì•ŠìŒ.

:::{figure-md}
<img src="../../pics/Shap_E/figure6.png" alt="figure11" class="bg-light mb-1" width="800px">

Shap-Eì™€ Point-Eë¹„êµ
:::
ì´ë¯¸ì§€ ì¡°ê±´ë¶€ ìƒì„±ì‹œì—ëŠ” ë¹„êµì  ì°¨ì´ê°€ í¬ë‹¤. 
ë²¤ì¹˜ ê²°ê³¼ë¥¼ ë³´ë©´ point-Eì—ì„œ ë‚˜ë¬´ì‚¬ì´ ë¹ˆê³µê°„ì„ ë¬´ì‹œí•´ë²„ë¦°ê²ƒì„ ë³¼ìˆ˜ ìˆë‹¤. 
ìœ„ì˜ ê°•ì•„ì§€ì™€ ì»µ ì´ë¯¸ì§€ ê¸°ë°˜ ìƒì„± ê²°ê³¼ë¥¼ ë³´ë©´ point-Eì™€ shap-Eê°€ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ì—ì„œ ì‹¤íŒ¨í•˜ëŠ” ëª¨ìŠµì„ ë³´ì˜€ë‹¤. 

## 5.3 Comparison to Other Methods
:::{figure-md}
<img src="../../pics/Shap_E/table2.png" alt="figure12" class="bg-light mb-1" width="800px">

COCO ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ë¹„êµê²°ê³¼
:::
reference latencyì—ì„œ point-Eì™€ Shap-Eì˜ ì°¨ì´ê°€ ìˆëŠ”ë°, ì´ëŠ” Shap-EëŠ” ì¶”ê°€ì ì¸ upsampling diffusion modelì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

# 6. Limitations and Future Work

:::{figure-md}
<img src="../../pics/Shap_E/figure7.png" alt="figure13" class="bg-light mb-1" width="800px">

í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ ìƒì„± ê²°ê³¼
:::
ì™¼ìª½ ê·¸ë¦¼ê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ íŠ¹ì„±ì„ ê°€ì§„ë¬¼ì²´ë¥¼ ìƒì„±í•˜ëŠ”ë°ì— ì–´ë ¤ì›€ì„ ê²ªëŠ” ëª¨ìŠµì„ ë³´ì¸ë‹¤. ì´ëŠ” í•™ìŠµì— ì‚¬ìš©í•œ paired dataê°€ ì œí•œì ì´ê¸° ë•Œë¬¸ìœ¼ë¡œ ë” ë§ì€ 3D datasetì„ ìˆ˜ì§‘í•˜ë©´ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆë‹¤. ë˜í•œ textureì˜ ì„¸ë¶€ íŠ¹ì„±ì„ encoderê°€ ë¬´ì‹œí•˜ëŠ” ê²½ìš°ë„ ìˆëŠ”ë°, ë” ë‚˜ì€ encoderë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê°œì„ ë ìˆ˜ ìˆë‹¤.

Shap-EëŠ” ë‹¤ì–‘í•œ 3D ìƒì„± ê¸°ìˆ ë“¤ì„ ìœµí•©í•˜ëŠ”ë°ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Shap-Eë¡œ ìƒì„±í•œ NeRFì™€ meshë¥¼ ë‹¤ë¥¸ ìµœì í™” ê¸°ë°˜  ëª¨ë¸ì„ ì´ˆê¸°í™” í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ ë” ë¹ ë¥¸ ìˆ˜ë ´ë„ ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤. 

# 7. Conclusion

Shap-EëŠ” latent diffusion modelì„ 3D implicit functionê³µê°„ì—ì„œ ì „ê°œí•˜ì—¬ NeRFì™€ textured mesh ëª¨ë‘ë¥¼ ìƒì„± í•  ìˆ˜ ìˆì—ˆë‹¤. ë™ì¼í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ë‹¤ë¥¸ ìƒì„±ëª¨ë¸ë“¤ê³¼ ë¹„êµí•˜ì˜€ì„ë•Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í–ˆë‹¤. ë˜í•œ text ì¡°ê±´ë¶€ ìƒì„±ì‹œ ì´ë¯¸ì§€ ì—†ì´ë„ ë‹¤ì–‘í•œ í¥ë¯¸ë¡œìš´ ë¬¼ì²´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŒí™•ì¸í–ˆë‹¤. ì´ëŠ” implicit representionì„ ìƒì„±í•¨ì— í° ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤€ë‹¤.

# 8. Acknowledgements

íŠ¹ì • ì¸ë¬¼ë“¤ì— ëŒ€í•œ ì–¸ê¸‰ ì™¸ì—ë„ ChatGPTë¡œ ë¶€í„° valuable writing feedbackì„ ë°›ì•˜ë‹¤ê³  í‘œí˜„í•œ ë¶€ë¶„ìˆì—ˆë‹¤.