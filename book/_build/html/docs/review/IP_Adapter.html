

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>&lt;no title&gt; &#8212; Text-to-Image Generation-feat-Diffusion</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/review/IP_Adapter';</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="HyperDreamBooth" href="HyperDreamBooth.html" />
    <link rel="prev" title="T2I-Adapter" href="t2i_adapter.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/PseudoLab_logo.png" class="logo__image only-light" alt="Text-to-Image Generation-feat-Diffusion - Home"/>
    <script>document.write(`<img src="../../_static/PseudoLab_logo.png" class="logo__image only-dark" alt="Text-to-Image Generation-feat-Diffusion - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to PseudoDiffusers!!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary Works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="DDPM.html">DDPM</a></li>




<li class="toctree-l1"><a class="reference internal" href="DDIM.html">DDIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Study_on_the_Evaluation_of_Generative_Models.html">A Study on the Evaluation of Generative Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cycleGAN.html">CycleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyleGAN.html">StyleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_beats_GANs.html">Diffusion Models Beat GANs on Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalle.html">DALL-E</a></li>
<li class="toctree-l1"><a class="reference internal" href="DALLE2.html">DALL-E 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="dreambooth.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="ControlNet.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Latent_Diffusion_Model.html">Introduction</a></li>



<li class="toctree-l1"><a class="reference internal" href="Textual_Inversion.html">Textual Inversion</a></li>








<li class="toctree-l1"><a class="reference internal" href="CustomDiffusion.html">Custom Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="LoRA.html">LoRA</a></li>









<li class="toctree-l1"><a class="reference internal" href="I-DDPM.html">I-DDPM</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyO.html">StyO</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen_editor.html">Imagen Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDEdit.html">SDEdit</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDXL.html">SDXL</a></li>
<li class="toctree-l1"><a class="reference internal" href="t2i_adapter.html">T2I-Adapter</a></li>
<li class="toctree-l1"><a class="reference internal" href="HyperDreamBooth.html">HyperDreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="CM3leon.html">CM3leon</a></li>

<li class="toctree-l1"><a class="reference internal" href="Synthetic_Data_from_Diffusion_Models_Improves_ImageNet_Classification.html">Synthetic Data from Diffusion Models Improves ImageNet Classification</a></li>






<li class="toctree-l1"><a class="reference internal" href="GLIDE.html">GLIDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="BBDM.html">BBDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Diffusion_Model_is_Secretly_a_Zero_Shot_Classifier.html">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="progressive_distillation.html">Progressive Distillation for Fast Sampling of Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ConceptLab.html">ConceptLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion_models_already_have_a_Semantic_Latent_Space.html">Diffusion Models already have a Semantic Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="Muse.html">Muse</a></li>


<li class="toctree-l1"><a class="reference internal" href="GIGAGAN.html">Scaling up GANs for Text-to-Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="consistency_models.html">Consistency Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="latent_consistency_models.html">Latent Consistency Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="LLM_grounded_Diffusion.html">LLM Grounded Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="DiT.html">DiT</a></li>






<li class="toctree-l1"><a class="reference internal" href="one-step-image-translation.html">One-Step Image Translation with Text-to-Image Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="LCM-LoRA.html">LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Video Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Make_A_Video.html">Make A Video</a></li>
<li class="toctree-l1"><a class="reference internal" href="VideoLDM.html">VideoLDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="AnimateDiff.html">AnimateDiff</a></li>
<li class="toctree-l1"><a class="reference internal" href="Animate_Anyone.html">Animate Anyone</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreaMoving.html">DreaMoving</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamPose.html">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3D Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NeRF.html">NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="3DGS.html">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li>
<li class="toctree-l1"><a class="reference internal" href="Point_E.html">Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)</a></li>








<li class="toctree-l1"><a class="reference internal" href="Shap-E.html">Shap-E</a></li>









<li class="toctree-l1"><a class="reference internal" href="DreamFusion.html"><strong>DreamFusion</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="magic-3d.html">Magic3D</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamBooth3D.html">Dream Booth 3D</a></li>



</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../experiments/js_exp.html">Synthetic Data with Stable Diffusion for Foliar Disease Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/swjo_exp.html">Training DreamBooth on Naver Webtoon Face Dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion/issues/new?title=Issue%20on%20page%20%2Fdocs/review/IP_Adapter.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/review/IP_Adapter.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1><no title></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="simple visible nav section-nav flex-column">
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2308.06721">https://arxiv.org/abs/2308.06721</a></p></li>
<li><p>Code: <a class="github reference external" href="https://github.com/tencent-ailab/IP-Adapter">tencent-ailab/IP-Adapter</a></p></li>
<li><p>Project Page : <a class="reference external" href="https://ip-adapter.github.io">https://ip-adapter.github.io</a></p></li>
</ul>
</li>
<li><p><strong>Author:</strong> Kyeongmin Yu</p></li>
<li><p><strong>Last updated on Sep. 18, 2024</strong></p></li>
</ul>
</div>
<div class="highlight-aside notranslate"><div class="highlight"><pre><span></span>ğŸ“Œ
| 
### ë¬¸ì œìƒí™©

text-to-image diffusion model(T2I diffusion model)ì´ ìƒì„±í•˜ëŠ” ì´ë¯¸ì§€ í’ˆì§ˆì€ í›Œë¥­í•˜ì§€ë§Œ text promptë¥¼ í†µí•´ ì›í•˜ëŠ” í˜•íƒœì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤. ë³µì¡í•œ prompt engineeringì„ ì‹œë„í•˜ê±°ë‚˜, image promptë¥¼ í™œìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ì„ fine-tuningí•˜ê²Œ ë˜ë©´ ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í•  ë¿ë§Œ ì•„ë‹ˆë¼ í•´ë‹¹ ë°©ì‹ì€ ë²”ìš©ì„±, í˜¸í™˜ì„±ë„ ë–¨ì–´ì§„ë‹¤.

### í•´ê²°ë°©ì•ˆ

**cross-attentionì„ text featuresì™€ image featuresë¡œ decouplingí•œë‹¤.** ê¸°ì¡´ í•™ìŠµëœ diffusion modelì€ text featureì— ë§ì¶° í•™ìŠµëœ ìƒíƒœì´ë¯€ë¡œ ê¸°ì¡´ layerì— image featureë¥¼ ë„£ê²Œ ë˜ë©´ image featureì™€ text featureë¥¼ alignì„ ìˆ˜í–‰í•˜ê²Œ ë˜ë¯€ë¡œ ê¸°ì¡´ cross-attention layer í•˜ë‚˜ë¥¼ í†µí•´  image-featureì™€ text-featureë¥¼ ê²°í•©í•˜ëŠ” ê²ƒì€ ì ì ˆí•˜ì§€ ì•Šë‹¤. 

### ë…¼ë¬¸ì˜ ê°•ì 

- ì–´ë–¤ ëª¨ë¸ êµ¬ì¡°ì—ë„ í™œìš©ê°€ëŠ¥í•˜ë‹¤.
- ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(22M)ë§Œ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ê°€ë³ë‹¤.
- ê¸°ì¡´ controllable toolsì— ë§ë¶™ì—¬ ì“¸ ìˆ˜ë„ ìˆë‹¤.
```` aside

![IMG_47A446BA601B-1.jpeg](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/IMG_47A446BA601B-1.jpeg)

# Introduction

**:image promptì˜ í•„ìš”ì„±ê³¼ ê¸°ì¡´ ì—°êµ¬ì—ì„œ image promptë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë ¤ëŠ” ì‹œë„ì˜ ì¢…ë¥˜ì™€ ì¥ë‹¨ì ì„ ë§í•œë‹¤.**

ë³µì¡í•œ sceneì´ë‚˜ conceptì„ ì…ë ¥í• ë•Œ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì´ ê°„í¸í•˜ê³  íš¨ê³¼ì ì´ë‹¤. 
image prompt + text prompt(â€œan image is worth a thousand wordsâ€)

![IMG_4891.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/IMG_4891.png)

**vs**

â€œë‚´ì¸„ëŸ´ í’ìœ¼ë¡œ ì¹´í˜ë¥¼ ê¾¸ë¯¸ê³  ì—¬ëŸ¬ ì‹ë¬¼ì„ ë‘ì–´ ì¥ì‹í•˜ê³  ì‹¶ì–´. ë‚´ê°€ ì¢‹ì•„í•˜ëŠ” ì‹ë¬¼ì€ ìŠ¤ë…¸ìš° ì‚¬íŒŒì´ì–´, í˜¸ì•¼, ìë¯¸ì˜¤ì¿¨ì¹´ìŠ¤ë“± ì´ê³ , ì˜ìì™€ í…Œì´ë¸”ì€ ì›ëª©ì„ ì„ í˜¸í•´.â€

DALL-E2ëŠ” ì²˜ìŒìœ¼ë¡œ image promptë¥¼ ì§€ì›í•œ ëª¨ë¸ìœ¼ë¡œ, T2I prior modelì´ image embeddingì„ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í–ˆë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ëŒ€ë¶€ë¶„ì˜ T2I ëª¨ë¸ì€ ì£¼ë¡œ textë¥¼ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ stable diffusion(SD) ëª¨ë¸ì˜ ê²½ìš° CLIP text encoderë¡œ ë¶€í„° text embeddingì„ ë½‘ì•„ë‚´ ì‚¬ìš©í–ˆë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” â€œimage promptë¥¼ ê¸°ì¡´ T2I ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€â€, image promptë¥¼ ì‚¬ìš©í•œ T2I ì´ë¯¸ì§€ ìƒì„±ì„ ë‹¨ìˆœí•œ ë°©ì‹ìœ¼ë¡œ ê°€ëŠ¥ì¼€ í•œë‹¤.

![ë¹„êµë¥¼ ìœ„í•œ DALL-E2(unCLIP) êµ¬ì¡°](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image.png)

ë¹„êµë¥¼ ìœ„í•œ DALL-E2(unCLIP) êµ¬ì¡°

![ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%201.png)

ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°

SD Image Variationsì™€ Stable UnCLIPê³¼ ê°™ì€ ê¸°ì¡´ ì—°êµ¬ì—ì„œ image promptë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ text-conditioned diffusion modelsì„ image embeddingì„ ì‚¬ìš©í•´ ì§ì ‘ fine-tuningí•˜ë ¤ëŠ” ì‹œë„ë¥¼ í–ˆë‹¤. í•˜ì§€ë§Œ ë§ì€ ì–‘ì˜ ì»´í“¨í„° ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ê³¼ ê¸°ì¡´ T2I ìƒì„±ëŠ¥ë ¥ ì €í•˜, ì¬ì‚¬ìš©ì„± ì €í•˜ë¼ëŠ” ë‹¨ì ì´ ìˆì—ˆë‹¤. ë˜í•œ í•´ë‹¹ ë°©ì‹ì€ ControlNetê³¼ ê°™ì€ ê¸°ì¡´ structural control toolsê³¼ í˜¸í™˜ë˜ì§€ ì•Šì•˜ë‹¤. ì´ëŠ” downstream applicationì— ì¹˜ëª…ì ì´ë‹¤.

ì´ë¥¼ í”¼í•˜ê¸° ìœ„í•´ diffusion model ìì²´ë¥¼ fine-tuningí•˜ì§€ ì•Šê³  text encoderë¥¼ image encoderë¡œ êµì²´í•˜ëŠ” ë°©ì‹ë„ ìˆì—ˆì§€ë§Œ text promptë¥¼ ì§€ì›í•  ìˆ˜ ì—†ê²Œ ë˜ê³  ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆë‹¤.

ìµœê·¼ì—ëŠ” T2I base modelì„ ê±´ë“œë¦¬ì§€ ì•Šê³  ì¶”ê°€ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•´ image promptë¥¼ ì§€ì›í•˜ëŠ” ì—°êµ¬ë“¤ì´ ìˆì—ˆë‹¤. ControlNet, T2I-Adapterì™€ ê°™ì€ ì—°êµ¬ë“¤ì€ ëŒ€ë¶€ë¶„ sketch, depth map, segmenation map ë“±ì˜ ì¶”ê°€ì ì¸ ì…ë ¥ì„ í™œìš©í–ˆë‹¤. ë˜í•œ T2I-Adapterë‚˜ Uni-ControlNet ê°™ì´reference imageë¥¼ ì…ë ¥í•´ style ì´ë‚˜ conceptì„ ì „ë‹¬í•˜ë ¤ëŠ” ì‹œë„ë„ ìˆì—ˆë‹¤. ì´ëŸ° íë¦„ì˜ ì—°êµ¬ë“¤ì€ CLIP image encoderì—ì„œ image embeddingì„ ì¶”ì¶œí•˜ì—¬ ì¶”ê°€ trainable networkì— ìƒˆë¡œìš´ featureë“¤ì„ mappingí•˜ì—¬ text featureì™€ ìœµí•©í•˜ê³ ì í–ˆë‹¤. ê¸°ì¡´ text featureëŒ€ì‹  text feature+image featureë¥¼ ë””í“¨ì „ ëª¨ë¸ ë‚´ UNet êµ¬ì¡°ì— ë„£ì–´ promptì— ë„£ì€ ì´ë¯¸ì§€ì— ì í•©í•œ(faithful) ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³ ì í–ˆë‹¤. ì´ëŸ° ì—°êµ¬ë“¤ì„ í†µí•´ image promptì˜ ê°€ëŠ¥ì„±ì„ ë³¼ìˆ˜ ìˆì—ˆì§€ë§Œ ê·¸ ì¶©ì‹¤ë„ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•˜ë‹¤. ë˜í•œ ì´ë¯¸ì§€ í’ˆì§ˆì´ fine-tuningëœ image prompt modelë³´ë‹¤ ë‚˜ë¹´ë‹¤.

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/f5ee9e88-e2b9-4ef8-9e8d-3fbe9b4b6a0a.png)

**image promptë¥¼ ì§€ì›í•˜ëŠ” ê¸°ì¡´ ë°©ì‹**

- input image embedding to T2I model
- base model fine-tuning
- text encoder â†’ image encoder
- additional network

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì•ì„œ ì–¸ê¸‰í•œ ë¬¸ì œì ì˜ ì›ì¸ì„ T2I modelë‚´ì˜ cross-attentionì´ë¼ê³  ë³´ê³  ìˆë‹¤. **ì‚¬ì „í•™ìŠµëœ cross-attentionì—ì„œ key, value projection weightsì€ text featureì— ë§ê²Œ í›ˆë ¨ë˜ì–´ ì‡ëŠ” ìƒíƒœì´ë‹¤.** 
ê²°ê³¼ì ìœ¼ë¡œ image featureì™€ text featureë¥¼ cross-attention layerì—ì„œ í•©ì³ì§€ëŠ”ë° ì´ë•Œ image-specific íŠ¹ì„±ë“¤ì´ ë¬´ì‹œë˜ì–´ reference imageì— ì•„ì£¼ ì¶©ì‹¤í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ëª»í•˜ê³  coarse-grained controllable generation(e.g., image style)ë§Œ ë‹¬ì„± ê°€ëŠ¥í•´ì§„ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ì•ì„  ì—°êµ¬ì˜ ë¬¸ì œì ë“¤ì„ ê·¹ë³µí•œ íš¨ê³¼ì ì¸ image prompt adapter, IP-Adapterë¥¼ ì œì•ˆí•œë‹¤. íŠ¹íˆ IP-Adapterì˜ ê²½ìš° decoupled cross-attention mechanismì„ ì‚¬ìš©í•´ text featureì™€ image featureë¥¼ ë¶„ë¦¬í•œë‹¤. image featureë¥¼ ìœ„í•´ base modelë‚´ ëª¨ë“  UNet cross-attention layerì— cross-attention layer ë¥¼ ì¶”ê°€í•˜ì—¬ í›ˆë ¨ë‹¨ê³„ì—ì„œëŠ” ì ì€ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(22M)ë§Œ í›ˆë ¨í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” IP-AdapterëŠ” ë§¤ìš° ê°€ë³ê³  íš¨ê³¼ì ì´ë‹¤. ë˜í•œ ì¼ë°˜í™” ëŠ¥ë ¥(generalization capability)ê°€ ë†’ê³  text promptì™€ë„ ì˜ ì–´ìš¸ë¦°ë‹¤(compatible). 

**IP-Adapterì—ì„œ ì œì•ˆí•˜ëŠ” ë°©ì‹**

- additional cross-attention layer in UNet of diffusion model
- reusable and flexible (base + IP-Adapter + ControlNetê°€ëŠ¥)
- multimodal compatibility (image prompt + text prompt)

# Related Works

### Text-to-Image Diffusion Models

large T2I modelì€ í¬ê²Œ autoregressive models, diffusion models ë‘ ë¶€ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. DALLE, CogView, Make-A-Sceneê³¼ ê°™ì€ ì´ˆê¸° ì—°êµ¬ë“¤ì€ autoregressive model ì´ì—ˆë‹¤. autoregressive modelì€ VQ-VAEì™€ ê°™ì€ image tokenizerë¥¼ ì‚¬ìš©í•´ imageë“¤ì„ tokení™” í•˜ì—¬ autoregressive transformerì— text tokenì„ ì´ìš©í•´ image tokenì„ ì˜ˆì¸¡í•˜ê²Œ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆë‹¤. í•˜ì§€ë§Œ autoregressive modelì€ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ê³  ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í–ˆë‹¤.

ìµœê·¼ì—ëŠ” diffusion models(DM)ì´ ë“±ì¥í•˜ì—¬ T2I ìƒì„±ëª¨ë¸ì˜ state-of-the-artë¥¼ ë‹¬ì„±í–ˆë‹¤. ì´ì „ì— GLIDEëŠ” cascaded diffusion êµ¬ì¡°ë¥¼ í†µí•´ 64x64 â†’ 256x256 ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. 
DALL-E2ì˜ ê²½ìš°, text promptë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•´ image embeddingì„ ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. DALL-E2ëŠ” text promptë¥¼ í†µí•œ ì´ë¯¸ì§€ ìƒì„±ì„ ì§€ì›í•˜ì§€ ì•Šì•˜ë‹¤. text ì´í•´ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ Imagenì€ ê±°ëŒ€ transformer language modelì¸ T5ë¥¼ ë„ì…í–ˆë‹¤. Re-Imagenì˜ ê²½ìš° ë“œë¬¼ê±°ë‚˜ í•™ìŠµí•œì ì—†ëŠ” entityì— ëŒ€í•œ imageì— ëŒ€í•œ ì¶©ì„±ë„ë¥¼ ê°œì„ í–ˆë‹¤. 
SDëŠ” latent diffusion modelë¡œ pixel spaceê°€ ì•„ë‹Œ latent spaceìƒì—ì„œ ë™ì‘í•˜ê²Œ í•˜ì—¬ diffusion modelë§Œ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. text ì¼ì¹˜ë„(alignment)ë¥¼ ë†’ì´ê¸° ìœ„í•´ eDiff-Iì˜ ê²½ìš° T2I diffusion modelê³¼ ìœ ì‚¬í•œ ë””ìì¸ì„ ì±„íƒí•˜ì—¬ T5 text, CLIP text embedding, CLIP image embeddingë“± ë©€í‹°ëª¨ë‹¬ ì¡°ê±´ì„ í™œìš©í–ˆë‹¤. Versatile Diffusionì€ unified multi-flow diffusion frameworkë¥¼ ì´ìš©í•´ T2I, I2T, ë“± ë‹¤ì–‘í•œ ìƒì„±ë°©ì‹ì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤. controllable image ìƒì„± ë©´ì—ì„œëŠ” Composerê°€ image embeddingì„ í™œìš©í•œ joint fine-tuningì„ ì‹œë„í–ˆì—ˆë‹¤. RAPHAELì€ mixture of experts(MoEs) ì „ëµì„ ì‚¬ìš©í•´ T2I modelì˜ ì´ë¯¸ì§€ í’ˆì§ˆì„ í–¥ìƒì‹œì¼°ë‹¤.

DALL-E2ëŠ” image promptë¥¼ í†µí•´ í•´ë‹¹ í’ì˜ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ë§¤ë ¥ì ì´ë‹¤. ë˜í•œ image promptë¥¼ T2I modelì—ì„œ ì§€ì›í•˜ê³ ì í•˜ëŠ” ì—°êµ¬ë“¤ì´ ìˆë‹¤. SD Image Variants modelì€ ë³€ê²½í•œ SDë¥¼ fine-tuningí•˜ì—¬ text featureë¥¼ CLIP image encoderì˜ image embeddingìœ¼ë¡œ êµì²´í•  ìˆ˜ ìˆê²Œ í–ˆë‹¤. Stable unCLIP ë˜í•œ SDë¥¼ fine-tuningí•˜ì—¬ time embeddingì— image embeddingì„ ì¶”ê°€í–ˆë‹¤. ê¸°ì¡´ ëª¨ë¸ì„ fine-tuningí•˜ëŠ” ë°©ì‹ì€ ê³ í’ˆì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ ë¹„êµì  training costê°€ ë†’ìœ¼ë©° ê¸°ì¡´ tools(e.g.,ControlNet)ê³¼ í˜¸í™˜ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.

### Adapters for Large Models

ê±°ëŒ€í•œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ì „ì²´ë¥¼ fine-tuningí•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë‹¤. ì´ ëŒ€ì•ˆìœ¼ë¡œ ë– ì˜¤ë¥´ëŠ” ê²ƒì´ adapterë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì¸ë°, ê¸°ì¡´ ëª¨ë¸ì€ freezeì‹œì¼œ í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. adapterëŠ” NLPì—ì„œ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©ë˜ë˜ ë°©ì‹ì´ë‹¤. ìµœê·¼ì—ëŠ” LLMì˜ vision-language ì´í•´ë¥¼ ìœ„í•´ adapterë¥¼ ì‚¬ìš©í•˜ê³  ìˆë‹¤.

T2I modelì˜ ìµœê·¼ ì¸ê¸°ë¡œ ì¸í•´ adapterë“¤ë„ ì—¬ê¸°ì— ì¶”ê°€ì ì¸ controlì„ ì£¼ëŠ” ë°©í–¥ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ControlNetì˜ ê²½ìš° ì‚¬ì „í•™ìŠµëœ T2I diffusion modelì— task-specificí•œ ì…ë ¥
(e.g.,canny edge)ì„ ì¶”ê°€ì ìœ¼ë¡œ ë„£ê¸°ìœ„í•´ adapterë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤. ìœ ì‚¬í•œ ì‹œê¸°ì— T2I-Adapterë„ ë“±ì¥í–ˆëŠ”ë° ë³´ë‹¤ ê°„ë‹¨í•˜ê³  ê°€ë²¼ìš´ í˜•íƒœë¡œ ìƒ‰ì´ë‚˜ êµ¬ì¡°ì ì¸ ë©´ì—ì„œ 
fine-grained controlì„ ì£¼ê³ ì í–ˆë‹¤. fine-tuningì— ì‚¬ìš©ë˜ëŠ” ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ Uni-ControlNetì€ multi-scale condition injectionì„ ì‚¬ìš©í–ˆë‹¤. 

structural controlì™¸ì— ì´ë¯¸ì§€ ì§‘í•©ì„ í†µí•´ contentë‚˜ styleì„ ì¡°ì ˆí•˜ê³ ì í•œ ì—°êµ¬ë„ ìˆë‹¤. ControlNet Shuffleì˜ ê²½ìš° ì´ë¯¸ì§€ë“¤ì„ recomposeí•˜ë„ë¡ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìê°€ ì œê³µí•œ ì´ë¯¸ì§€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆì—ˆë‹¤. ë˜í•œ ControlNet Reference-onlyì˜ ê²½ìš°, í•™ìŠµì—†ì´ SDì— feature injectionì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë³€í˜•í–ˆë‹¤. T2I-Adapterì˜ ìµœê·¼ ë²„ì „ì˜ ê²½ìš°, CLIP image encoderë¡œ ë¶€í„° reference imageì˜ image featureë¥¼ text featureì— ë”í•´ì¤Œìœ¼ë¡œì„œ style adapterë¡œì„œì˜ ì—­í• ë„ ê°€ëŠ¥í•˜ë‹¤. Uni-ControlNetì˜ global control adapter ë˜í•œ CLIP image encoderë¡œ ë¶€í„° ì¶”ì¶œí•œ image embeddingì„ ì‘ì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ projectioní•˜ condition embeddingìœ¼ë¡œ projectioní•˜ì—¬ ì‚¬ìš©í•œë‹¤. SeeCoderëŠ” ê¸°ì¡´ text encoderë¥¼ semantic context encoderë¡œ êµì²´í•˜ì—¬ image variantsë¥¼ ìƒì„±í•˜ê³ ì í–ˆë‹¤.

**ControlNet**

![ë¹„êµë¥¼ ìœ„í•œ ControlNetì˜ ì‘ë™ ë°©ì‹](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%202.png)

ë¹„êµë¥¼ ìœ„í•œ ControlNetì˜ ì‘ë™ ë°©ì‹

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%203.png)

**Uni-ControlNet**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%204.png)

**T2I-Adapter**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%205.png)

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%206.png)

ë¹„êµë¥¼ ìœ„í•œ T2I Adapterì˜ ì‘ë™ ë°©ì‹

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%207.png)

**SeeCoder**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%208.png)

# Method

### Preliminaries

&lt;aside&gt;
ğŸ“Œ

ìƒì„±ëª¨ë¸ì˜ ì¼ì¢…ì¸ diffusion modelì˜ ì´ë¯¸ì§€ ìƒì„±ë‹¨ê³„ 

1. **diffusion process (forward process)**
    
    T stepì˜ fixed Markov chainì„ í†µí•´ë°ì´í„°ì— gaussian noiseë¥¼ ì ì°¨ ì¶”ê°€.  
    
2. **denoising process**
    
    gaussian noiseë¡œ ë¶€í„° learnable modelì„ í†µí•´ sampleì„ ìƒì„±.
    
&lt;/aside&gt;

ì¼ë°˜ì ìœ¼ë¡œ noise ì˜ˆì¸¡ì„ ìœ„í•œ diffusion model($\epsilon_\theta$)ì˜ training objectiveëŠ” ì•„ë˜ì™€ ê°™ì´ ë‹¨ìˆœí•œ variant of variational bound ë¡œ í‘œí˜„ëœë‹¤. 

$$
L_{\text{simple}}=\Bbb E_{x_0, \epsilon\sim \mathcal N(0,I),c,t}\|\epsilon-\epsilon_\theta(x_t,\bold c,t)\|^2 \tag{1}
$$

$x_0$ ëŠ” real data, $\bold c$ ëŠ” ì¶”ê°€ì¡°ê±´, $t$ ëŠ” time stepì„ ë§í•˜ë©° $[0,T]$ ë‚´ì— ì†í•œë‹¤.  $x_t=\alpha_t x_0+\sigma_t\epsilon$ì€ step tì— í•´ë‹¹í•˜ëŠ” noisy dataë¥¼ ë§í•˜ê³ , $\alpha_t, \sigma_t$ëŠ” diffusino processë¥¼ ê²°ì •í•˜ëŠ” predefined functionì´ë‹¤. $\epsilon_\theta$ê°€ í•œë²ˆ í•™ìŠµë˜ê³  ë‚˜ë©´ ëœë¤ ë…¸ì´ì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìƒì„± ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ DDIM, PNDM, DPM-solverì™€ ê°™ì€ fast samplerë¥¼ inferenceì‹œ ì‚¬ìš©í•œë‹¤.

conditional diffusion modelì—ì„œ classifier guidanceë¥¼ í†µí•´ ì´ë¯¸ì§€ ì •í™•ë„(fidelity)ì™€ ë‹¤ì–‘ì„±(sample diversity)ë¥¼ ë°¸ëŸ°ì‹±í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ë”°ë¡œ í•™ìŠµëœ classifierì˜ gradientë¥¼ í™œìš©í•˜ëŠ”ë°, classifierë¥¼ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ë²ˆê±°ë¡œì›€ì„ ì§€ìš°ê¸° ìœ„í•´ classifier-free guidanceë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. ì´ëŸ° ì ‘ê·¼ì—ì„œ conditional, unconditional diffusion modelsëŠ” í•™ìŠµì‹œ ëœë¤í•˜ê²Œ ì¡°ê±´ $c$ ë¥¼ ë°°ì œí•˜ì—¬ í•©ë™ í•™ìŠµ(joint training)ëœë‹¤. samplingë‹¨ê³„ ì—ì„œëŠ” conditional modelê³¼ unconditional modelì˜ predictionì„ ëª¨ë‘ ì´ìš©í•˜ì—¬ noiseë¥¼ ê³„ì‚°í•œë‹¤. 

$$
\hat \epsilon_\theta(x_t,\bold c,t)=\mathcal w \epsilon_\theta(x_t,\bold c, t)+(1-\mathcal w)\epsilon_\theta(x_t,t) \tag{2}
$$

$\mathcal w$ì€ guidance scale í˜¹ì€ guidance weightë¡œ ë¶ˆë¦¬ëŠ”ë° condition $c$ì˜ ì˜í–¥ë ¥ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•œ ìƒìˆ˜ê°’ì´ë‹¤. T2I diffusion modelì˜ ê²½ìš° image-text ì¼ì¹˜ì„±ì„ ë†’ì´ëŠ”ë° classifier-free guidanceê°€ í° ì—­í• ì„ í•œë‹¤. 

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” open-source SDì— IP-Adapterë¥¼ ë§ë¶™ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆë‹¤. SDëŠ” latent diffusion modelë¡œ frozen CLIP text encoderë¡œ ë½‘ì•„ë‚¸ text featureë¥¼ conditionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. diffusion modelì€ Unetì— attention layerê°€ ì¶”ê°€ëœ í˜•íƒœì´ë‹¤. Imagenê³¼ ê°™ì€ pixel-based diffusion modelê³¼ ë¹„êµí•´ SDëŠ” ì‚¬ì „í•™ìŠµëœ auto-encoder modelì„ í™œìš©í•´ latent spaceì—ì„œ ë™ì‘í•˜ë¯€ë¡œ íš¨ìœ¨ì ì´ë‹¤. 

### Image Prompt Adapter

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%209.png)

![ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%201.png)

ë¹„êµë¥¼ ìœ„í•œ Stable Diffusionì˜ êµ¬ì¡°

**Image Encoder**

pretained CLIP image encoderë¥¼ ì‚¬ìš©í•´ image promptì—ì„œ image featureë¥¼ ë½‘ì•„ëƒˆë‹¤. CLIPì€ multimodal modelë¡œ ê±°ëŒ€ image-text pair ë°ì´í„°ì…‹ìœ¼ë¡œ contrastive learningì‹œí‚¨ ëª¨ë¸ì´ë‹¤. CLIP image encoderë¥¼ í†µí•´ global image embeddingì„ ì–»ì—ˆë‹¤. ì´ëŠ” imageë¡œ ë¶€í„° í’ë¶€í•œ ë‚´ìš©(content)ì™€ ìŠ¤íƒ€ì¼ì„ ë‹´ì€ image captionê³¼ ì˜ ì¡°ì •ë˜ì–´(well-aligned) ìˆë‹¤. í•™ìŠµë‹¨ê³„ì—ì„œ CLIP image encoderëŠ” frozenë˜ì–´ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤.

**Decoupled Cross-Attention**

image featureëŠ” ì‚¬ì „í•™ìŠµëœ UNetì— decoupled cross-attentionì„ í†µí•´ ê²°í•©ëœë‹¤. ì´ˆê¸° SD modelì—ì„œëŠ” CLIP text encoderë¥¼ í†µí•´ ë½‘ì•„ë‚¸ text featureë¥¼ UNetì˜ cross-attention layerì— ë„£ì—ˆë‹¤. 

$$
\mathbf Z&#39;=\text{Attention}(\bold{Q,K,V})=\text{Softmax}(\frac{\bold {QK}^T}{\sqrt{d}})\bold V, \tag{3}
$$

query featureëŠ” $Z$, text featureëŠ” $c_t$, cross-attentionì˜ ê²°ê³¼ëŠ” $Zâ€™$ì´ê³ , $\bold{Q=ZW_q, K=c_tW_k, V=c_tW_v}$ëŠ” attention ì—°ì‚°ì˜ ê°ê° query, key, value í–‰ë ¬ì´ë‹¤. $\bold{W_q, W_k, W_v}$ëŠ” linear projection layersì˜ í•™ìŠµê°€ëŠ¥í•œ weigth matricesë‹¤.

image featureë¥¼ ì´ë¯¸ì§€ ìƒì„±ì— ë°˜ì˜í•˜ëŠ” ì§ê´€ì ì¸ ë°©ë²•ì€ cross-attentionì‹œ text feature+image featureë¡œ ê²°í•©(concatenate)í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ì€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ì—¬ decoupled cross-attentionì„ ì œì•ˆí•œë‹¤. ì´ëŠ” cross-attention ì—ì„œ image featureì™€ text featureë¥¼ ë”°ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ê¸°ì¡´ cross-attention layerê°€ ì¡´ì¬í•˜ë˜ ê³³ì— ìƒˆë¡œìš´ cross-attention layerë¥¼ ì¶”ê°€í•˜ì—¬ image featureë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í–ˆë‹¤. image feature $c_i$ê°€ ì£¼ì–´ì§ˆë•Œ ìƒˆë¡œìš´ attention layerì˜ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 

$$
\mathbf Z&#39;&#39;=\text{Attention}(\bold{Q,K&#39;,V&#39;})=\text{Softmax}(\frac{\bold {Q(K&#39;)}^T}{\sqrt{d}})\bold V&#39;, \tag{4}
$$

$\bold{Q=ZW_q, K&#39;=c_tW&#39;_k, V&#39;=c_tW&#39;_v}$ëŠ” image featureë¥¼ ìœ„í•œ query, key, value í–‰ë ¬ì´ë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ì€ text cross-attentionê³¼ image cross-attentionì—ì„œ ë™ì¼í•œ qeuryë¥¼ ì‚¬ìš©í–ˆë‹¤ëŠ” ì ì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œëŠ” ê° cross-attention layer ë§ˆë‹¤ 2ê°œì˜ íŒŒë¼ë¯¸í„° $\bold{W&#39;_k,W&#39;_v}$ ë¥¼ ì¶”ê°€í•˜ê²Œ ëœë‹¤. ìˆ˜ë ´ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ $\bold{W&#39;_k,W&#39;_v}$ëŠ” $\bold{W_k,W_v}$ë¡œ ì´ˆê¸°í™”í–ˆë‹¤. ê·¸ëŸ¬ë©´ ë‘ cross-attention layerì˜ ê²°ê³¼ë¥¼ ë”í•¨ìœ¼ë¡œì¨ ìµœì¢… ê²°ê³¼ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. decoupled cross-attentionì˜ ìµœì¢…ì ì¸ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\mathbf Z^\text{new}=\text{Softmax}(\frac{\bold {QK}^T}{\sqrt{d}})\bold V + \text{Softmax}(\frac{\bold {Q(K&#39;)}^T}{\sqrt{d}})\bold V&#39;\\ \text{where} \space \bold{Q=ZW}_q,\space \bold{k=c}_t\bold W_k,\space \bold{K&#39;=c}_i\bold W&#39;_k, \space \bold{V&#39;=c}_i\bold W&#39;_v \tag{5}
$$

ì‚¬ì „í•™ìŠµí•œ UNetì€ freezeì‹œí‚¤ê³  í›ˆë ¨ì„ ì§„í–‰í•˜ë¯€ë¡œ $\bold{W&#39;_k,W&#39;_v}$ **ë§Œ** í•™ìŠµëœë‹¤.

 

**Training and Inference**

í•™ìŠµì‹œ IP-Adapterë§Œ ìµœì í™”í•˜ê³  ê¸°ì¡´ ì‚¬ì „í•™ìŠµëœ diffusion modelì€ ê³ ì •í•œë‹¤. IP-AdapterëŠ” image-text pair datasetìœ¼ë¡œ í•™ìŠµì‹œí‚¤ë©° original SDì™€ ë™ì¼í•œ objectiveë¥¼ ì‚¬ìš©í•œë‹¤.

$$
L_{\text{simple}}=\Bbb E_{x_0, \epsilon\sim \mathcal N(0,I),c_t,c_i,t}\|\epsilon-\epsilon_\theta(x_t,\bold {c_t,c_i},t)\|^2 \tag{6}
$$

ë˜ randomí•˜ê²Œ image conditionì„ dropí•˜ì—¬ inference ë‹¨ê³„ì—ì„œ classifier-free guidanceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

$$
\hat \epsilon_\theta(x_t,\bold {c_t,c_i},t)=\mathcal w \epsilon_\theta(x_t,\bold {c_t,c_i}, t)+(1-\mathcal w)\epsilon_\theta(x_t,t) \tag{7}
$$

image conditionì´ dropë˜ë©´  CLIP image embeddingì€ 0ìœ¼ë¡œ ì²˜ë¦¬í–ˆë‹¤. text cross-attentionê³¼ image cross-attentionì„ detachë˜ë©° inferenceì‹œ image conditionì˜ ê°€ì¤‘ì¹˜ë„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. $\lambda$ ê°€ 0ì´ ë˜ë©´ ê¸°ì¡´ T2I ëª¨ë¸ì´ ëœë‹¤.

$$
\mathbf Z^\text{new}=\text{Softmax}(\bold {Q,K,V})+ \lambda\cdot\text{Softmax}(\bold {Q,K&#39;,V&#39;})\tag{8}
$$

# Experiments

### Experimental Setup

| base model | SD v1.5 |
| --- | --- |
| image encoder | OpenCLIP ViT-H/14 |
| resolution | 512x512 (resized and center crop) |
| optimizer | AdamW |
| learning rate | 0.0001 |
| weight decay | 0.01 |
| libraries | Hugging Face diffusers, DeepSpeed SeRO-2 |
| GPU | 8 V100 |
| training step | 1M |
| batch size | 8 per GPU |
| classifier-free guidance | 0.05 |

| training data | LAION-2B, COYO-700M |
| --- | --- |
| sampler for inference | DDIM (50steps) |
| guidance scale | 7.5 |
| $\lambda$ | 1.0 for only image prompt |

### Comparison with Existing Methods

**Quantitative Comparison**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2010.png)

**Qualitative Comparison**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2011.png)

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2012.png)

### More Results

**Generalizable to Custom Models**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2013.png)

**Structure Control**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2014.png)

**Image-to-Image Inpainting**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2015.png)

**Multimodal Prompts**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2016.png)

### Ablation Study

**Importance of Decoupled Cross-Attention**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2017.png)

**Comparison of Fine-grained Features and Global Features**

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2018.png)

IP-AdapterëŠ” CLIP image encoderë¡œ ë¶€í„° ì¶”ì¶œí•œ global image embeddingë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— reference imageì˜ ì¼ë¶€ íŠ¹ì„±ì„ ìƒì–´ë²„ë¦´ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ fine-grained featureë¥¼ ìœ„í•œ IP-Adapterë¥¼ ë””ìì¸í–ˆë‹¤. ì²«ë²ˆì§¸ë¡œ CLIP image encoderì—ì„œ penultimate layerì—ì„œ grid featureë¥¼ ë½‘ì•„ë‚¸ë‹¤. ì´í›„ ì‘ì€ query networkë¥¼ ì´ìš©í•´ featureë¥¼ í•™ìŠµí•œë‹¤. grid featureë¡œ ë¶€í„° ì •ë³´ë¥¼ ë½‘ì•„ë‚´ê¸° ìœ„í•´ lightweight transformerë¥¼ ì‚¬ìš©í•´ learnable 16 tokenë“¤ì„ ì •ì˜í•œë‹¤. ì´ token featureë“¤ì„ query networkì˜ cross-attention layerì— ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤€ë‹¤.

ë‘ adapterì˜ ìƒì„± ê²°ê³¼ë¥¼ ë¹„êµí•˜ë©´ finer-grained featureë¥¼ ì´ìš©í•˜ë©´ ë³´ë‹¤ image promptì™€ ê°€ê¹Œìš´ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. finer-grained featureëŠ” spatial structure informationì„ í•™ìŠµí•˜ì—¬ ìƒì„±ëœ ì´ë¯¸ì§€ì˜ diversityë¥¼ ë‚®ì¶”ëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë‚˜ ì¶”ê°€ì ì¸ ì¡°ê±´(text prompt, structure map)ì„ í™œìš©í•˜ë©´ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‚¬ì§„+poseë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ìƒì„± í•  ìˆ˜ ìˆë‹¤.

# Conclusion

ë³¸ ì—°êµ¬ì—ì„œëŠ” ì‚¬ì „ í•™ìŠµëœ T2I diffusion modelì— image prompt capabilityë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ IP-Adapterë¥¼ ì œì•ˆí•œë‹¤. IP-Adapterì˜ í•µì‹¬ ë””ìì¸ì€ decoupled cross-attentionìœ¼ë¡œ image featureë¥¼ ë¶„ë¦¬í•˜ì—¬ cross-attentionì„ ìˆ˜í–‰í•œë‹¤. ê³ ì‘ 22M parameterê°€ ì¶”ê°€ëœ IP-AdapterëŠ” qualitative, quantitative experimental results ëª¨ë‘ì—ì„œ ë¹„ë“±í•˜ê±°ë‚˜ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ë˜í•œ IP-AdapterëŠ” í™•ì¥ì„±ì´ ì¢‹ì•„ í•œë²ˆ í›ˆë ¨ëœ ë’¤, ë‹¤ë¥¸ custom model, structural controllable toolsì— ê³§ë°”ë¡œ ë§ë¶™ì—¬ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ë”ìš± ì¤‘ìš”í•œ ì ì€ image promptë¥¼ text promptì™€ ë”ì•  ë©€í‹°ëª¨ë‹¬ ì´ë¯¸ì§€ ìƒì„±ì„ ê°€ëŠ¥ì¼€í•œë‹¤ëŠ” ì ì´ë‹¤.

IP-AdapterëŠ” íš¨ê³¼ì ì´ì§€ë§Œ reference imageì™€ content, styleì´ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë§Œ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆë‹¤. ë•Œë¬¸ì— Textual Inversionì´ë‚˜ DreamBoothì™€ ê°™ì´ íŠ¹ì • ì´ë¯¸ì§€ ì§‘í•© í’ì˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ëŠ” ëª»í•œë‹¤. ë¯¸ë˜ì— consistencyë¥¼ í–¥ìƒì‹œí‚¨ ë” ê°•ë ¥í•œ Image prompt adapterë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.

Textural Inversion

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2019.png)

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2020.png)

DreamBooth

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2021.png)

![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2022.png)

- 
    
    
    ![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2023.png)
    
    ![image.png](IP-Adapter%2011aa380d107c800eb2f1cf2bff1b23cd/image%2024.png)
</pre></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="t2i_adapter.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">T2I-Adapter</p>
      </div>
    </a>
    <a class="right-next"
       href="HyperDreamBooth.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">HyperDreamBooth</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="simple visible nav section-nav flex-column">
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PseudoLab
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>