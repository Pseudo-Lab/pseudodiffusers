

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module &#8212; Text-to-Image Generation-feat-Diffusion</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/review/LCM-LoRA';</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="MimicBrush: Zero-shot Image Editing with Reference Imitation" href="MimicBrush.html" />
    <link rel="prev" title="One-Step Image Translation with Text-to-Image Models" href="one-step-image-translation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/PseudoLab_logo.png" class="logo__image only-light" alt="Text-to-Image Generation-feat-Diffusion - Home"/>
    <script>document.write(`<img src="../../_static/PseudoLab_logo.png" class="logo__image only-dark" alt="Text-to-Image Generation-feat-Diffusion - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to PseudoDiffusers!!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary Works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="DDPM.html">DDPM</a></li>




<li class="toctree-l1"><a class="reference internal" href="DDIM.html">DDIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Study_on_the_Evaluation_of_Generative_Models.html">A Study on the Evaluation of Generative Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Generation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cycleGAN.html">CycleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyleGAN.html">StyleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_beats_GANs.html">Diffusion Models Beat GANs on Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalle.html">DALL-E</a></li>
<li class="toctree-l1"><a class="reference internal" href="DALLE2.html">DALL-E 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="dreambooth.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="ControlNet.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Latent_Diffusion_Model.html">Introduction</a></li>



<li class="toctree-l1"><a class="reference internal" href="Textual_Inversion.html">Textual Inversion</a></li>








<li class="toctree-l1"><a class="reference internal" href="CustomDiffusion.html">Custom Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="LoRA.html">LoRA</a></li>









<li class="toctree-l1"><a class="reference internal" href="I-DDPM.html">I-DDPM</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyO.html">StyO</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen_editor.html">Imagen Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDEdit.html">SDEdit</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDXL.html">SDXL</a></li>
<li class="toctree-l1"><a class="reference internal" href="t2i_adapter.html">T2I-Adapter</a></li>
<li class="toctree-l1"><a class="reference internal" href="IP_Adapter.html">IP-Adapter</a></li>





<li class="toctree-l1"><a class="reference internal" href="HyperDreamBooth.html">HyperDreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="CM3leon.html">CM3leon</a></li>

<li class="toctree-l1"><a class="reference internal" href="Synthetic_Data_from_Diffusion_Models_Improves_ImageNet_Classification.html">Synthetic Data from Diffusion Models Improves ImageNet Classification</a></li>






<li class="toctree-l1"><a class="reference internal" href="GLIDE.html">GLIDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="BBDM.html">BBDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Diffusion_Model_is_Secretly_a_Zero_Shot_Classifier.html">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="progressive_distillation.html">Progressive Distillation for Fast Sampling of Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ConceptLab.html">ConceptLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion_models_already_have_a_Semantic_Latent_Space.html">Diffusion Models already have a Semantic Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="Muse.html">Muse</a></li>


<li class="toctree-l1"><a class="reference internal" href="GIGAGAN.html">Scaling up GANs for Text-to-Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="consistency_models.html">Consistency Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="latent_consistency_models.html">Latent Consistency Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="LLM_grounded_Diffusion.html">LLM Grounded Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="DiT.html">DiT</a></li>






<li class="toctree-l1"><a class="reference internal" href="one-step-image-translation.html">One-Step Image Translation with Text-to-Image Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</a></li>


<li class="toctree-l1"><a class="reference internal" href="MimicBrush.html">MimicBrush: Zero-shot Image Editing with Reference Imitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="one_step_diffusion_with_distribution_matching_distillation.html">One-step Diffusion with Distribution Matching Distillation</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Video Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Make_A_Video.html">Make A Video</a></li>
<li class="toctree-l1"><a class="reference internal" href="VideoLDM.html">VideoLDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="AnimateDiff.html">AnimateDiff</a></li>
<li class="toctree-l1"><a class="reference internal" href="Animate_Anyone.html">Animate Anyone</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreaMoving.html">DreaMoving</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamPose.html">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3D Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NeRF.html">NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="3DGS.html">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li>
<li class="toctree-l1"><a class="reference internal" href="Point_E.html">Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)</a></li>








<li class="toctree-l1"><a class="reference internal" href="Shap-E.html">Shap-E</a></li>









<li class="toctree-l1"><a class="reference internal" href="DreamFusion.html"><strong>DreamFusion</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="magic-3d.html">Magic3D</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamBooth3D.html">Dream Booth 3D</a></li>



<li class="toctree-l1"><a class="reference internal" href="zero123plus.html">Zero123++</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamGaussian.html">DreamGaussian</a></li>






</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../experiments/js_exp.html">Synthetic Data with Stable Diffusion for Foliar Disease Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/swjo_exp.html">Training DreamBooth on Naver Webtoon Face Dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion/issues/new?title=Issue%20on%20page%20%2Fdocs/review/LCM-LoRA.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/review/LCM-LoRA.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proposal">Proposal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">기존 연구의 한계점</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lcms">LCMs 기반 연구</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-models">Consistency Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-consistency-models">Latent Consistency Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cms">CMs과 차이점</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation">Low Rank Adaptation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-arithmetic-in-pretrained-models">Task Arithmetic in Pretrained Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lcm-lora">3. LCM-LoRA</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-distillation-for-lcm">3.1 LoRA Distillation for LCM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lcm-lora-as-universal-acceleration-module">3.2 LCM-LoRA as Universal Acceleration Module</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">4. Conclusion</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2311.05556">https://arxiv.org/pdf/2403.12036</a></p></li>
<li><p>Code: <a class="reference external" href="https://github.com/luosiallen/latent-consistency-model">Official</a></p></li>
</ul>
</li>
<li><p><strong>Author:</strong> Donghyun Han</p></li>
<li><p><strong>Last updated on Oct. 02, 2024</strong></p></li>
</ul>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="lcm-lora-a-universal-stable-diffusion-acceleration-module">
<h1>LCM-LoRA: A Universal Stable-Diffusion Acceleration Module<a class="headerlink" href="#lcm-lora-a-universal-stable-diffusion-acceleration-module" title="Permalink to this heading">#</a></h1>
<section id="proposal">
<h2>Proposal<a class="headerlink" href="#proposal" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Latent Consistency Models(LCMs)에 Low Rank Adaptation (LoRA)을 적용하였다.</p></li>
<li><p>LoRA를 이용하여 Stable Diffusion에 대한 추가적인 학습 없이도 fine-tuning 가능. (Accelerate 효과 극대화)</p></li>
<li><p>이전의 다양한 PF-ODE (Probability-Flow ODE) solver를 사용한 방법론들보다 더 generalized 된 성능</p></li>
</ul>
</section>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>기존 연구의 한계점<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Latent Diffusion Models(LDMs)은 image generation 분야에서 좋은 성능을 보이고 있다. 그러나 아직까지는 느린 reverse process 때문에 사용자가 직접 사용하기에는 무리가 있다.
따라서 LDMs을 가속화(Accelerate)하기 위한 기법들이 제안되어 왔는데 크게 2가지로 나눌 수 있다:</p>
<ol class="arabic simple">
<li><p>DDIM, DPM-Solver, DPM-Solver++ 등 <strong>ODE-Solver 기반 방법론</strong>.</p></li>
<li><p>LDM을 경량화 하기 위한 <strong>Distillation 기반 방법론</strong>.</p></li>
</ol>
<p>ODE-Solver 방법론은 sampling step을 줄일 수 있지만 Classifier-free Guidance(CFG) 등을 사용할 때 Computation 적으로 Overhead가 있을 수 있다.
Distillation 방법론 또한 Distillation 시 Computation적으로 Overhead가 있어 한계가 있다.
ex)<a class="reference external" href="https://arxiv.org/pdf/2210.03142">Guided Distill</a> : 2 stage의 distillation  방식 + high resolution image 생성 한계</p>
</section>
<section id="lcms">
<h3>LCMs 기반 연구<a class="headerlink" href="#lcms" title="Permalink to this heading">#</a></h3>
<p>이에 반해 Consistency Models(CMs)에서 영감을 받은 Latent Consistency Models(LCMs)은 매우 좋은 대안이다. backward process를 augmented Probability Flow ODE(PF-ODE) problem으로 접근하여 반복적인 step을 획기적으로 줄일 수 있었다. LCMs은 1~4 step만으로도 높은 퀄리티의 고해상도 이미지를 생성해낼 수 있으며 큰 리소스가 필요하지 않다.</p>
<p>그러나 LCMs을 기반으로 하는 방법론은 새로운 데이터셋에 대해 finetuning이 필요하거나 pretrained LDMs을 필요로 하는 한계가 존재한다.</p>
<p>따라서 본 연구는 추가 학습없이 Stable Diffusion(SD)이나 SD-LoRA 등에 plug-in 해서 사용할 수 있는 LCM-LoRA를 제안한다. LCM-LoRA는 새로운 종류의 neural network 기반 PF-ODE Solver이며, 강력한 일반화 성능을 보여준다.</p>
</section>
</section>
<section id="related-work">
<h2>2. Related Work<a class="headerlink" href="#related-work" title="Permalink to this heading">#</a></h2>
<section id="consistency-models">
<h3>Consistency Models<a class="headerlink" href="#consistency-models" title="Permalink to this heading">#</a></h3>
<p>CMs은 sampling step을 획기적으로 줄이면서도 Quality를 유지할 수 있는 방법론이다.</p>
<figure class="align-default" id="id2">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_1.png"><img alt="Consistency Models" class="bg-primary mb-1" src="../../_images/LCM-LoRA_1.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 560 </span><span class="caption-text">Consistency Models</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>CMs의 핵심은 PF-ODE의 궤적의 points가 solution에 mapping 되는 function <span class="math notranslate nohighlight">\((f: (x_t,t) \mapsto x_\epsilon)\)</span>을 추정하는 것이다.
쉽게 말해 어떤 step의 noise image 던지 <span class="math notranslate nohighlight">\(x_0\)</span> (정확히는 <span class="math notranslate nohighlight">\(x_\epsilon\)</span>)의 결과가 나오는 function을 추정한다. 또한 각 timestep에 관한function의 결과값은 self-consistency를 만족해야 한다.</p>
<div class="math notranslate nohighlight">
\[
f(x_t,t)=f(x_{t'},t'), \forall t, t' \in [\epsilon, T]. 
\]</div>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span>은 매우작은 양수 값이다. 이때 <span class="math notranslate nohighlight">\(f_\theta(x,\epsilon)=x\)</span>를 만족하는 model <span class="math notranslate nohighlight">\(f_\theta\)</span>는 다음과 같이 정의한다:</p>
<div class="math notranslate nohighlight">
\[
f_\theta(x,t)=c_{skip}(t)x+c_{out}(t)F_\theta(x,t).
\]</div>
<p><span class="math notranslate nohighlight">\(c_{skip}(\epsilon)=1\)</span>, <span class="math notranslate nohighlight">\(c_{out}(\epsilon)=0\)</span> 이기 때문에 <span class="math notranslate nohighlight">\(f_\theta(x,\epsilon)=x\)</span>를 만족한다. 위 수식은 미분 가능함을 증명하기 위한 수식이다. <span class="math notranslate nohighlight">\(F_\theta\)</span>는 심층신경망을 의미한다.</p>
<p>CMs은 scratch부터 학습하는 방식과 Distillation 방식으로 나뉘는데 보편적으로 Distillation이 사용된다. Distillation 방식은 지수평균이동(Exponential Moving Average, EMA)를 통해 self-consistency를 학습할 수 있다:</p>
<div class="math notranslate nohighlight">
\[
L(\theta,\theta^-;\Phi)=\mathbb{E}_{x,t}\bigg[d\bigg(f_\theta(x_{t_{n+1}},t_{n+1}),f_{\theta^-}(\hat{x}^\phi_{t_n},t_n)\bigg)\bigg].
\]</div>
<p><span class="math notranslate nohighlight">\(\theta^-\)</span>는 <span class="math notranslate nohighlight">\(\theta\)</span>에 대한 EMA를 의미하며 <span class="math notranslate nohighlight">\(d(\cdot, \cdot)\)</span>은 두 sample 사이의 거리를 측정하는 지표이다. <span class="math notranslate nohighlight">\(\hat{x}^{\phi}_{t_n}\)</span>는 <span class="math notranslate nohighlight">\(x_{t_{n+1}}\)</span>에 대한 <span class="math notranslate nohighlight">\(x_{t_n}\)</span>을 추정한 값으로 다음과 같다:</p>
<div class="math notranslate nohighlight">
\[
\hat{x}^{\phi}_{t_n} \leftarrow x_{t_{n+1}}+(t_n-t_{n+1})\Phi(x_{t_{n+1}},t_{n+1};\phi)
\]</div>
<p><span class="math notranslate nohighlight">\(\Phi\)</span>는 numerical PF-ODE를 의미한다. (보통 DDIM을 사용하는 것 같다) 즉 <span class="math notranslate nohighlight">\(x_{t_n}\)</span>을 PF-ODE로 예측한 값을 입력으로 하는 예측값과 <span class="math notranslate nohighlight">\(x_{t_{n+1}}\)</span>을 입력으로 하는 예측값이 같도록 self-consistency를 비교하는 것이 핵심이다.</p>
</section>
<section id="latent-consistency-models">
<h3>Latent Consistency Models<a class="headerlink" href="#latent-consistency-models" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id3">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_2.png"><img alt="Latent Diffusion Models" class="bg-primary mb-1" src="../../_images/LCM-LoRA_2.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 561 </span><span class="caption-text">Latent Diffusion Models</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>LCMs은 CMs에 condition을 추가해주고 <span class="math notranslate nohighlight">\(F_\theta(x,t)\)</span>를 <span class="math notranslate nohighlight">\(\epsilon-Prediction\)</span>의 수식으로 치환한다. (<span class="math notranslate nohighlight">\(\mu\)</span>나 <span class="math notranslate nohighlight">\(v\)</span> prediction을 사용해도 됨.) 추가로 LDMs 기반이기 때문에 latent <span class="math notranslate nohighlight">\(z\)</span>에 대한 수식으로 변경해준다.</p>
<div class="math notranslate nohighlight">
\[f_\theta(z,c,t)=c_{skip}(t)z+c_{out}(t)\bigg(\frac{z-\sigma(t)\hat{\epsilon}_\theta(z,c,t)}{\alpha(t)}\bigg). (\epsilon-Prediction)\]</div>
<div class="math notranslate nohighlight">
\[L_{CD}(\theta,\theta^-;\psi)=\mathbb{E}_{z,c,n}\bigg[d\bigg(f_\theta(z_{t_{n+1}},c,t_{n+1}),f_{\theta^-}(\hat{z}^\psi_{t_n},c,t_n)\bigg)\bigg].\]</div>
<p><span class="math notranslate nohighlight">\(n\)</span>은 timestep이지만 기존<span class="math notranslate nohighlight">\(t\)</span>와는 다른 timestep <span class="math notranslate nohighlight">\([t,T]\)</span>에 대한 하위 간격이다. <span class="math notranslate nohighlight">\((t_1=\epsilon&lt;t_2&lt;...&lt;t_N=T)\)</span></p>
<div class="math notranslate nohighlight">
\[t_i=\bigg(\epsilon^{1/\rho}+\frac{i-1}{N-1}(T^{1/\rho}-\epsilon^{1/\rho})\bigg)^\rho, \rho=7\]</div>
<section id="cms">
<h4>CMs과 차이점<a class="headerlink" href="#cms" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>LDMs 기반 모델이다.</p></li>
<li><p>LCMs는 CMs와 다르게 Classifier-free Guidance(CFG)를 포함한 Distillation도 정의되어있다.(<span class="math notranslate nohighlight">\(\tilde{\epsilon}_\theta\)</span>)</p></li>
<li><p>LCMs는 <span class="math notranslate nohighlight">\(t_n\)</span>과 <span class="math notranslate nohighlight">\(t_{n+1}\)</span>의 차이가 너무 적어 학습의 수렴이 늦어지게 된다 가정하고 <span class="math notranslate nohighlight">\(t_n\)</span>과 <span class="math notranslate nohighlight">\(t_{n+k}\)</span>의 consistency를 비교하는 Skipping timestep 방법을 제시했다. (k는 trade-off를 가지며 최적의 값은 20으로 지정.)</p></li>
<li><p>Latent Consistency Finetuning: 새로운 데이터셋에 대해 distillation할 때 LDMs를 학습 할 필요 없이 LCMs의 Consistency Distillation만 학습하여 사용할 수 있다.</p></li>
</ul>
<p>(자세한 내용은 <a class="reference external" href="https://pseudo-lab.github.io/pseudodiffusers/docs/review/latent_consistency_models.html">LCMs review</a>를 참고)</p>
</section>
</section>
</section>
<section id="parameter-efficient-fine-tuning">
<h2>Parameter-Efficient Fine-Tuning<a class="headerlink" href="#parameter-efficient-fine-tuning" title="Permalink to this heading">#</a></h2>
<p>Parameter-Efficient Fine-Tuning(PEFT)이란 파라미터를 효율적으로 사용하면서 fine-tuning 할수 있는 연구를 의미한다. Knowledge Distillation, Pruning, Quantization 등이 있다.</p>
<p>본 연구에서는 PERF 기법 중 RoLA를 사용했다.</p>
<section id="low-rank-adaptation">
<h3>Low Rank Adaptation<a class="headerlink" href="#low-rank-adaptation" title="Permalink to this heading">#</a></h3>
<p>기존에 pre-trained 된 가중치 <span class="math notranslate nohighlight">\(\Phi_0\)</span>에 대하여 새로운 task에 fine-tuning하는 모델 <span class="math notranslate nohighlight">\(P_\Phi(y|x)\)</span>는 다음과 같이 가중치가 업데이트 된다. (<span class="math notranslate nohighlight">\(\Phi_0+\Delta\Phi\)</span>)</p>
<div class="math notranslate nohighlight">
\[\underset{\Phi}{max}\sum_{(x,y)\in Z}\sum^{|y|}_{t=1}\log{(P_\Phi(y_t|x,y&lt;t))}\]</div>
<p>LLM이나 Stable Diffusion과 같은 대규모 모델은 새로운 task로 fine-tuning 시 매우 큰 차원의 모델 파라미터를 다시 학습하기 때문에 매우 큰 Cost가 생긴다. (시간적, 자원적) 이때 weight의 차원은 줄이면서 변화량을 기록하는 또다른 weight를 만들어 더 효율적으로 계산하는 방식은 다음과 같이 나타낼 수 있다: (파라미터 <span class="math notranslate nohighlight">\(\Theta\)</span>에 대해 <span class="math notranslate nohighlight">\(\Delta\Phi=\Delta\Phi(\Theta), |\Theta|&lt;&lt;|\Phi_0|\)</span>)</p>
<div class="math notranslate nohighlight">
\[\underset{\Phi}{max}\sum_{(x,y)\in Z}\sum^{|y|}_{t=1}\log{(P_{\Phi_0+\Delta\Phi(\Theta)}(y_t|x,y&lt;t))}\]</div>
<p>즉 기존의 잘 학습된 weight는 그대로 두고 low rank로 decomposition 된 weight만 optimization 하는 방법론을 Low Rank Adaptation(LoRA)라고 한다.</p>
<figure class="align-default" id="id4">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_3.png"><img alt="Low Rank Adaptation" class="bg-primary mb-1" src="../../_images/LCM-LoRA_3.png" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 562 </span><span class="caption-text">Low Rank Adaptation</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>위의 그림과 같이 원본 모델 weight는 freeze, LoRA는 rank를 r로 낮추어 finetuning한다. 이때 LoRA의 A는 random Gauissian으로, B는 zero로 weight initializing 한다.</p>
<figure class="align-default" id="id5">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_4.png"><img alt="Low Rank Adaptation matrix" class="bg-primary mb-1" src="../../_images/LCM-LoRA_4.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 563 </span><span class="caption-text">Low Rank Adaptation matrix</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>위 그림처럼 기존에는 d x d의 매우 큰 weight를 finetuning 해야 했지만, LoRA는 r만큼 압축된 weight matrix만 finetuning 하면 되기 때문에 훨씬 효율적이고 때에 따라 Fully fine-tuning 하는 방법들보다 더 좋은 성능을 보여주기도 한다. (그림은 <a class="reference external" href="https://ffighting.net/deep-learning-paper-review/language-model/lora/">이곳</a>을 참고하였습니다.)</p>
<p>원본 논문의 LoRA는 LLM을 target으로 만들어졌기 때문에 Transformer의 query, key, value에 대한 parameter로 사용하였지만 Diffusion이나 다른 모델의 finetuning시에도 간단하게 사용 가능하다.</p>
</section>
</section>
<section id="task-arithmetic-in-pretrained-models">
<h2>Task Arithmetic in Pretrained Models<a class="headerlink" href="#task-arithmetic-in-pretrained-models" title="Permalink to this heading">#</a></h2>
<p>task Arithmetic은 특정 task에서 학습된 Model의 가중치를 task vector라 보고 각 task vector를 조합하여 새로운 task vector를 생성하는 방법론이다.</p>
<figure class="align-default" id="id6">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_5.png"><img alt="Task Arithmetic" class="bg-primary mb-1" src="../../_images/LCM-LoRA_5.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 564 </span><span class="caption-text">Task Arithmetic</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>pre-trained parameter를 <span class="math notranslate nohighlight">\(\theta_{pre}\)</span>, fine-tuning parameter를 <span class="math notranslate nohighlight">\(\theta_{ft}\)</span>라고 할때 task vector <span class="math notranslate nohighlight">\(\tau\)</span>는 <span class="math notranslate nohighlight">\(\theta_{ft}-\theta_{pre}\)</span>로 정의할 수 있다.
이를 다양하게 조합하고 특히 d)처럼 task 간 analogy를 고려하여 연산하는 경우 새로운 task에 대한 성능을 높일 수 있다.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="lcm-lora">
<h1>3. LCM-LoRA<a class="headerlink" href="#lcm-lora" title="Permalink to this heading">#</a></h1>
<section id="lora-distillation-for-lcm">
<h2>3.1 LoRA Distillation for LCM<a class="headerlink" href="#lora-distillation-for-lcm" title="Permalink to this heading">#</a></h2>
<p>LCMs의 Latent Consistency Distillation에 대한 pseudo code는 다음과 같다:</p>
<figure class="align-default" id="id7">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_6.png"><img alt="Latent Consistency Distillation" class="bg-primary mb-1" src="../../_images/LCM-LoRA_6.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 565 </span><span class="caption-text">Latent Consistency Distillation</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>논문의 저자는 LCMs의 Distillation은 LDMs에 관한 일종의 fine-tuning으로 보고 LoRA를 적용하는 방법을 제안하였다.
pre-trained 된 weight matrix <span class="math notranslate nohighlight">\(W_0\)</span>에 대하여 기울기 업데이트는 <span class="math notranslate nohighlight">\(W_0+\Delta W=W_0+BA, W_0\in \mathbb{R}^{d\times k}, B\in \mathbb{R}^{d\times r}, A\in \mathbb{R}^{r\times k}\)</span> 로 표현할 수 있으며 rank <span class="math notranslate nohighlight">\(r \leq \min{(d,k)}\)</span> 로 작은 값을 갖는다. <span class="math notranslate nohighlight">\(W_0\)</span>의 weight는 고정되며 input <span class="math notranslate nohighlight">\(x\)</span> 에 대한 forward pass는 다음과 같다:</p>
<div class="math notranslate nohighlight">
\[h=W_0x+\Delta Wx=W_0x+BAx. \tag{1}\]</div>
<p>위와같이 LCMs에 LoRA를 적용할 경우 학습 parameter를 크게 줄일 수 있어 효율적이다.</p>
<figure class="align-default" id="id8">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_7.png"><img alt="compare trainable parameter" class="bg-primary mb-1" src="../../_images/LCM-LoRA_7.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 566 </span><span class="caption-text">compare trainable parameter</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>따라서 LCM-loRA는 기존 LCMs 보다 더 큰 모델의 훈련과 실사용이 가능하다. LCMs의 경우 SD-V1.5나 SD-V2.1의 base Stable Diffusion을 사용했지만, LCM-LoRA는 SDXL과 SSD-1B(Segmind)을 확장하여 사용하였다. large Model에서도 LCD을 적용했을 때 잘 적응하는 모습을 볼 수 있었다.</p>
<figure class="align-default" id="id9">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_8.png"><img alt="1024 x 1024 resolution image results with CFG scale w=7.5" class="bg-primary mb-1" src="../../_images/LCM-LoRA_8.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 567 </span><span class="caption-text">1024 x 1024 resolution image results with CFG scale w=7.5</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="lcm-lora-as-universal-acceleration-module">
<h2>3.2 LCM-LoRA as Universal Acceleration Module<a class="headerlink" href="#lcm-lora-as-universal-acceleration-module" title="Permalink to this heading">#</a></h2>
<p>LCM-LoRA는 sampling step을 줄이는 distillation에 LoRA를 적용하였다. LoRA는 이외에도 custionized datasets에 대해 fine-tuning할 때 주로 쓰이는데 이같은 style에 대한 LoRA와 LCM-LoRA가 추가 학습없이 바로 합쳐져 사용할 수 있음을 발견했다. 저자는 이 발견이 task arithmetic에 대한 관점으로 해석할 수 있다고 주장하였다.</p>
<figure class="align-default" id="id10">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_9.png"><img alt="Style-LoRA with LCM-LoRA" class="bg-primary mb-1" src="../../_images/LCM-LoRA_9.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 568 </span><span class="caption-text">Style LoRA with LCM-LoRA</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>LCM-LoRA의 fine-tuned parameter를 <span class="math notranslate nohighlight">\(\tau_{LCM}\)</span>이라 할 때, <span class="math notranslate nohighlight">\(\tau_{LCM}\)</span>은 acceleration vector라 할수 있다. 그리고 custom dataset에서 학습한 LoRA의 fine-tuned parameter를 <span class="math notranslate nohighlight">\(\tau'\)</span>이라 할 때, <span class="math notranslate nohighlight">\(\tau'\)</span>은 style vector라 할 수 있다. LCMs를 통해 custom dataset에 대한 image를 생성할 때, 파라미터는 다음과 같이 조합된다:</p>
<div class="math notranslate nohighlight">
\[\theta'_{LCM}=\theta_{pre}+\tau'_{LCM} \tag{2}\]</div>
<div class="math notranslate nohighlight">
\[\tau'_{LCM}=\lambda_1\tau'+\lambda_2\tau_{LCM} \tag{3}\]</div>
<p>파라미터는 단순한 선형 결합을 통해 이루어지며 <span class="math notranslate nohighlight">\(\lambda_1\)</span>과 <span class="math notranslate nohighlight">\(\lambda_2\)</span>는 하이퍼파라미터다. 추가적인 학습없이 다음과 같은 결과를 얻을 수 있었다:</p>
<figure class="align-default" id="id11">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/LCM-LoRA_10.png"><img alt="fine-tuning with LCM-LoRA" class="bg-primary mb-1" src="../../_images/LCM-LoRA_10.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 569 </span><span class="caption-text">fine-tuning with LCM-LoRA</span><a class="headerlink" href="#id11" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>4. Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>training-free acceleration module인 LCM-LoRA를 제안.</p></li>
<li><p>PF-ODE를 예측하며 Stable Diffusion 및 SD LoRA에 fast inference, minimal step을 제공함.</p></li>
<li><p>강력한 일반화 성능 증명.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="one-step-image-translation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">One-Step Image Translation with Text-to-Image Models</p>
      </div>
    </a>
    <a class="right-next"
       href="MimicBrush.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">MimicBrush: Zero-shot Image Editing with Reference Imitation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#proposal">Proposal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">기존 연구의 한계점</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lcms">LCMs 기반 연구</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-models">Consistency Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-consistency-models">Latent Consistency Models</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cms">CMs과 차이점</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-efficient-fine-tuning">Parameter-Efficient Fine-Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#low-rank-adaptation">Low Rank Adaptation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-arithmetic-in-pretrained-models">Task Arithmetic in Pretrained Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lcm-lora">3. LCM-LoRA</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-distillation-for-lcm">3.1 LoRA Distillation for LCM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lcm-lora-as-universal-acceleration-module">3.2 LCM-LoRA as Universal Acceleration Module</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">4. Conclusion</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PseudoLab
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>