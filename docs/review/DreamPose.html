

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion &#8212; Text-to-Image Generation-feat-Diffusion</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/review/DreamPose';</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis" href="NeRF.html" />
    <link rel="prev" title="DreaMoving" href="DreaMoving.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/PseudoLab_logo.png" class="logo__image only-light" alt="Text-to-Image Generation-feat-Diffusion - Home"/>
    <script>document.write(`<img src="../../_static/PseudoLab_logo.png" class="logo__image only-dark" alt="Text-to-Image Generation-feat-Diffusion - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to PseudoDiffusers!!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary Works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="DDPM.html">DDPM</a></li>




<li class="toctree-l1"><a class="reference internal" href="DDIM.html">DDIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Study_on_the_Evaluation_of_Generative_Models.html">A Study on the Evaluation of Generative Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cycleGAN.html">CycleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyleGAN.html">StyleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_beats_GANs.html">Diffusion Models Beat GANs on Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalle.html">DALL-E</a></li>
<li class="toctree-l1"><a class="reference internal" href="DALLE2.html">DALL-E 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="dreambooth.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="ControlNet.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Latent_Diffusion_Model.html">Latent Diffusion Model</a></li>

<li class="toctree-l1"><a class="reference internal" href="Textual_Inversion.html">Textual Inversion</a></li>








<li class="toctree-l1"><a class="reference internal" href="CustomDiffusion.html">Custom Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="LoRA.html">LoRA</a></li>









<li class="toctree-l1"><a class="reference internal" href="I-DDPM.html">I-DDPM</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyO.html">StyO</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen_editor.html">Imagen Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDEdit.html">SDEdit</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDXL.html">SDXL</a></li>
<li class="toctree-l1"><a class="reference internal" href="t2i_adapter.html">T2I-Adapter</a></li>
<li class="toctree-l1"><a class="reference internal" href="HyperDreamBooth.html">HyperDreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="CM3leon.html">CM3leon</a></li>

<li class="toctree-l1"><a class="reference internal" href="Synthetic_Data_from_Diffusion_Models_Improves_ImageNet_Classification.html">Synthetic Data from Diffusion Models Improves ImageNet Classification</a></li>






<li class="toctree-l1"><a class="reference internal" href="GLIDE.html">GLIDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="BBDM.html">BBDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Diffusion_Model_is_Secretly_a_Zero_Shot_Classifier.html">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="progressive_distillation.html">Progressive Distillation for Fast Sampling of Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ConceptLab.html">ConceptLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion_models_already_have_a_Semantic_Latent_Space.html">Diffusion Models already have a Semantic Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="Muse.html">Muse</a></li>


<li class="toctree-l1"><a class="reference internal" href="GIGAGAN.html">Scaling up GANs for Text-to-Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="consistency_models.html">Consistency Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="latent_consistency_models.html">Latent Consistency Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Video Generation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Make_A_Video.html">Make A Video</a></li>
<li class="toctree-l1"><a class="reference internal" href="VideoLDM.html">VideoLDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Animate_Anyone.html">Animate Anyone</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreaMoving.html">DreaMoving</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3D Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NeRF.html">NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="3DGS.html">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../experiments/js_exp.html">Synthetic Data with Stable Diffusion for Foliar Disease Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/swjo_exp.html">Training DreamBooth on Naver Webtoon Face Dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion/issues/new?title=Issue%20on%20page%20%2Fdocs/review/DreamPose.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/review/DreamPose.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction"><strong>1. Introduction</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">2.1. Diffusion models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#still-image-animation">2.2. Still Image Animation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fashion-image-synthesis">2.3. Fashion Image Synthesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models-for-video-synthesis">2.4. Diffusion Models for Video Synthesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditioning-mechanisms-for-diffusion-models">2.5. Conditioning Mechanisms for Diffusion Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background">3. Background</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#method"><strong>4. Method</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview"><strong>4.1. Overview</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">4.2. Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-clip-vae-encoder"><strong>4.2.1 Split CLIP-VAE Encoder</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modified-unet">4.2.2 Modified UNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning">4.2.3 <strong>Finetuning</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pose-and-image-classifier-free-guidance">4.4. Pose and Image Classifier-Free Guidance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">5. Experiments</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details">5.1.  Implementation Details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">5.2. Dataset</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">6. Results</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparisons">6.1. Comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantitative-analysis">6.1.1 Quantitative Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-analysis">6.2.2 Qualitative Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-studies">6.2. Ablation Studies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-input-images">6.3. Multiple Input Images</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-future-work"><strong>7. Limitations &amp; Future Work</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">8. Conclusion</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper:  <a class="reference external" href="https://arxiv.org/abs/2304.06025">https://arxiv.org/abs/2304.06025</a></p></li>
<li><p>Project: <a class="reference external" href="https://grail.cs.washington.edu/projects/dreampose/">https://grail.cs.washington.edu/projects/dreampose/</a></p></li>
</ul>
</li>
<li><p><strong>Author:</strong> <a class="reference external" href="https://www.linkedin.com/in/jeonghwa-yoo-8403a716b">Jeonghwa Yoo</a></p></li>
<li><p><strong>Last updated on May. 08, 2023</strong></p></li>
</ul>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="dreampose-fashion-image-to-video-synthesis-via-stable-diffusion">
<h1>DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion<a class="headerlink" href="#dreampose-fashion-image-to-video-synthesis-via-stable-diffusion" title="Permalink to this heading">#</a></h1>
<figure class="align-default" id="id1">
<img alt="DreamPose_input_output" class="bg-primary mb-1" src="pics/DreamPose/00.png" />
<figcaption>
<p><span class="caption-number">Fig. 531 </span><span class="caption-text">DreamPose 입출력</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<aside>
💡핵심 요약
- 입력: 이미지, 포즈 시퀀스 / 출력: 입력 이미지의 사람이 포즈 시퀀스에 따라 움직이는 듯한 비디오 영상
- 이미지와 포즈 시퀀스가 주어지면 사람과 옷감의 움직임을 모두 포함하는 비디오를 합성하는 DreamPose를 제안하였다.
- 사전 학습된 스테이블 디퓨전을 파인튜닝하여 해당 태스크를 수행 하였다.
- 입력 이미지의 특성을 잘 포착하기 위해 CLIP 인코더와 VAE 인코더를 도입하였고, 이를 통합하는 Adapter를 도입하여 스테이블 디퓨전의 U-Net의 조건으로 넣어줬다.
- 포즈 시퀀스를 잘 반영하기 위해 연속적인 5개의 프레임을 노이즈와 concat하여 디노이징 UNet에 입력으로 주었다.
- 두 단계의 파인튜닝 전략을 사용하였다.
    - 첫 번째 파인튜닝: 전체 데이터셋에 대해 파인튜닝
    - 두 번째 파인튜닝: 피사체에 대해 파인튜닝
- 듀얼 classfier-free guidance를 사용하여 입력 이미지와 포즈 시퀀스에 대한 충실도의 강도를 조정한다.
- 패션 비디오 합성 태스크에 대해서 SOTA 성능을 달성 하였다.
</aside>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1><strong>1. Introduction</strong><a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>DreamPose가 제안된 배경</p>
<ul>
<li><p>패션 사진은 온라인에 널리 퍼져 있지만, 전달할 수 있는 정보가 제한적이며 입었을 때 옷의 늘어진 모양이나 흐름 등 옷의 중요한 뉘앙스를 포착하지 못한다.</p></li>
<li><p>패션 동영상은 이러한 모든 디테일을 보여주기에 소비자의 의사 결정에 유용한 정보를 제공하지만, 동영상이 있는 상품은 매우 드물다.</p></li>
</ul>
</li>
<li><p>DreamPose</p>
<ul>
<li><p>본 논문에서는 포즈 시퀀스를 따라 패션 사진을 사실적인 애니메이션 비디오로 변환하는 방법인 DreamPose를 소개한다.</p></li>
<li><p>Stable diffusion을 기반으로 한다.</p></li>
<li><p>하나 이상의 사람 이미지와 포즈 시퀀스가 주어지면, 포즈 시퀀스를 따라 고품질 비디오를 생성한다.</p></li>
</ul>
</li>
<li><p>기존 비디오 생성 모델들의 문제점</p>
<ul>
<li><p>이미지 생성 디퓨전 모델은 좋은 결과를 보여주었지만, 비디오 생성 디퓨전 모델은 동일한 품질의 결과를 얻지 못했으며, 텍스처 움직임이나 카툰과 같은 모양으로 제한된 경우가 많다.</p></li>
<li><p>시간적 일관성이 떨어진다.</p></li>
<li><p>모션 jitter가 발생한다.</p></li>
<li><p>사실성(realism)이 부족하다.</p></li>
<li><p>대상 비디오의 움직임이나 세부적인 물체 모양을 제어할 수 없다.</p>
<ul>
<li><p>기존 모델이 주로 텍스트에 기반으로 하기 때문에</p></li>
</ul>
</li>
</ul>
</li>
<li><p>DreamPose의 접근법</p>
<ul>
<li><p>이미지 및 포즈 시퀀스를 조건으로 받는 방식을 사용하여 fidelity와 프레임 간 일관성을 높일 수 있다.</p></li>
<li><p>이미지 분포를 효과적으로 모델링하는 기존 사전 학습된 이미지 디퓨전 모델을 파인 튜닝하였다. → 이미지 애니메이션 태스크를 컨디셔닝 신호와 일치하는 이미지의 부분 공간을 찾는 것으로 단순화 할 수 있다.</p></li>
<li><p>해당 태스크를 위해 스테이블 디퓨전의 인코더와 컨디셔닝 메커니즘을 재설계하였다.</p></li>
<li><p>2-스테이지 파인튜닝 방식을 사용한다</p>
<ul>
<li><p>UNet과 VAE를 하나 혹은 여러 입력 이미지에 대해서 파인튜닝</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Contribution</p>
<ol class="arabic simple">
<li><p>DreamPose: 패션 이미지 애니메이션을 위해 이미지 및 포즈를 조건으로 하는 디퓨전 방식</p></li>
<li><p>프레임 간 시간적 일관성을 크게 향상 시키는 간단하지만 효과적인 포즈 컨디셔닝 방식</p></li>
<li><p>컨디셔닝 이미지의 fidelity를 높여주는 split CLIP-VAE 인코더</p></li>
<li><p>이미지의 fidelity와 새로운 포즈에 대한 일반화 사이의 균형을 효과적으로 맞추는 파인튜닝 전략</p></li>
</ol>
</li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="related-work">
<h1>2. Related Work<a class="headerlink" href="#related-work" title="Permalink to this heading">#</a></h1>
<section id="diffusion-models">
<h2>2.1. Diffusion models<a class="headerlink" href="#diffusion-models" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>디퓨전 모델은 최근 text-to-image, 비디오 생성, 3D 생성 태스크에서 인상적인 결과를 보여주고 있다.</p></li>
<li><p>하지만 이러한 모델을 처음부터 훈련하는 것은 비용이 많이 들고 많은 양의 데이터가 필요하다.</p></li>
<li><p>스테이블 디퓨전과 같은 latent diffusion 모델은 디퓨전과 디노이징 과정을 latent space에서 수행하기 때문에 계산 요구 사항과 훈련 시간을 대폭 줄일 수 있다.</p></li>
<li><p>스테이블 디퓨전과 그 사전 훈련된 체크포인트는 출시 이후 다양한 이미지 생성 작업에 사용되었다.</p></li>
<li><p>본 논문에서도 사전 훈련된 스테이블 디퓨전 모델을 활용하고, subject에 특화된 파인튜닝을 한다.</p></li>
</ul>
</section>
<section id="still-image-animation">
<h2>2.2. Still Image Animation<a class="headerlink" href="#still-image-animation" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>하나 이상의 입력 이미지로부터 동영상을 생성하는 태스크</p></li>
<li><p>기존에 디퓨전 기반이 아닌 접근 방식들은 배경 예측, 모션 표현, occlusion map이나 dense map 등 여러 개의 개별 네트워크로 구성되는 경우가 많았다.</p>
<ul>
<li><p>각 단계마다 별도의 훈련이 필요하고 ground-truth 모션이나 depth등의 ground-truth 데이터를 사용할 수 없거나 불완전할 가능성이 있다.</p></li>
<li><p>모션이 크고 복잡할 경우에 groud-truth에 대한 예측은 도출하기 더 어렵고 오류가 발생하기 쉽다.</p></li>
</ul>
</li>
<li><p>최근 여러 방법들은 엔드 투 엔드 싱글 네트워크 접근 방식을 탐구하고 있다. (예: optical flow and warping, cross-attention 모듈, NeRF 표현을 사용한 애니메이션이 가능한 3D 휴먼 생성 등)</p></li>
</ul>
</section>
<section id="fashion-image-synthesis">
<h2>2.3. Fashion Image Synthesis<a class="headerlink" href="#fashion-image-synthesis" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>기존 pose-guided 패션 이미지 합성 방법</p>
<ul>
<li><p>일반적으로 GAN을 기반으로 했고, optical flow에 의존해 이미지 특징을 포즈에 맞추는 방법을 사용했다. → 큰 포즈 변화, 가려진 영역 합성, 의상 스타일 보존에 어려움을 겪는 경우가 많다.</p></li>
<li><p>최근엔 어텐션 기반 메커니즘을 사용하여, 셀프/크로스 어텐션을 사용하여 이미지 특징을 목표 프레임에 맞추려고 하였다.</p></li>
<li><p>디퓨전 기반</p>
<ul>
<li><p>DiffFashion: 레퍼런스 이미지의 스타일을 트랜스퍼하여 의류 아이템을 편집하는 것을 목표로 한다.</p></li>
<li><p>PIDM: 포즈를 조건으로 넣어 사람 이미지를 생성한다 → 시간적 일관성을 위한 최적화는 하지 않는다.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="diffusion-models-for-video-synthesis">
<h2>2.4. Diffusion Models for Video Synthesis<a class="headerlink" href="#diffusion-models-for-video-synthesis" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>많은 text-to-video 디퓨전 모델은 text-to-image 디퓨전 모델을 활용하여 적용한다.</p></li>
<li><p>결과를 기대할만 하지만, 여전히 텍스트-이미지 모델과 같은 성능은 나오지 않고 있다.</p></li>
<li><p>프레임 간의 시간적 일관성을 유지하는 것과 사실적인 모션을 생성하는 것이 어렵다.</p></li>
<li><p>어떤 디퓨전 기반 방법들은 스크래치부터 학습하기 때문에 값비싼 컴퓨팅 리소스, 방대학 학습 데이터셋, 오랜 학습 시간이 필요하다.</p></li>
<li><p>Turn-A-Video는 텍스트와 이미지가 조건으로 들어왔을 때 비디오 생성을 위해 사전 학습된 텍스트-이미지 디퓨전 모델을 파인튜닝한다.  → 이전 방법들과 마찬가지로 텍스처 깜빡거림(textural flickering), 구조적인 불일치가 나타난다.</p></li>
<li><p>본 논문에서는 위의 문제를 해결하여 사람과 섬유의 움직임의 싱크를 맞추는 것을 목표로 한다.</p></li>
</ul>
</section>
<section id="conditioning-mechanisms-for-diffusion-models">
<h2>2.5. Conditioning Mechanisms for Diffusion Models<a class="headerlink" href="#conditioning-mechanisms-for-diffusion-models" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>텍스트를 조건으로 하는 이미지 생성 디퓨전 모델을 널리 사용되어 왔다.</p></li>
<li><p>텍스트 컨디셔닝은 높은 수준의 디테일을 조절하는 데는 효과적이지만, 사람과 의상의 정확한 identity나 포즈에 대한 풍부하고 상세한 정보를 제공하는 것은 어렵다.</p></li>
<li><p>여러 연구에서 사전 학습된 text-to-image 스테이블 디퓨전 모델을 위한 이미지 컨디셔닝 문제를 다루고 있다.</p>
<ul>
<li><p>이러한 모델에는 종종 어떤 종류의 텍스트 임베딩이 포함된다. (ex: DreamBooth: 피사체별 파인 튜닝을 수행하기 위해 고유한 텍스트 토큰을 학습)</p></li>
<li><p>기존 이미지와 동영상의 모양을 편집하기 위해 텍스트를 통합하기도 한다.</p></li>
<li><p>PIDM은 별도의 텍스처 인코더를 사용하여 이미지 텍스처를 인코딩하고 입력된 노이즈 이미지와 대상 포즈를 연결한다.</p></li>
</ul>
</li>
<li><p>DreamPose는 영상 속 피사체의 외형뿐만 아니라 구조와 움직임까지 제어할 수 있다.</p></li>
<li><p>PIDM과 마찬가지로 이미지 임베딩을 UNet의 크로스 어텐션 레이어에 직접 통합하지만, 이미지 임베딩에 대해 사전 학습된 두 개의 인코더(CLIP, VAE)를 혼합하여 사용한다. → 입력 노이즈에 연결된(concatenated) 멀티 포즈 입력 표현(multi-pose input representation)을 이용해 부드럽고 시간적으로 일관된 모션을 구현할 수 있다.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background">
<h1>3. Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h1>
<ul>
<li><p>디퓨전 모델</p>
<ul class="simple">
<li><p>디퓨전 모델은 품질, 다양성, 학습 안정성 측면에서 합성 태스크에서 GAN을 능가하는 최신 생성 모델이다.</p></li>
<li><p>표준 이미지 디퓨전 모델은 정규 분포된 랜덤 노이즈에서 이미지를 반복적으로 복원하는 방법을 학습한다.</p></li>
</ul>
</li>
<li><p>Latent diffusion model (ex. Stable Diffusion)</p>
<figure class="align-default" id="id2">
<img alt="latent diffusion" class="bg-primary mb-1" src="../../_images/Untitled.png" />
<figcaption>
<p><span class="caption-number">Fig. 532 </span><span class="caption-text">Latent Diffusion Model</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</li>
<li><p>오토인코더의 인코딩된 latent space에서 작동하므로 최소한의 품질을 희생하면서 계산 복잡성을 절약한다.</p></li>
<li><p>스테이블 디퓨전 모델은 VAE와 디노이징 UNet의 두 가지 모델로 구성된다.</p>
<ul>
<li><p>VAE 오토인코더</p>
<ul class="simple">
<li><p>인코더   <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>: 프레임 <span class="math notranslate nohighlight">\(x\)</span>를 컴팩트한 latent 표현 <span class="math notranslate nohighlight">\(z\)</span>로 추출 (<span class="math notranslate nohighlight">\(z=\mathcal{E}\)</span><span class="math notranslate nohighlight">\((x)\)</span>)</p></li>
<li><p>디코더 <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>: latent 표현에서 이미지를 복원 (<span class="math notranslate nohighlight">\(x’=\mathcal{D}(z)\)</span>)</p></li>
</ul>
</li>
<li><p>학습하는 동안, latent feature <span class="math notranslate nohighlight">\(z\)</span>는 결정론적 가우시안 프로세스에 의해 타임 스탬프 <span class="math notranslate nohighlight">\(T\)</span>로 디퓨즈되어 노이지 feature인 <span class="math notranslate nohighlight">\(\tilde{z}_T\)</span>를 만듦</p></li>
<li><p>원본 이미지를 복구하기 위해 각 타임스탬프에 해당하는 latent feature의 노이즈를 반복적으로 예측하도록 시간으로 컨디셔닝된 UNet이 학습 된다.</p></li>
<li><p>UNet의 목적 함수</p>
<div class="math notranslate nohighlight">
\[
        \begin{align}{\cal L}_{D M}=\mathbb{R}_{z,\epsilon\in{\mathcal{N}}(0,1)}[||\epsilon-\epsilon_{\theta}({\tilde{z}}_{t},t,c)]|_{2}^{2}]\end{align}
        \]</div>
<ul class="simple">
<li><p>c: 컨디셔닝 정보의 임베딩 (텍스트, 이미지, 세그멘테이션 마스크등, 스테이블 디퓨전에서는 CLIP 텍스트 인코더로부터 얻어짐</p></li>
</ul>
</li>
<li><p>예측된 latent <span class="math notranslate nohighlight">\(z’\)</span>은 예측된 이미지 <span class="math notranslate nohighlight">\(x’ = \mathcal{D}(z')\)</span>를 복구하도록 디코딩 된다.</p></li>
</ul>
</li>
<li><p>Classifier-free guidance</p>
<ul>
<li><p>Implicit classifier를 통해 예측된 노이즈 분포를 조건으로 주어진 분포로 밀어붙이는 샘플링 메커니즘이다.</p></li>
<li><p>이는 랜덤한 확률로 실제 조건으로 주어진 입력을 널 입력(∅)으로 대체하는 훈련 방식인 드롭아웃을 통해 달성된다.</p></li>
<li><p>인퍼런스하는 동안 조건으로 주어진 예측은 스칼라 가중치 s를 사용하여 unconditional한 예측을 조건부로 가이드하는 데 사용된다.</p>
<div class="math notranslate nohighlight">
\[
        \begin{align}\epsilon_{\theta}=\epsilon_{\theta}(\tilde{z}_{t},t,\emptyset)+s\cdot(\epsilon_{\theta}(\tilde{z}_{t},t,\mathrm{c})-\epsilon_{\theta}(\tilde{z}_{t},t,\emptyset))\end{align}
        \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon_{\theta}(\tilde{z}_{t},t,\emptyset)\)</span>: 조건이 없는 경우에 노이즈 벡터</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_{\theta}(\tilde{z}_{t},t,c)\)</span>: 조건이 있는 경우에 노이즈 벡터</p></li>
</ul>
<p>→ 조건을 Null로 줬을 때의 모델의 예측값과 조건을 줬을 때의 모델이 예측값을 보간한다.</p>
</li>
</ul>
</li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="method">
<h1><strong>4. Method</strong><a class="headerlink" href="#method" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>본 논문에서는  단일 이미지와 포즈 시퀀스로부터 사실적인 애니메이션 동영상을 만드는 것을 목표로 한다.</p></li>
<li><p>이를 위해 패션 동영상 컬렉션에서 사전 학습된 스테이블 디퓨전을 패션 동영상 컬렉션에 맞게 파인튜닝한다.</p></li>
<li><p>추가 컨디셔닝 신호(이미지 및 포즈)를 받고 동영상으로 볼 수 있는 시간적으로 일관된 콘텐츠를 출력하기 위해 스테이블 디퓨전의 구조를 조정하는 작업이 포함된다.</p></li>
</ul>
<section id="overview">
<h2><strong>4.1. Overview</strong><a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>입출력</p>
<ul>
<li><p>입력: 입력 이미지 <span class="math notranslate nohighlight">\(x_0\)</span>, 포즈 <span class="math notranslate nohighlight">\(\{p_1, …, p_n\}\)</span></p></li>
<li><p>출력: 비디오 <span class="math notranslate nohighlight">\(\{x'_1, …, x’_N\}\)</span> (<span class="math notranslate nohighlight">\(x’_i\)</span>:  입력 포즈 <span class="math notranslate nohighlight">\(p_i\)</span>에 해당하는 i 번째 예측된 프레임)</p></li>
</ul>
</li>
<li><p>입력 이미지와 포즈 시퀀스를 조건으로 하는 사전 훈련된 latent diffusion model을 사용한다.</p></li>
<li><p>추론 시에는 일반적인 디퓨전 샘플링 절차를 통해 각 프레임을 독립적으로 생성한다.</p>
<ul>
<li><p>균일하게 분포된 가우시안 노이즈로 시작하여 두 조건 신호로 디퓨전 모델을 반복적으로 쿼리하여  noisy latent의 노이즈를 제거한다.</p></li>
</ul>
</li>
<li><p>마지막으로 예측된 디노이즈된 latent <span class="math notranslate nohighlight">\(z’_i\)</span>를 디코딩하여 예측된 비디오 프레임 <span class="math notranslate nohighlight">\(x’_i=\mathcal{D}(z’_i)\)</span>를 만든다.</p></li>
</ul>
</section>
<section id="architecture">
<h2>4.2. Architecture<a class="headerlink" href="#architecture" title="Permalink to this heading">#</a></h2>
<ul>
<li><p>이미지 애니메이션을 위해 원래의 text-to-image 스테이블 디퓨전 모델을 수정하고 파인튜닝한다. (조건: 이미지, 포즈)</p></li>
<li><p>이미지 애니메이션의 목표</p>
<ol class="arabic simple">
<li><p>제공된 입력 이미지에 대한 충실도</p></li>
<li><p>시각적 품질</p></li>
<li><p>생성된 프레임의 전반적인 시간적인 안정성</p></li>
</ol>
</li>
<li><p>이러한 목표를 달성하기 위해 아키텍처를 아래와 같이 구성하였다.</p>
<figure class="align-default" id="id3">
<img alt="DreamPose Architecture" class="bg-primary mb-1" src="../../_images/021.png" />
<figcaption>
<p><span class="caption-number">Fig. 533 </span><span class="caption-text">DreamPose Architecture</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</li>
</ul>
<section id="split-clip-vae-encoder">
<h3><strong>4.2.1 Split CLIP-VAE Encoder</strong><a class="headerlink" href="#split-clip-vae-encoder" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id4">
<img alt="DreamPose Encoder" class="bg-primary mb-1" src="../../_images/031.png" />
<figcaption>
<p><span class="caption-number">Fig. 534 </span><span class="caption-text">DreamPose Encoder</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>해당 모듈-컨디셔닝 어댑터(custom conditioning adapter)의 필요성</p>
<ul>
<li><p>이미지를 조건으로 넣기 위한 이전 연구 (ex: <a class="reference external" href="https://arxiv.org/abs/2211.09800">InstructPix2Pix</a>)는 주로 조건으로 들어오는 이미지 신호를 디노이징 U-Net에 대한 입력 노이즈와 concat한다.</p></li>
<li><p>이는 원하는 출력 이미지와 공간적으로(spatially) 정렬된 조건 신호에 대한 조건화에 효과적이지만, DreamPose의 경우에는 네트워크가 입력 이미지와 공간적으로 정렬되지 않은 이미지를 생성하는 것을 목표로 한다.</p></li>
<li><p>따라서 해당 태스크에는 다른 방법이 필요하고, 이를 위해 맞춤형 컨디셔닝 어댑터를 구현하였다.</p></li>
<li><p>맞춤형 컨디셔닝 어댑터는 CLIP 텍스트 인코더를 사용하여 이미지 조건화를 대체하는 맞춤형 컨디셔닝 어댑터(custom conditioning adapter)를 구현하였다.</p></li>
<li><p>이 어댑터는 사전 학습된 CLIP 이미지 및 VAE 인코더에서 인코딩된 정보를 결합한다.</p></li>
</ul>
</li>
<li><p>디퓨전 기반 파인튜닝</p>
<ul>
<li><p>목표: 입력 신호를 원래 네트워크 학습에 사용된 신호와 최대한 유사하게 만들어 학습 기울기를 가능한 한 의미 있게 만드는 것 → 학습된 prior 값의 손실을 방지하는 데 도움이 된다.</p></li>
<li><p>이러한 이유로 대부분의 디퓨전 기반 파인튜닝 체계는 모든 원래 컨디셔닝 신호를 유지하고 새로운 컨디셔닝 신호와 상호 작용하는 네트워크 가중치를 0으로 초기화한다.</p></li>
</ul>
</li>
<li><p>VAE Encoder의 필요성</p>
<ul>
<li><p>스테이블 디퓨전이 텍스트 프롬프트의 CLIP 임베딩으로 컨디셔닝 되고 CLIP이 텍스트와 이미지를 공유 임베딩 스페이스(shared embedding space)로 인코딩한다는 점을 감안할 때 CLIP 컨디셔닝을 조건으로 주어진 이미지에서 파생된 임베딩으로 간단히 대체하는 것이 자연스러워 보일 수 있다.</p></li>
<li><p>하지만 실제로는 CLIP 이미지 임베딩만으로는 조건으로 주어진 이미지에서 세밀한 디테일을 캡처하기에 충분하지 않다.</p></li>
<li><p>따라서 스테이블 디퓨전의 VAE에서 인코딩된 latent 임베딩을 추가로 입력한다.</p></li>
<li><p>이를 통해 디퓨전의 출력 도메인과 일치하는 추가적인 장점을 가지게 된다.</p></li>
</ul>
</li>
<li><p>어댑터 <span class="math notranslate nohighlight">\(\mathcal{A}\)</span></p>
<ul>
<li><p>스테이블 디퓨전 아키텍처는 기본적으로 컨디셔닝 신호로 VAE latent를 지원하지 않기 때문에 어댑터 모듈 <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>를 추가한다.</p></li>
<li><p>해당 어댑터는 CLIP과 VAE 임베딩을 결합하여 네트워크의 일반적인 cross-attention 연산에 사용되는 하나의 임베딩을 생성한다.</p></li>
<li><p>이 어댑터는 두 신호를 함께  혼합하고 디노이징 U-Net의 cross-attention 모듈에서 예상하는 일반적인 모양으로 출력을 변환한다.</p></li>
</ul>
</li>
<li><p>디퓨전 기반 파인튜닝에서 언급했 듯이 학습에서 네트워크의 충격을 완화하기 위해 처음에는 VAE 임베딩에 해당하는 가중치는 0으로 설정되어 네트워크가 CLIP 임베딩으로만 학습을 시작한다.</p></li>
<li><p>최종 이미지 컨디셔닝 신호 <span class="math notranslate nohighlight">\(c_I\)</span>를 다음과 같이 정의한다.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\begin{align}c_{I}={\mathcal{A}}(c_{\mathrm{CLIP}},c_{\mathrm{VAE}})\end{align}
\]</div>
</section>
<section id="modified-unet">
<h3>4.2.2 Modified UNet<a class="headerlink" href="#modified-unet" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id5">
<img alt="Modified UNet" class="bg-primary mb-1" src="../../_images/041.png" />
<figcaption>
<p><span class="caption-number">Fig. 535 </span><span class="caption-text">Modified UNet</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>이미지 컨디셔닝과 달리 포즈 컨디셔닝은 이미지와 정렬 된다.</p></li>
<li><p>Noisy latent <span class="math notranslate nohighlight">\(\tilde{z}_i\)</span>를 타겟 포즈 표현 <span class="math notranslate nohighlight">\(c_p\)</span>와 concat한다.</p></li>
<li><p>실제 비디오에서 추정된 포즈의 노이즈를 고려하고 생성된 프레임에서의 시간적 일관성을 극대화하기 위해, <span class="math notranslate nohighlight">\(c_p\)</span>를 다섯 개의 연속된 포즈 프레임으로 구성하였다. 즉, <span class="math notranslate nohighlight">\(c_p = \{p_{i-2}, p_{i-1}, pi, p_{i+1}, p_{i+2}\}\)</span> → 개별 포즈로 네트워크를 학습하는 것보다 연속 포즈로 학습하면  전반적인 움직임의 부드러움과 시간적 일관성이 증가한다.</p></li>
<li><p>구조적으로 0으로 초기화된 10개의 추가 입력 채널을 받아들이도록 UNet 입력 레이어를 수정하고 noisy latent에 해당하는 원래 채널은 사전 학습된 가중치에서 수정되지 않는다.</p></li>
</ul>
</section>
<section id="finetuning">
<h3>4.2.3 <strong>Finetuning</strong><a class="headerlink" href="#finetuning" title="Permalink to this heading">#</a></h3>
<ul>
<li><p>스테이블 디퓨전 모델의 대부분의 레이어 weight는 미리 학습된 text-to-image 스테이블 디퓨전 체크포인트로 초기화된다.</p></li>
<li><p>이 때, CLIP 이미지 인코더는 별도의 미리 학습된 체크포인트에서 로드된다.</p></li>
<li><p>새로운 레이어는 초기에 새로운 컨디셔닝 신호가 네트워크 출력에 기여하지 않도록 초기화 된다.</p></li>
<li><p>초기화 후 DreamPose는 아래의 두 단계로 파인튜닝된다.</p>
<figure class="align-default" id="id6">
<img alt="Two-phase finetuning" class="bg-primary mb-1" src="../../_images/051.png" />
<figcaption>
<p><span class="caption-number">Fig. 536 </span><span class="caption-text">Two-phase Finetuning</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ol class="arabic simple">
<li><p>Full Dataset Finetuning</p>
<ul class="simple">
<li><p>전체 훈련 데이터셋에 대한 UNet과 어댑터 모듈을 파인 튜닝하여 입력 이미지 및 포즈와 일치하는 프레임을 합성한다.</p></li>
</ul>
</li>
<li><p>Subset-Specific Finetuning</p>
<ul class="simple">
<li><p>하나 이상의 피사체별 입력 이미지에 대해 UNet과 어댑터 모듈을 파인튜닝한 다음 VAE 디코더를 통해 기본 모델을 개선하여 추론에 사용되는 피사체별 맞춤형 모델을 생성한다.</p></li>
</ul>
</li>
</ol>
</li>
<li><p>다른 이미지 조건부 디퓨전 방법과 마찬가지로, 입력 이미지의 사람과 의상의 identity를 보존하고 프레임 간에 일관성을 유지하려면 샘플별 파인튜닝이 필수적이었다.</p></li>
<li><p>그러나 단순히 단일 프레임과 포즈 쌍에 대해 훈련하면 텍스처 고착(texture-sticking)과 같은 아티팩트가 출력 비디오에 발생한다.</p></li>
<li><p>이를 방지하기 위해 각 단계에서 랜덤 크롭을 추가하는 등의 방법으로 이미지-포즈쌍을 증강한다.</p></li>
<li><p>VAE 디코더를 파인튜닝하는 것이 더 선명하고 사실적인 디테일을 복구하는 데 중요하다.</p>
<figure class="align-default" id="id7">
<img alt="Importance of VAE finetuning" class="bg-primary mb-1" src="../../_images/061.png" />
<figcaption>
<p><span class="caption-number">Fig. 537 </span><span class="caption-text">VAE 파인튜닝의 중요성</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</li>
</ul>
</section>
</section>
<section id="pose-and-image-classifier-free-guidance">
<h2>4.4. Pose and Image Classifier-Free Guidance<a class="headerlink" href="#pose-and-image-classifier-free-guidance" title="Permalink to this heading">#</a></h2>
<ul>
<li><p>추론시 단일 입력 이미지와 피사체별 모델(subject-specific model)을 사용하는 일련의 포즈에서 프레임별로 동영상을 생성한다.</p></li>
<li><p>이중(dual) classifier-free guidance를 사용하여 추론 시에 이미지 컨디셔닝 <span class="math notranslate nohighlight">\(c_I\)</span>와 포즈 컨디셔닝 <span class="math notranslate nohighlight">\(c_p\)</span>의 강도를 조절한다.</p></li>
<li><p>이중 classfier-free guidance는 식 (3)에서 다음과 같이 수정된다.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    {\epsilon_{\theta}(z_{t},c_{I},c_{p})} &amp; {= \epsilon_{\theta}(z_{t},\emptyset,\emptyset)} \\
    {} &amp; {+\, s_{I}(\epsilon_{\theta}(z_{t},c_{I},\mathcal{\emptyset})-\epsilon_{\theta}(z_{t},\emptyset,\emptyset))} \\
    {} &amp; {+\, s_{p}(\epsilon_{\theta}(z_{t},\mathcal{c}_{I},\mathcal{c}_p)-\epsilon_{\theta}(z_{t},\mathcal{c}_{I},\emptyset))}
    \end{align*}
    \end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s_I\)</span>, <span class="math notranslate nohighlight">\(s_p\)</span>: 가이던스 웨이트</p></li>
<li><p><span class="math notranslate nohighlight">\(c_I\)</span>: 이미지 컨디셔닝</p></li>
<li><p><span class="math notranslate nohighlight">\(c_p\)</span>: 포즈 컨디셔닝</p></li>
</ul>
<p>→ 이미지 컨디셔닝이 있는 경우와 없는 경우의 노이즈 벡터 차이를 계산하고, 포즈 컨디셔닝이 있는 경우와 없는 경우의 노이즈 벡터 차이를 계산해서 이를 가이던스 웨이트를 통해 강도를 조정해서 반영</p>
</li>
<li><p><span class="math notranslate nohighlight">\(s_I\)</span>가 크면 입력 이미지에 높은 외관 충실도를 보장하고, <span class="math notranslate nohighlight">\(s_p\)</span>가 크면 입력 포즈에 대한 정렬을 보장한다.</p></li>
<li><p>이중 classifier-free guidance는 포즈 및 이미지 가이드를 강화하는 것 에외도, 피사체별 모델 파인튜닝 후 하나의 입력 포즈에 대한 오버피팅을 방지한다.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="experiments">
<h1>5. Experiments<a class="headerlink" href="#experiments" title="Permalink to this heading">#</a></h1>
<section id="implementation-details">
<h2>5.1.  Implementation Details<a class="headerlink" href="#implementation-details" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>입력 이미지 resolution: 512x512</p></li>
<li><p>GPU: NVIDIA A100 2개</p></li>
<li><p>첫 번째 훈련 단계</p>
<ul>
<li><p>전체 훈련 데이터셋 사용</p></li>
<li><p>5 epoch</p></li>
<li><p>5e-6 learning rate</p></li>
<li><p>배치사이즈: 16 (4 gradient accumulation step)</p></li>
<li><p>Dropout: 포즈 입력 5%, 이미지 입력 5%</p></li>
</ul>
</li>
<li><p>두 번째 훈련 단계</p>
<ul>
<li><p>특정 샘플 프레임 사용</p></li>
<li><p>500 step</p></li>
<li><p>1e-5 learning rate</p></li>
<li><p>Dropout 적용 X</p></li>
</ul>
</li>
<li><p>VAE 디코더 파인튜닝</p>
<ul>
<li><p>1500 step</p></li>
<li><p>5e-5 learning rate</p></li>
</ul>
</li>
<li><p>추론 시에는 PNDM 샘플러 사용 (100step)</p></li>
</ul>
</section>
<section id="dataset">
<h2>5.2. Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>UBC Fashion 데이터셋 사용</p></li>
<li><p>Split</p>
<ul>
<li><p>Train: 339개의 영상</p></li>
<li><p>Test: 100개의 영상</p></li>
</ul>
</li>
<li><p>각 비디오의 프레임 속도는 초당 30프레임이며 길이는 약 12초</p></li>
<li><p>학습 중에는 학습 비디오로부터 랜덤으로 프레임 쌍을 샘플링 하였다.</p></li>
<li><p>DensePose를 이용해서 포즈를 계산하였다.</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="results">
<h1>6. Results<a class="headerlink" href="#results" title="Permalink to this heading">#</a></h1>
<section id="comparisons">
<h2>6.1. Comparisons<a class="headerlink" href="#comparisons" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>공개적으로 사용 가능한 두 가지 최신 비디오 합성 방법인 MRAA(Motion Representations for Articulated Animation)과 Thin-Plate Spline Mothion Model(TPSMM)과 수치적 및 정성적인 비교를 하였다.</p></li>
<li><p>제공된 훈련 스크립트와 권장 에폭 수를 사용하여 두 가지 모델을 UBC 패션 데이터셋을 이용해서 스크래치부터 학습하였다.</p></li>
<li><p>평가를 위해서는 AVD 모드에서 제공된 테스트 스크립트를 사용하였다.</p></li>
<li><p>PIDM과도 정성적인 비교를 하였다. PIDM의 경우 훈련 스크립트를 사용할 수 없어서 DeepFashion 데이터셋에 대해 학습된 체크포인트를 통해 비교하였다.</p></li>
<li><p>100개의 디노이징 스텝을 사용하여 PIDM과 DreamPose를 실행하였다.</p></li>
</ul>
<section id="quantitative-analysis">
<h3>6.1.1 Quantitative Analysis<a class="headerlink" href="#quantitative-analysis" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id8">
<img alt="result 1" class="bg-primary mb-1" src="../../_images/071.png" />
<figcaption>
<p><span class="caption-number">Fig. 538 </span><span class="caption-text">정량적 성능 비교</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>256 픽셀 해상도의 100개의 고유한 패션 동영상으로 구성된 UBC 패션 테스트 셋에 대해 모든 모델을 테스트 하였다.</p></li>
<li><p>각 동영상에 대해 입력 프레임에서 최소 50프레임 이상 떨어져 있는 50개의 프레임을 추출하여 테스트하였다.</p></li>
<li><p>MRAA와 TPSMM은 모두 driving video에서 추출된 feautre에 의존하는 반면, DreamPose는 UV-포즈 시퀀스에만 의존한다는 점에 유의하라.</p></li>
<li><p>그럼에도 불구하고 DreamPose 모델은 네 가지 정량적 지표 모두에서 두 가지 방법보다 정량적으로 우수한 성능을 보였다.</p></li>
</ul>
</section>
<section id="qualitative-analysis">
<h3>6.2.2 Qualitative Analysis<a class="headerlink" href="#qualitative-analysis" title="Permalink to this heading">#</a></h3>
<figure class="align-default" id="id9">
<img alt="result 2" class="bg-primary mb-1" src="../../_images/081.png" />
<figcaption>
<p><span class="caption-number">Fig. 539 </span><span class="caption-text">정성적 성능 비교</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>MRAA와 TPSMM은 새로운 포즈를 취할 때 인물의  identity, 옷감 주름, 미세한 패턴이 손실되는 반면 DreamPose는 디테일을 정확하게 유지한다.</p></li>
<li><p>포즈를 크게 변경하는 동안 MRAA는 팔 다리가 분리 될 수 있다.</p></li>
<li><p>PIDM과의 비교</p>
<figure class="align-default" id="id10">
<img alt="result 3" class="bg-primary mb-1" src="../../_images/091.png" />
<figcaption>
<p><span class="caption-number">Fig. 540 </span><span class="caption-text">PIDM과의 비교</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>DreamPose는 얼굴의 identity와 의상 패턴 모두 더 충실도 높은 결과를 생성한다.</p></li>
<li><p>PIDM은 사실적인 얼굴을 합성하지만, 원본 인물의 identity와 일치하지 않고, identity와 옷차림이 프레임마다 달랐다. → PIDM이 비디오 합성에서는 잘 동작하지 않는다.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="ablation-studies">
<h2>6.2. Ablation Studies<a class="headerlink" href="#ablation-studies" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>아래 네 가지 변형에 대해 성능을 비교한다.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\text{Ours}_{\text{CLIP}}\)</span>: 듀얼 CLIP-VAE 인코더 대신에 사전 학습된 CLIP 이미지 인코더를 사용 → CLIP-VAE 인코더 효과 테스트</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Ours}_{\text{NO-VAE-FT}}\)</span>: VAE 디코더를 파인튜닝하지 않은 버전 → 디코더 파인튜닝 효과 테스트</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Ours}_{\text{1-pose}}\)</span>: 5개의 연결된 연속 포즈 대신 하나의 대상 포즈만 노이즈에 연결한 버전 → 연결된 5개의 프레임 효과 테스트</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Ours}_{\text{Full}}\)</span>: 논문에서 제안한 모든 방법이 다 적용된 DreamPose</p></li>
</ol>
</li>
</ul>
<p><strong>Quantitative Comparison</strong></p>
<figure class="align-default" id="id11">
<img alt="result 4" class="bg-primary mb-1" src="../../_images/10.png" />
<figcaption>
<p><span class="caption-number">Fig. 541 </span><span class="caption-text">Ablation Studies - 정량적 비교</span><a class="headerlink" href="#id11" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Qualitative Comparison</strong></p>
<figure class="align-default" id="id12">
<img alt="result 5" class="bg-primary mb-1" src="../../_images/11.png" />
<figcaption>
<p><span class="caption-number">Fig. 542 </span><span class="caption-text">Ablation Studies - 정성적 비교</span><a class="headerlink" href="#id12" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>기존의 스테이블 디퓨전에서는 인물의 identity에 대한 디테일을 보존할 수 없었다.</p></li>
<li><p>텍스트 인코더를 CLIP 인코더로 교체한 결과 대부분의 이미지 디테일은 캡처할 수 있지만, 여전히 외형에 대한 정보 손실이 발생한다.</p></li>
<li><p>VAE 디코더를 파인튜닝하면 디테일의 선명도가 크게 향상되고 입력 포즈에 대한 오버피팅이 발생하지 않는다.</p></li>
<li><p>한 가지 포즈만 입력하면 팔과 머리카락 주변에서의 피사체의 형태가 눈에 띄게 깜박이는 현상이 나타났다.</p></li>
</ul>
</section>
<section id="multiple-input-images">
<h2>6.3. Multiple Input Images<a class="headerlink" href="#multiple-input-images" title="Permalink to this heading">#</a></h2>
<ul>
<li><p>DreamPose는 피사체에 대한 입력 이미지를 여러 장 넣어서 파인튜닝할 수 있다.</p></li>
<li><p>피사체의 입력 이미지를 추가하면 품질과 시점의 일관성이 향상된다.</p>
<figure class="align-default" id="id13">
<img alt="result 6" class="bg-primary mb-1" src="../../_images/12.png" />
<figcaption>
<p><span class="caption-number">Fig. 543 </span><span class="caption-text">Multiple Input Images 결과</span><a class="headerlink" href="#id13" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="limitations-future-work">
<h1><strong>7. Limitations &amp; Future Work</strong><a class="headerlink" href="#limitations-future-work" title="Permalink to this heading">#</a></h1>
<ul>
<li><p>실패 사례</p>
<figure class="align-default" id="id14">
<img alt="result 7" class="bg-primary mb-1" src="../../_images/13.png" />
<figcaption>
<p><span class="caption-number">Fig. 544 </span><span class="caption-text">실패 사례 예시</span><a class="headerlink" href="#id14" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>드문 경우지만 팔다리가 옷 속으로 사라지고(왼쪽),  hallucinate feature가 보이고(중간)와  대상 포즈가 뒤를 향할 때 방향이 잘못 정렬 되는 경우(오른쪽)가 관찰된다.</p></li>
</ul>
</li>
<li><p>또한 단순한 패턴의 옷에서 사실적인 결과를 생성하지만 일부 결과는 크고 복잡한 패턴에서 약간의 깜박임 동작을 보인다.</p></li>
<li><p>다른 디퓨전 모델과 마찬가지로 파인튜닝 및 추론 시간이 GAN 또는 VAE에 비해 느리다.</p>
<ul class="simple">
<li><p>특정 피사체에 대한 모델 파인튜닝은 프레임당 18초의 렌더링 시간 외의 UNet의 경우 약 10분, VAE 디코더의 경우 약 20분이 소요된다.</p></li>
</ul>
</li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>8. Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>스틸 패션 이미지 애니메이션을 위한 새로운 디퓨전 기반 방법인 DreamPose를 제안하였다.</p></li>
<li><p>한 장의 이미지와 포즈 시퀀스가 주어졌을 때,  섬유, 패턴, 사람의 identity를 애니메이션 하는 사실적인 사실적인 패션 동영상을 생성하는 방법을 증명하였다.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DreaMoving.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">DreaMoving</p>
      </div>
    </a>
    <a class="right-next"
       href="NeRF.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction"><strong>1. Introduction</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">2.1. Diffusion models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#still-image-animation">2.2. Still Image Animation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fashion-image-synthesis">2.3. Fashion Image Synthesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models-for-video-synthesis">2.4. Diffusion Models for Video Synthesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditioning-mechanisms-for-diffusion-models">2.5. Conditioning Mechanisms for Diffusion Models</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#background">3. Background</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#method"><strong>4. Method</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview"><strong>4.1. Overview</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture">4.2. Architecture</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-clip-vae-encoder"><strong>4.2.1 Split CLIP-VAE Encoder</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modified-unet">4.2.2 Modified UNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#finetuning">4.2.3 <strong>Finetuning</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pose-and-image-classifier-free-guidance">4.4. Pose and Image Classifier-Free Guidance</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">5. Experiments</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details">5.1.  Implementation Details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">5.2. Dataset</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#results">6. Results</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparisons">6.1. Comparisons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quantitative-analysis">6.1.1 Quantitative Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#qualitative-analysis">6.2.2 Qualitative Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ablation-studies">6.2. Ablation Studies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-input-images">6.3. Multiple Input Images</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-future-work"><strong>7. Limitations &amp; Future Work</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">8. Conclusion</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PseudoLab
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>