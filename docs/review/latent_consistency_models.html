

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Latent Consistency Models &#8212; Text-to-Image Generation-feat-Diffusion</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/review/latent_consistency_models';</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Make A Video" href="Make_A_Video.html" />
    <link rel="prev" title="Consistency Models" href="consistency_models.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/PseudoLab_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/PseudoLab_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to PseudoDiffusers!!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary Works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="DDPM.html">DDPM</a></li>




<li class="toctree-l1"><a class="reference internal" href="DDIM.html">DDIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Study_on_the_Evaluation_of_Generative_Models.html">A Study on the Evaluation of Generative Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Generation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cycleGAN.html">CycleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyleGAN.html">StyleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_beats_GANs.html">Diffusion Models Beat GANs on Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalle.html">DALL-E</a></li>
<li class="toctree-l1"><a class="reference internal" href="DALLE2.html">DALL-E 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="dreambooth.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="ControlNet.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Latent_Diffusion_Model.html">Latent Diffusion Model</a></li>

<li class="toctree-l1"><a class="reference internal" href="Textual_Inversion.html">Textual Inversion</a></li>








<li class="toctree-l1"><a class="reference internal" href="CustomDiffusion.html">Custom Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="LoRA.html">LoRA</a></li>









<li class="toctree-l1"><a class="reference internal" href="I-DDPM.html">I-DDPM</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyO.html">StyO</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen_editor.html">Imagen Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDEdit.html">SDEdit</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDXL.html">SDXL</a></li>
<li class="toctree-l1"><a class="reference internal" href="t2i_adapter.html">T2I-Adapter</a></li>
<li class="toctree-l1"><a class="reference internal" href="HyperDreamBooth.html">HyperDreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="CM3leon.html">CM3leon</a></li>

<li class="toctree-l1"><a class="reference internal" href="Synthetic_Data_from_Diffusion_Models_Improves_ImageNet_Classification.html">Synthetic Data from Diffusion Models Improves ImageNet Classification</a></li>






<li class="toctree-l1"><a class="reference internal" href="GLIDE.html">GLIDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="BBDM.html">BBDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Diffusion_Model_is_Secretly_a_Zero_Shot_Classifier.html">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="progressive_distillation.html">Progressive Distillation for Fast Sampling of Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ConceptLab.html">ConceptLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion_models_already_have_a_Semantic_Latent_Space.html">Diffusion Models already have a Semantic Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="Muse.html">Muse</a></li>


<li class="toctree-l1"><a class="reference internal" href="GIGAGAN.html">Scaling up GANs for Text-to-Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="consistency_models.html">Consistency Models</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Latent Consistency Models</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Video Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Make_A_Video.html">Make A Video</a></li>
<li class="toctree-l1"><a class="reference internal" href="VideoLDM.html">VideoLDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Animate_Anyone.html">Animate Anyone</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreaMoving.html">DreaMoving</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamPose.html">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../experiments/js_exp.html">Synthetic Data with Stable Diffusion for Foliar Disease Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/swjo_exp.html">Training DreamBooth on Naver Webtoon Face Dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion/issues/new?title=Issue%20on%20page%20%2Fdocs/review/latent_consistency_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/review/latent_consistency_models.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Latent Consistency Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Latent Consistency Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">2. Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">Diffusion Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-models">Consistency Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3. Latent Consistency Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-distillation-in-the-latent-space">3.1 Consistency Distillation in the Latent Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stage-guided-distillation-by-solving-augmented-pf-ode">3.2 One-Stage Guided Distillation by solving augmented PF-ODE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accelerating-distillation-with-skipping-time-steps">3.3 Accelerating Distillation with Skipping Time Steps</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-consistency-fine-tuning-for-customized-dataset">3.4 Latent Consistency Fine-tuning for customized dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">4. Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-image-generation">4.1 Text-To-Image Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abulation-study">4.2 Abulation Study</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ode-solvers-skipping-step-schedule">ODE Solvers &amp; Skipping-Step Schedule</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-effect-of-guidance-scale-omega">The Effect of Guidance Scale <span class="math notranslate nohighlight">\(\omega\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downstream-consistency-fine-tuning-results">4.3 Downstream Consistency Fine-tuning Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/pdf/2310.04378">https://arxiv.org/pdf/2310.04378</a></p></li>
<li><p>Code: <a class="github reference external" href="https://github.com/luosiallen/latent-consistency-model">luosiallen/latent-consistency-model</a></p></li>
<li><p>Project Page: <a class="reference external" href="https://latent-consistency-models.github.io/">https://latent-consistency-models.github.io/</a></p></li>
</ul>
</li>
<li><p><strong>Author:</strong> Donghyun Han</p></li>
<li><p><strong>Last updated on May. 1, 2024</strong></p></li>
</ul>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="latent-consistency-models">
<h1>Latent Consistency Models<a class="headerlink" href="#latent-consistency-models" title="Permalink to this heading">#</a></h1>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Diffusion model은 다양한 분야에서 주목할만한 성과를 거두었지만  매우 느린 sampling 속도를 가지기 때문에 실시간 사용이 불가능하다. 이 같은 단점을 극복하기 위해 sampling 속도를 향상시키는 다양한 accelerating 방법이 제안되었다:</p>
<p> </p>
<ol class="arabic simple">
<li><p>ODE solver의 성능개선을 통해 10~20 step만으로도 좋은 성능을 가지는 방법.</p>
<ul class="simple">
<li><p>DPM-Solver (<a class="reference external" href="https://arxiv.org/pdf/2206.00927">lu et al.</a>)</p></li>
</ul>
</li>
<li><p>사전 학습된 Diffusion model을 몇 step만으로도 추론할수 있도록 <strong>distillation</strong>하는 방법.</p>
<ul class="simple">
<li><p>PD (Progressive Distillation). → 2 stage (<a class="reference external" href="https://arxiv.org/pdf/2202.00512">Salimans et al.</a>)</p></li>
<li><p>On Distillation of Guided Diffusion Models. (<a class="reference external" href="https://arxiv.org/pdf/2210.03142">Meng et al.</a>)</p></li>
<li><p>Consistency Models (<a class="reference external" href="https://arxiv.org/pdf/2303.01469">Song et al.</a>)</p></li>
</ul>
</li>
</ol>
<p>이중 특히 Consistency Models은 ODE-trajectory에 대한 일관성을 갖도록 하는 모델로서, single step만으로도 이미지를 생성할 수 있기 때문에 반복적인 계산이 필요하지 않다. 그러나 이 모델 또한 2가지의 단점을 가지고 있다:</p>
<p> </p>
<ol class="arabic simple">
<li><p>Pixel space의 Flow-based Model이기 때문에 <strong>high-resolution 이미지 생성</strong>에 적합하지 않음.</p></li>
<li><p>Conditional(Classifer-free Guidance)한 이미지 생성을 고려하지 않아 <strong>text2img</strong>에 적합하지 않음.</p></li>
</ol>
<hr class="docutils" />
<p>본 논문의 제안점은 다음 3가지다:</p>
<p> </p>
<ul class="simple">
<li><p>빠르고 high-resolution 이미지를 생성하기 위한 Latent Consistency Models(LCMs)를 제안한다. LCMs은 영상의 latent space에 Consistency Models 개념을 적용해 매우 적은 step 만으로도 <strong>고품질의 이미지</strong>를 생성할 수 있다.</p></li>
<li><p>guided consistency distillation을 통해 Stable Diffusion을 매우 적은 step(1~4)으로 sampling 할 수 있는 방법을 제공한다. <strong>Skipping-Step</strong>이라는 테크닉을 통해 학습을 가속화 한다. 2, 4 step Model의 경우 학습에 A100 GPU 32시간 밖에 걸리지 않으며 LAION-5B-Aesthetics dataset에서 SOTA의 성능을 달성했다.</p></li>
<li><p>LCMs에 대한 새로운 fine-tuning 방식인 Latent Consistency Fine-tuning을 통해 <strong>빠른 추론 속도를 유지하면서도 Custom Dataset에 효율적으로 적용</strong>할 수 있다.</p></li>
</ul>
<p> </p>
<figure class="align-default" id="id2">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_01.png"><img alt="ldm_01" class="bg-primary mb-1" src="../../_images/ldm_01.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 480 </span><span class="caption-text">768x768 Resolution image in 1~4 steps.</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="preliminaries">
<h2>2. Preliminaries<a class="headerlink" href="#preliminaries" title="Permalink to this heading">#</a></h2>
<section id="diffusion-models">
<h3>Diffusion Models<a class="headerlink" href="#diffusion-models" title="Permalink to this heading">#</a></h3>
<p>Diffusion Models 혹은 Score-based Models는 데이터에 점진적으로 Gaussian noise를 주입하고 reverse denoise process로 noise를 제거하여 데이터를 sampling하는 기법이다. 반면 forwad process는 원본 데이터 분포인 <span class="math notranslate nohighlight">\(p_{data}(x)\)</span>를 주변 확률분포인 <span class="math notranslate nohighlight">\(q_{t}(x_{t})\)</span>로 변환한다:</p>
<div class="math notranslate nohighlight">
\[
q_{0t}(x_{t}|x_{0})=\mathcal{N}(x_t|\alpha(t)x_0,\sigma^2(t)I)
\]</div>
<p>여기서 <span class="math notranslate nohighlight">\(\alpha(t)\)</span>와 <span class="math notranslate nohighlight">\(\sigma(t)\)</span>는 noise scheduler를 의미한다. 연속적인 timestep의 관점에서 이를 확률미분방정식(Stochastic Differential Equation, SDE)으로 나타낼 수 있는데, 다음과 같다:</p>
<div class="math notranslate nohighlight">
\[
f(t)=\frac{d\log{\alpha(t)}}{dt}, g^2(t)=\frac{d\sigma^2(t)}{dt}-2\frac{d\log{\alpha(t)}}{dt}\sigma^2(t). \tag{1}
\]</div>
<p>또한 주변 확률분포 <span class="math notranslate nohighlight">\(q_t(x)\)</span>는 **Ptobability Flow ODE(PF-ODE)**라는 상미분방정식(Ordinary Differential Equation, ODE)을 만족하는데  다음과 같다:</p>
<div class="math notranslate nohighlight">
\[
\frac{dx_t}{dt}=f(x)x_t-\frac{1}{2}g^2(t)\nabla_x\log{q_t(x_t)}, \ x_T \sim q_T(x_T). \tag{2}
\]</div>
<p>이때 Diffusion model은 <span class="math notranslate nohighlight">\(-\nabla\log{q_t(x_t)}\)</span>(score function)를 예측하는 noise 예측 모델(<span class="math notranslate nohighlight">\(\epsilon_\theta(x_t,t)\)</span>)을 학습시킨다. 학습된 모델은 score function의 근사치를 예측하고 sampling하는데 이를 empirical PF-ODE라 한다 (경험적 PF-ODE):</p>
<div class="math notranslate nohighlight">
\[
\frac{dx_t}{dt}=f(t)x_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(x_t,t), \ x_T \sim \mathcal{N}(0, \tilde{\sigma}^2I). \tag{3}
\]</div>
<p>Classifier-Free Guidance (CFG)는 sampling의 퀄리티를 높이기 위해 GLIDE, Stable Diffusion, DALL<span class="math notranslate nohighlight">\(\cdot\)</span>E2, Imagen 등 다양한 conditional model에서 사용되었다. CFG의 scale <span class="math notranslate nohighlight">\(\omega\)</span>가 주어졌을 때 원본 noise prediction은 conditional, unconditional noise prediction을 선형적으로 혼합하여 대체된다:</p>
<div class="math notranslate nohighlight">
\[
\tilde{\epsilon}_\theta(z_t,\omega, c,t)=(1+\omega)\epsilon_\theta(z_t, c,t)-\omega\epsilon_\theta(z, \emptyset, t).
\]</div>
<p> </p>
</section>
<section id="consistency-models">
<h3>Consistency Models<a class="headerlink" href="#consistency-models" title="Permalink to this heading">#</a></h3>
<p>Consistenct Model(CM)은 몇 step 혹은 한번의 step 만으로 데이터를 생성할 수 있는 모델이다. CM의 핵심은 <strong>PF-ODE의 궤적에 어떤 point와 PF-ODE의 solution에 대해 mapping되는 function (<span class="math notranslate nohighlight">\(f:(x_t, t) \mapsto x_\epsilon\)</span>)을 추정</strong>하는 것이다.</p>
<figure class="align-default" id="id3">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_02.png"><img alt="ldm_02" class="bg-primary mb-1" src="../../_images/ldm_02.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 481 </span><span class="caption-text">Consistency Models (CM).</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(\epsilon\)</span>은 고정된 매우 작은 양수값을 가지며 CM의 function은 자기 자신에 대한 <strong>self-consistency</strong>를 만족해야한다. 즉 어떠한 time step에 대해서도 <span class="math notranslate nohighlight">\(x_\epsilon\)</span>을 sampling 할 수 있어야 한다.</p>
<div class="math notranslate nohighlight">
\[
f(x_t,t)=f(x_{t'},t'), \forall t,t' \in [\epsilon, T]. \tag{4}
\]</div>
<p><span class="math notranslate nohighlight">\(f_\theta(x, \epsilon)=x\)</span>를 만족하는 모델 <span class="math notranslate nohighlight">\(f_\theta\)</span>는 다음과 같이 정리할 수 있다:</p>
<div class="math notranslate nohighlight">
\[
f_\theta(x,t)=c_{skip}(t)x+c_{out}(t)F_\theta(x,t). \tag{5}
\]</div>
<p><span class="math notranslate nohighlight">\(c_{skip}(t)\)</span>와 <span class="math notranslate nohighlight">\(c_{out}(t)\)</span>는 미분 가능한 함수이며 <span class="math notranslate nohighlight">\(c_{skip}=1, c_{out}=0\)</span>이기 때문에 <span class="math notranslate nohighlight">\(f_\theta(x, \epsilon)=x\)</span>를 만족한다.  <span class="math notranslate nohighlight">\(\theta\)</span>는 학습 가능한 파라미터로 <span class="math notranslate nohighlight">\(F_\theta\)</span>는 심층 신경망을 의미한다.</p>
<p> </p>
<p>CM은 pre-trained 모델에 대한 Distillation 방식과 scratch부터 학습하는 방식이 있는데 주로 <strong>Distillation 방식</strong>을 사용한다. Distillation 방식은 parameter <span class="math notranslate nohighlight">\(\theta^-\)</span>가 <span class="math notranslate nohighlight">\(\theta\)</span>를 통해 학습하며 모델에 대한 self-consistency를 위해 다음과 같이 손실함수를 구성한다:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta,\theta^-;\Phi)=\mathbb{E}_{x,t}\bigg[d\bigg(f_\theta(x_{t_{n+1}, t_{n+1}}), f_{\theta^-}(\hat{x}^\phi_{t_n}, t_n)\bigg)\bigg]. \tag{6}
\]</div>
<p>이 때 <span class="math notranslate nohighlight">\(\theta^-\)</span>는 <span class="math notranslate nohighlight">\(\theta\)</span>에 대한 지수평균이동(Exponential Moving Average, EMA)이며 <span class="math notranslate nohighlight">\(\theta^-  \leftarrow \mu\theta^-+(1-\mu)\theta\)</span>이다. <span class="math notranslate nohighlight">\(d(\cdot, \cdot)\)</span>은 두 sample 사이의 거리를 측정하는 지표이다. <span class="math notranslate nohighlight">\(\hat{x}^{\phi}_{t_n}\)</span>은 <span class="math notranslate nohighlight">\(x_{t_{n+1}}\)</span>에 대한 <span class="math notranslate nohighlight">\(x_{t_n}\)</span>을 추정한 값으로 다음과 같다:</p>
<div class="math notranslate nohighlight">
\[
\hat{x}^\phi_{t_n} \leftarrow x_{t_{n+1}}+(t_{n}-t_{n+1})\Phi(x_{t_{n+1}}, t_{n+1};\phi). \tag{7}
\]</div>
<p><span class="math notranslate nohighlight">\(\Phi\)</span>는 PF-ODE에 사용되는 ODE Solver로 <a class="reference external" href="https://en.wikipedia.org/wiki/Euler_method">Euler</a>나 <a class="reference external" href="https://en.wikipedia.org/wiki/Heun%27s_method">Heun</a> Method등의 수치적인 ODE solver를 사용할 수 있다. 즉 Consistency Distillation은 ODE Solver로 예측한 <span class="math notranslate nohighlight">\(\hat{x}^{\phi}_{t_n}\)</span>과 <span class="math notranslate nohighlight">\(x_{t_{n+1}}\)</span>을 입력으로 <span class="math notranslate nohighlight">\(f_{\theta^-}\)</span>와 <span class="math notranslate nohighlight">\(f_\theta\)</span>로 <strong>예측한 값의 Consistency를 비교하는 방식으로 Distillation을 수행</strong>한다.</p>
</section>
</section>
<section id="id1">
<h2>3. Latent Consistency Models<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>CM의 한계:</p>
<ul class="simple">
<li><p>ImageNet 64x64, LSUN 256x256 영상에 대한 Generation만 수행</p>
<ul>
<li><p><strong>High Resolution</strong>의 잠재성이 아직 탐구되지 않았음.</p></li>
<li><p><strong>Classifier-free Guidance(CFG)</strong> 등을 사용하지 않음.</p></li>
</ul>
</li>
</ul>
<p>Latent Consistency Models(LCMs)는 CM의 잠재력을 충분히 발휘하여 좀더 도전적인 task를 수행한다.</p>
<section id="consistency-distillation-in-the-latent-space">
<h3>3.1 Consistency Distillation in the Latent Space<a class="headerlink" href="#consistency-distillation-in-the-latent-space" title="Permalink to this heading">#</a></h3>
<p>본 논문에서는 pre-trained 된 Stable Diffusion에 Consistency Distillation을 적용한 Latent Consistency Distillation (LCD)을 제안한다. LCMs는 LDM(SD)을 기반으로 설계되었기 때문에 <span class="math notranslate nohighlight">\(z=\varepsilon(x)\)</span>를 통해 <span class="math notranslate nohighlight">\(x\)</span>를 latent vector로 임베딩하고 <span class="math notranslate nohighlight">\(\hat{x}=\mathcal{D}(z)\)</span>를 통해 원본 영상으로 복원한다. latent space 상에서 연산이 이뤄지기 때문에 <strong>Computation Cost를 크게 줄일 수 있어</strong> high-resolution 영상을 laptop GPU에서 생성할 수도 있다.</p>
<p>condition을 추가한 PF-ODE의 reverse process는 다음과 같이 정의된다:</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_t}{dt}=f(t)z_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(z_t,c,t), \ z_T\sim\mathcal{N}(0,\tilde{\sigma}^2I). \tag{8}
\]</div>
<p><span class="math notranslate nohighlight">\(z_t\)</span>는 t step의 image latents, <span class="math notranslate nohighlight">\(\epsilon_\theta(z_t,c,t)\)</span>는 noise 예측 모델, c는 text와 같은 conditional prompt를 의미한다. PF-ODE상에서 모든 t step에 대해 consistency function <span class="math notranslate nohighlight">\(f_\theta :(z_t,c,t) \mapsto z_0\)</span>이기 때문에 이를 수식으로 정리하자면 다음과 같이 나타낼 수 있다 (<span class="math notranslate nohighlight">\(\hat{\epsilon}_\theta\)</span>는 noise prediction model.):</p>
<div class="math notranslate nohighlight">
\[
f_\theta(z,c,t)=c_{skip}(t)z+c_{out}(t)\bigg( \frac{z-\sigma_t\hat{\epsilon}_\theta(z,c,t)}{\alpha_t} \bigg). \ (\epsilon-Prediction) \tag{9}
\]</div>
<p>수식을 살펴보면 ddpm 등의 reparameterization trick인 <span class="math notranslate nohighlight">\(x_t := \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon\)</span>을 변형하여 식에 대입한 것을 알 수 있음. (<span class="math notranslate nohighlight">\(x\)</span>→<span class="math notranslate nohighlight">\(z\)</span>로 치환)</p>
<div class="math notranslate nohighlight">
\[
x_0 = \frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon}{\sqrt{\bar{\alpha}_t}}, \ \hat{z}_0 = \frac{z_t-\sigma(t)\hat{\epsilon}_{\theta}(z,c,t)}{\alpha(t)}.
\]</div>
<p>CM과 마찬가지로 <span class="math notranslate nohighlight">\(c_{skip}(0)=1, c_{out}(0)=0\)</span>이고 <span class="math notranslate nohighlight">\(\hat{\epsilon}_{\theta}(z,c,t)\)</span>는 teacher diffusion model과 유사한 noise 예측 모델 parameter이다. <span class="math notranslate nohighlight">\(f_\theta\)</span>는 <span class="math notranslate nohighlight">\(\epsilon-Prediction\)</span> 외에도 <span class="math notranslate nohighlight">\(x-Prediction\)</span>이나 <span class="math notranslate nohighlight">\(v-Prediction\)</span>을 사용할 수도 있다. (<span class="math notranslate nohighlight">\(x-Prediction\)</span>은 DDPM, <span class="math notranslate nohighlight">\(v-prediction\)</span>은 PD에서 나온 개념)</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L_{CD}}(\theta,\theta^-;\psi)=\mathbb{E}_{z,c,n}\bigg[ d(f_\theta(z_{t_{n+1}},c,t_{n+1}), f_{\theta^-}(\hat{z}^\psi_{t_n},c,t_n)) \bigg]. \tag{10}
\]</div>
<p><span class="math notranslate nohighlight">\(\psi(z_t,t,x,c)\)</span>는 ODE solver이며 특정한 time step <span class="math notranslate nohighlight">\(t \sim s\)</span> 사이에 대한 Eq. 8의 우항을 근사한 값이다. ODE Solver이기 때문에 <strong>DDIM, DPM-Solver, DPM-Solver++ 등을 사용할 수 있다.</strong> 또한 <span class="math notranslate nohighlight">\(\psi\)</span>는 학습 및 Distillation시에만 사용한다. 이때 <span class="math notranslate nohighlight">\(t_n\)</span>은 EDM을 토대로 CM에서 나오는 값이다. 기존 timestep <span class="math notranslate nohighlight">\([t, T]\)</span>에 대한 하위 간격으로 <span class="math notranslate nohighlight">\(t_1=\epsilon&lt;t_2&lt;\cdots&lt;t_N=T\)</span>인 어떠한간격을 의미한다. <span class="math notranslate nohighlight">\(t_i\)</span>는 다음과 같이 나타낼 수 있다:</p>
<div class="math notranslate nohighlight">
\[
t_i=(\epsilon^{1 / \rho} +\frac{i-1}{N-1}(T^{1 / \rho}-\epsilon^{1 / \rho}))^\rho, \rho=7
\]</div>
<p>Eq, 8을 <span class="math notranslate nohighlight">\(t_{n+1} \sim t_n\)</span>까지 t에 대해 적분 했을 때 다음과 같은 수식을 얻을 수 있다:</p>
<div class="math notranslate nohighlight">
\[
\hat{z}_{t_n}^\psi-z_{t_{n+1}}=\int^{t_n}_{t_{n+1}}{\bigg( f(t)z_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(z_t,c,t) \bigg)}dt \approx \psi(z_{t_{n+1}}, t_{n+1},c). \tag{11}
\]</div>
</section>
<section id="one-stage-guided-distillation-by-solving-augmented-pf-ode">
<h3>3.2 One-Stage Guided Distillation by solving augmented PF-ODE<a class="headerlink" href="#one-stage-guided-distillation-by-solving-augmented-pf-ode" title="Permalink to this heading">#</a></h3>
<p>Clasifier-free Guidance(CFG)는 high-quality의 conditional 이미지 생성을 가능하게 했다. 다만 CFG는 2개의 Diffusion Model을 훈련해야하기 때문에 효율적이지 못하며, <strong>LCMs와 같은 few-step sampling method에 사용하기 힘들다.</strong> 따라서 이를 해결하기 위해 본 논문에서는 CFG를 Distillation 과정에서 통합하였다.</p>
<p>Guided-Distill의 경우 two-stage Distillation을 통해  few-step sampling에 CFG를 통합하였으나 학습시간이 길고 <strong>2단계를 거치며</strong> 손실이 누적되기 때문에 최적의 성능을 내기 힘들다.</p>
<figure class="align-default" id="id4">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_03.png"><img alt="ldm_03" class="bg-primary mb-1" src="../../_images/ldm_03.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 482 </span><span class="caption-text">2 Stage Distillation.</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>t이에 반해 LCMs는 augmented PF-ODE를 해결하는 방식으로 one-stage의 guided Distillation을 제안했다. 일단 CFG에 대한 reverse diffusion process는 다음과 같다:</p>
<div class="math notranslate nohighlight">
\[
\tilde{\epsilon}_{\theta}(z_t,\omega,c,t):=(1+\omega)\epsilon_\theta(z_t,c,t)-\omega\epsilon_\theta(z_t,\varnothing,t). \tag{12}
\]</div>
<p>CFG는 conditional noise 예측값과 unconditional noise 예측값을 선형 결합하여 사용한다. 즉 noise 값이 <span class="math notranslate nohighlight">\(\omega\)</span>에 따라 변형되므로 <strong>augmented PF-ODE</strong>라고 한다. augmented PF-ODE는 다음과 같이 나타낼 수 있다:</p>
<div class="math notranslate nohighlight">
\[
\frac{dz_t}{dt}=f(t)z_t+\frac{g^2(t)}{2\sigma_t}\tilde{\epsilon}_\theta(z_t,\omega,c,t), \ z_T\sim\mathcal{N}(0,\tilde{\sigma}^2I). \tag{13}
\]</div>
<p>consistency function도 <span class="math notranslate nohighlight">\(\omega\)</span>를 변수로 받아오기 때문에 <span class="math notranslate nohighlight">\(f_\theta:(z_t,\omega,c,t)\mapsto z_0\)</span>로 다시 정의된다. Consistency Distillation Loss 또한 다음과 같이 나타낼 수 있다:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L_{CD}}(\theta,\theta^-;\psi)=\mathbb{E}_{z,c,\omega,n}\bigg[ d\bigg( f_\theta(z_{t_{n+1}},\omega,c,t_{n+1}), f_{\theta^-}(\hat{z}_{t_n}^{\psi,\omega},\omega,c,t_n) \bigg) \bigg] \tag{14}
\]</div>
<p><span class="math notranslate nohighlight">\(\omega\)</span>와 <span class="math notranslate nohighlight">\(n\)</span>는 각각 <span class="math notranslate nohighlight">\([\omega_{min}, \omega_{max}]\)</span>, <span class="math notranslate nohighlight">\(\{1,…,N-1\}\)</span>에서 sampling된다. <span class="math notranslate nohighlight">\(\hat{z}^{\psi, \omega}_{t_n}\)</span>는 이전과 마찬가지로 CFG가 추가된 ODE-Solver를 사용하여 근사한 값을 의미한다. 이때 사용되는 새로운 noise 예측모델 <span class="math notranslate nohighlight">\(\tilde{\epsilon}_\theta(z_t,\omega,c,t)\)</span>는  Eq. 11처럼  <span class="math notranslate nohighlight">\(t_{n+1} \sim t_n\)</span>까지 t에 대해 적분 했을 때 다음과 같이 나타낼 수 있다:</p>
<div class="math notranslate nohighlight">
\[
\hat{z}^{\psi, \omega}_{t_n}-z_{t_n+1}=\int^{t_n}_{t_{n+1}}\bigg(f(t)z_t+\frac{g^2(t)}{2\sigma_t}\tilde{\epsilon}_\theta(z_t,\omega,c,t)\bigg)dt
\]</div>
<div class="math notranslate nohighlight">
\[
=(1+\omega)\int^{t_n}_{t_{n+1}}\bigg(f(t)z_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(z_t,c,t)\bigg)dt
\]</div>
<div class="math notranslate nohighlight">
\[
-\omega\int^{t_n}_{t_{n+1}}\bigg(f(t)z_t+\frac{g^2(t)}{2\sigma_t}\epsilon_\theta(z_t,\varnothing,t)\bigg)dt
\]</div>
<div class="math notranslate nohighlight">
\[
\approx(1+\omega)\psi(z_{t_{n+1}}, t_{n+1},t_n,c)-\omega\psi(z_{t_{n+1}}, t_{n+1},t_n,\varnothing). \tag{15}
\]</div>
<p>마찬가지로 PF-ODE Solver <span class="math notranslate nohighlight">\(\psi(\cdot,\cdot,\cdot,\cdot)\)</span>에는 DDIM, DPM-Solver, DPM-Solver++ 등을 사용할 수 있다.</p>
</section>
<section id="accelerating-distillation-with-skipping-time-steps">
<h3>3.3 Accelerating Distillation with Skipping Time Steps<a class="headerlink" href="#accelerating-distillation-with-skipping-time-steps" title="Permalink to this heading">#</a></h3>
<p>Stable Diffusion 등 보통의 Diffusion Model들은 매우 큰 step을 전체 time step으로 잡고 학습한다. 그러나 이같이 촘촘한 time step은 각 <span class="math notranslate nohighlight">\(t_n\)</span>과 <span class="math notranslate nohighlight">\(t_{n+1}\)</span>의 변화량을 감소시키기 때문에 자연스럽게 Consistency Distillation Loss도 작아지게 된다. <strong>Loss가 작아지면 학습의 수렴속도도 느려지게 된다.</strong> 따라서 LCMs는 학습 수렴의 속도를 높이기 위해 time step을 수천에서 수십으로 크기 단축시키는 SKIPPING-STEP 방법을 제안하였다.</p>
<p>기존 CMs 모델의 경우 time scheduler로 EDM을 사용하고 ODE-Solver로 Euler 방법이나 Heun 방법을 사용한다.  그러나 LCMs는 Eq. 8을 통해 DDIM, DPM-Solver, DPM-Solver++와 같은 효율적인 solver도 효과적으로 데이터를 생성할 수 있다는 것을 증명했다. 따라서 <strong>SKIPPING-STEP 방법은 <span class="math notranslate nohighlight">\(t_{n+1} → t_n\)</span> 사이의 Consistency를 비교하는것이 아니라 특정 k-step만큼 거리가 있는 time step에 대한 Consistency를 비교한다.</strong> (<span class="math notranslate nohighlight">\(t_{n+k}→t_n\)</span>)</p>
<p>이때 <span class="math notranslate nohighlight">\(k\)</span>값의 크기는 trade-off 관계를 가진다. 너무작으면 (<span class="math notranslate nohighlight">\(k=1\)</span>) 기존과 같이 느린 수렴속도를 갖게되며, 너무 큰 값일 때는 ODE solver 를 통해 근사할 때 오차가 매우 커질수 있다. 논문의 저자는 <span class="math notranslate nohighlight">\(k=20\)</span>을 사용해 <strong>time step을 수천에서 수십으로 대폭 줄여</strong> 학습을 Accelerating 할 수 있었다. Eq. 14에 k값을 추가해 SKIPPING-STEP을 표현할 수 있다.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L_{CD}}(\theta,\theta^-;\psi)=\mathbb{E}_{z,c,\omega,n}\bigg[ d\bigg( f_\theta(z_{t_{n+k}},\omega,c,t_{n+k}), f_{\theta^-}(\hat{z}_{t_n}^{\psi,\omega},\omega,c,t_n) \bigg) \bigg]. \tag{16}
\]</div>
<p><span class="math notranslate nohighlight">\(\hat{z}^{\psi, \omega}_{t_n}\)</span>에 대한 수식도 다음과 같이 변경할 수 있다.</p>
<div class="math notranslate nohighlight">
\[
\hat{z}^{\psi, \omega}_{t_n} \leftarrow z_{t_{n+k}}+(1+\omega)\psi(z_{t_{n+k}}, t_{n+k},t_n,c)-\omega\psi(z_{t_{n+k}}, t_{n+k},t_n,\varnothing). \tag{17}
\]</div>
</section>
</section>
<section id="latent-consistency-fine-tuning-for-customized-dataset">
<h2>3.4 Latent Consistency Fine-tuning for customized dataset<a class="headerlink" href="#latent-consistency-fine-tuning-for-customized-dataset" title="Permalink to this heading">#</a></h2>
<p>Stable Diffusion과 같은 Foundation 생성 모델은 거의 대부분의 text-to-image Generation task에서 잘 되지만 가끔 downstream task를 위해 Cunstom dataset에 대한 fine-tuning이 필요할 때가 있다. Latent Consistency Fine-tuning(LCF)는 Custom Dataset도 teacher model에 대한 종속없이 few-step inference를 성공적으로 할수 있도록 한다. 따라서 LCM은 <strong>기존의 Diffusion model에 대한 추가적인 fine tuning 방법론 없이도 Custom Dataset을 바로바로 학습하여 사용</strong>할수 있다.</p>
<p>따로 추가적인 fine-tuning 방법이 있는것은 아니고 Consisteny Distillation 시 pre-trained 된 LDM을 사용하여 EMA를 통해 Distillation을 하기 때문에 Dataset을 Custom Dataset으로 사용하기만하면 된다. 즉 pre-trained Diffuson model → Custom Dataset fine-tuning → few step inference를 위한 Consistency Distillation을 할 필요 없이 바로학습이 가능하다는 의미이다.</p>
</section>
<section id="experiments">
<h2>4. Experiments<a class="headerlink" href="#experiments" title="Permalink to this heading">#</a></h2>
<section id="text-to-image-generation">
<h3>4.1 Text-To-Image Generation<a class="headerlink" href="#text-to-image-generation" title="Permalink to this heading">#</a></h3>
<p>3가지 데이터셋에 대한 평가를 진행했다. (LAION-5B, LAION-Aesthetics-6+(12M),  LAION-Aesthetics-6.5+(650k)) 앞서말한것처럼 하나의 Resolution이 아닌 512x512, 768x768의 high resolution을 생성했다. 512 size는 <span class="math notranslate nohighlight">\(\epsilon\)</span>-prediction, 768 size는 <span class="math notranslate nohighlight">\(v\)</span>-prediction을 사용했고 ODE-Solver로는 DDIM을 사용했다. 앞서말한것처럼 SKIPPING-STEP은 20의 값을 가진다.</p>
<figure class="align-default" id="id5">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_04.png"><img alt="ldm_04" class="bg-primary mb-1" src="../../_images/ldm_04.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 483 </span><span class="caption-text">Quantitative results at 512 x 512 &amp; 768 x 768 resolution.</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id6">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_05.png"><img alt="ldm_05" class="bg-primary mb-1" src="../../_images/ldm_05.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 484 </span><span class="caption-text">Qualitative results on LAION-Aesthetic-6.5+ Dataset. (2,4 steps)</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>DDIM, DPM-Solver, DPM-Solver++, Guided-Distill 4가지 모델에 대해 LCM과 성능비교를 했는데 이때 Guided-Distill은 오픈소스 코드가 없기 때문에 논문의 내용과 동일하게 Implementation 해서 성능을 비교하였다. LCM은 같은 메모리 Cost 대비 더 빠르게 수렴하고 더 좋은 품질의 영상을 생성하였다. 특히 Guided-Distill은 2 stage Distillation이지만 LCM은 <strong>1 Stage</strong>만 사용해도 이같은 성능을 보여줬다.</p>
</section>
<section id="abulation-study">
<h3>4.2 Abulation Study<a class="headerlink" href="#abulation-study" title="Permalink to this heading">#</a></h3>
<section id="ode-solvers-skipping-step-schedule">
<h4>ODE Solvers &amp; Skipping-Step Schedule<a class="headerlink" href="#ode-solvers-skipping-step-schedule" title="Permalink to this heading">#</a></h4>
<p>augmented PF-ODE를 푸는 solver들(DDIM, DPM, DPM++)을 LCM에 사용할 때 성능 비교와 SKIPPING-STEP schedule의 <span class="math notranslate nohighlight">\(k\)</span>값에 따른 성능 변화를 비교하였다. 모든 모델은 2,000 iteration에서의 4-step inference로 고정해서 비교했다.</p>
<figure class="align-default" id="id7">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_06.png"><img alt="ldm_06" class="bg-primary mb-1" src="../../_images/ldm_06.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 485 </span><span class="caption-text">Different ODE solvers and skipping step k.</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Skipping step의 경우 <span class="math notranslate nohighlight">\(k\)</span> 값을 올렸을 때 훨씬더 빠르게 수렴하며 때때로 더 좋은 FID 값을 보여주었다. 또한 DPM과 DPM++은 <span class="math notranslate nohighlight">\(k\)</span>가 50일 때 DDIM보다 더 좋은 성능을 보였다. 이는 <strong><span class="math notranslate nohighlight">\(k\)</span> 값이 클수록 더 큰 ODE approximation error를 가지는 DDIM에 비해 오차가 적기 때문</strong>이다.</p>
<p><span class="math notranslate nohighlight">\(k=20\)</span>일 때, 3가지 모델 모두 좋은 성능이 보였다.</p>
</section>
<section id="the-effect-of-guidance-scale-omega">
<h4>The Effect of Guidance Scale <span class="math notranslate nohighlight">\(\omega\)</span><a class="headerlink" href="#the-effect-of-guidance-scale-omega" title="Permalink to this heading">#</a></h4>
<p>일반적으로 <span class="math notranslate nohighlight">\(\omega\)</span>값이 클수록 CLIP score 같은 품질의 지표는 좋아지지만 작을수록 다양성이 떨어져 FID Score가 떨어진다. 즉 <span class="math notranslate nohighlight">\(\omega\)</span>의 크기는 <strong>Quality와 Diversity에 대한 trade-off가 있다.</strong></p>
<figure class="align-default" id="id8">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_07.png"><img alt="ldm_07" class="bg-primary mb-1" src="../../_images/ldm_07.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 486 </span><span class="caption-text">Different classifier-free guidance scales <span class="math notranslate nohighlight">\(\omega\)</span>.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>그래프를 보면 2~8 step inference는 성능에 큰 차이를 가지지는 않는것으로 확인된다. 그러나 <strong>1 step inference는 아직 개선의 여지가 있는것</strong>을 확인할 수 있다.</p>
<figure class="align-default" id="id9">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_08.png"><img alt="ldm_08" class="bg-primary mb-1" src="../../_images/ldm_08.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 487 </span><span class="caption-text">Different classifier-free guidance scales <span class="math notranslate nohighlight">\(\omega\)</span>.</span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(\omega\)</span>에 따른 실제 생성 이미지를 비교해 봤을 때 생성 영상의 Quality 차이가 확연하게 들어난다. 즉 Distillation 시에도 CFG를 적용하는 것이 성능을 크게 개선할 수 있다는 것을 증명한다.</p>
</section>
</section>
<section id="downstream-consistency-fine-tuning-results">
<h3>4.3 Downstream Consistency Fine-tuning Results<a class="headerlink" href="#downstream-consistency-fine-tuning-results" title="Permalink to this heading">#</a></h3>
<p>포켓몬 데이터셋과 심슨 데이터셋에 LCF를 적용했을 때를 비교하였다. 90%는 학습 데이터로, 10%는 검증 데이터로 사용했다. 완벽하진 않지만 Custom Dataset의 style을 잘 catch한 모습을 보여준다.</p>
<figure class="align-default" id="id10">
<a class="bg-primary mb-1 reference internal image-reference" href="../../_images/ldm_09.png"><img alt="ldm_09" class="bg-primary mb-1" src="../../_images/ldm_09.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 488 </span><span class="caption-text">Latent Consistency Fine-tuning(LCF) on two customized dataset.. <span class="math notranslate nohighlight">\(\omega\)</span>.</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">#</a></h1>
<p>LCM은 Consistency Distillation을 Latent 상에 적용하여 <strong>고화질의 영상을 매우 적은 time step으로 inference 할 수 있도록 한 모델</strong>이다. 즉 성능 좋고 고해상도의 영상을 few-step으로 가능하게 만들었다. 특히 Custom Dataset에도 Distillation을 적용했을 때 적은 time step으로도 어느정도의 style을 간단하게 학습하는 결과를 보여주었다.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs/review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="consistency_models.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Consistency Models</p>
      </div>
    </a>
    <a class="right-next"
       href="Make_A_Video.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Make A Video</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Latent Consistency Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">2. Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diffusion-models">Diffusion Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-models">Consistency Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">3. Latent Consistency Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#consistency-distillation-in-the-latent-space">3.1 Consistency Distillation in the Latent Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stage-guided-distillation-by-solving-augmented-pf-ode">3.2 One-Stage Guided Distillation by solving augmented PF-ODE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accelerating-distillation-with-skipping-time-steps">3.3 Accelerating Distillation with Skipping Time Steps</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-consistency-fine-tuning-for-customized-dataset">3.4 Latent Consistency Fine-tuning for customized dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">4. Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-image-generation">4.1 Text-To-Image Generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#abulation-study">4.2 Abulation Study</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#ode-solvers-skipping-step-schedule">ODE Solvers &amp; Skipping-Step Schedule</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-effect-of-guidance-scale-omega">The Effect of Guidance Scale <span class="math notranslate nohighlight">\(\omega\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#downstream-consistency-fine-tuning-results">4.3 Downstream Consistency Fine-tuning Results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PseudoLab
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>