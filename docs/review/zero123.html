

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>zero 123 &#8212; Text-to-Image Generation-feat-Diffusion</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/review/zero123';</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Zero123++" href="zero123plus.html" />
    <link rel="prev" title="Dream Booth 3D" href="DreamBooth3D.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/PseudoLab_logo.png" class="logo__image only-light" alt="Text-to-Image Generation-feat-Diffusion - Home"/>
    <script>document.write(`<img src="../../_static/PseudoLab_logo.png" class="logo__image only-dark" alt="Text-to-Image Generation-feat-Diffusion - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to PseudoDiffusers!!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary Works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="vae.html">VAE</a></li>
<li class="toctree-l1"><a class="reference internal" href="gan.html">GAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="DDPM.html">DDPM</a></li>




<li class="toctree-l1"><a class="reference internal" href="DDIM.html">DDIM</a></li>
<li class="toctree-l1"><a class="reference internal" href="A_Study_on_the_Evaluation_of_Generative_Models.html">A Study on the Evaluation of Generative Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cycleGAN.html">CycleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyleGAN.html">StyleGAN</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion_beats_GANs.html">Diffusion Models Beat GANs on Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="dalle.html">DALL-E</a></li>
<li class="toctree-l1"><a class="reference internal" href="DALLE2.html">DALL-E 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="dreambooth.html">DreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="ControlNet.html">ControlNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Latent_Diffusion_Model.html">Introduction</a></li>



<li class="toctree-l1"><a class="reference internal" href="Textual_Inversion.html">Textual Inversion</a></li>








<li class="toctree-l1"><a class="reference internal" href="CustomDiffusion.html">Custom Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="LoRA.html">LoRA</a></li>









<li class="toctree-l1"><a class="reference internal" href="I-DDPM.html">I-DDPM</a></li>
<li class="toctree-l1"><a class="reference internal" href="StyO.html">StyO</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen.html">Imagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="imagen_editor.html">Imagen Editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDEdit.html">SDEdit</a></li>
<li class="toctree-l1"><a class="reference internal" href="SDXL.html">SDXL</a></li>
<li class="toctree-l1"><a class="reference internal" href="t2i_adapter.html">T2I-Adapter</a></li>
<li class="toctree-l1"><a class="reference internal" href="IP_Adapter.html">IP-Adapter</a></li>





<li class="toctree-l1"><a class="reference internal" href="HyperDreamBooth.html">HyperDreamBooth</a></li>
<li class="toctree-l1"><a class="reference internal" href="CM3leon.html">CM3leon</a></li>

<li class="toctree-l1"><a class="reference internal" href="Synthetic_Data_from_Diffusion_Models_Improves_ImageNet_Classification.html">Synthetic Data from Diffusion Models Improves ImageNet Classification</a></li>






<li class="toctree-l1"><a class="reference internal" href="GLIDE.html">GLIDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="BBDM.html">BBDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Your_Diffusion_Model_is_Secretly_a_Zero_Shot_Classifier.html">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="progressive_distillation.html">Progressive Distillation for Fast Sampling of Diffusion Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="ConceptLab.html">ConceptLab</a></li>
<li class="toctree-l1"><a class="reference internal" href="Diffusion_models_already_have_a_Semantic_Latent_Space.html">Diffusion Models already have a Semantic Latent Space</a></li>
<li class="toctree-l1"><a class="reference internal" href="Muse.html">Muse</a></li>


<li class="toctree-l1"><a class="reference internal" href="GIGAGAN.html">Scaling up GANs for Text-to-Image Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="consistency_models.html">Consistency Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="latent_consistency_models.html">Latent Consistency Models</a></li>

<li class="toctree-l1"><a class="reference internal" href="LLM_grounded_Diffusion.html">LLM Grounded Diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="DiT.html">DiT</a></li>






<li class="toctree-l1"><a class="reference internal" href="one-step-image-translation.html">One-Step Image Translation with Text-to-Image Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="LCM-LoRA.html">LCM-LoRA: A Universal Stable-Diffusion Acceleration Module</a></li>


<li class="toctree-l1"><a class="reference internal" href="MimicBrush.html">MimicBrush: Zero-shot Image Editing with Reference Imitation</a></li>
<li class="toctree-l1"><a class="reference internal" href="one_step_diffusion_with_distribution_matching_distillation.html">One-step Diffusion with Distribution Matching Distillation</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Video Generation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Make_A_Video.html">Make A Video</a></li>
<li class="toctree-l1"><a class="reference internal" href="VideoLDM.html">VideoLDM</a></li>
<li class="toctree-l1"><a class="reference internal" href="AnimateDiff.html">AnimateDiff</a></li>
<li class="toctree-l1"><a class="reference internal" href="Animate_Anyone.html">Animate Anyone</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreaMoving.html">DreaMoving</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamPose.html">DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">3D Generation</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="NeRF.html">NeRF : Representing Scenes as Neural Radiance Fields for View Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="3DGS.html">3D Gaussian Splatting for Real-Time Radiance Field Rendering</a></li>
<li class="toctree-l1"><a class="reference internal" href="Point_E.html">Point-E: A System for Generating 3D Point Clouds from Complex Prompts (Arxiv 2022)</a></li>








<li class="toctree-l1"><a class="reference internal" href="Shap-E.html">Shap-E</a></li>









<li class="toctree-l1"><a class="reference internal" href="DreamFusion.html"><strong>DreamFusion</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="magic-3d.html">Magic3D</a></li>
<li class="toctree-l1"><a class="reference internal" href="DreamBooth3D.html">Dream Booth 3D</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">zero 123</a></li>







<li class="toctree-l1"><a class="reference internal" href="zero123plus.html">Zero123++</a></li>
<li class="toctree-l1"><a class="reference internal" href="ProlificDreamer.html">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</a></li>






<li class="toctree-l1"><a class="reference internal" href="DreamGaussian.html">DreamGaussian</a></li>





<li class="toctree-l1"><a class="reference internal" href="Coin3D.html">Coin3D</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Experiments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../experiments/js_exp.html">Synthetic Data with Stable Diffusion for Foliar Disease Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/swjo_exp.html">Training DreamBooth on Naver Webtoon Face Dataset</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pseudo-lab/text-to-image-generation-feat-diffusion/issues/new?title=Issue%20on%20page%20%2Fdocs/review/zero123.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/docs/review/zero123.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>zero 123</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">zero 123</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract-zero-shot-one-image-to-3d-object">Abstract : Zero-shot One Image to 3D Object</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methods">3. Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-to-control-camera-viewpoint">3.1. Learning to Control Camera Viewpoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-conditioned-diffusion">3.2. View-Conditioned Diffusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-reconstruction">3.3 3D Reconstruction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">3.4. Dataset</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">4.Experiments</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks">4.1. Tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines">4.2. Baselines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarks-and-metrics">4.3. Benchmarks and Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#novel-view-synthesis-results">4.4. Novel View Synthesis Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-reconstruction-results">4.5. 3D Reconstruction Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-image-to-3d">4.6. Text to Image to 3D-</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">5. Discussion</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-work">5.1. Future Work</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-objects-to-scenes">From objects to scenes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">6. Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-finetuning-stable-diffusion">C. Finetuning Stable Diffusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-1">C.1 훈련 세부사항</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-2">C.2 추론 세부사항</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-3d-reconstruction">D. 3D Reconstruction</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="zero-123">
<h1>zero 123<a class="headerlink" href="#zero-123" title="Permalink to this heading">#</a></h1>
<div class="admonition-information admonition">
<p class="admonition-title">Information</p>
<ul class="simple">
<li><p><strong>Title:</strong> zero 1-to-3 : Zero-shot One Image to 3D Object</p></li>
<li><p><strong>Reference</strong></p>
<ul>
<li><p>Paper: <a class="reference external" href="https://arxiv.org/abs/2303.11328"><a class="reference external" href="https://arxiv.org/abs/2303.11328">https://arxiv.org/abs/2303.11328</a></a></p></li>
<li><p>Code: <a class="reference external" href="https://github.com/cvlab-columbia/zero123"><a class="github reference external" href="https://github.com/cvlab-columbia/zero123">cvlab-columbia/zero123</a></a></p></li>
</ul>
</li>
<li><p><strong>Author:</strong> Jeongin Lee</p></li>
<li><p><strong>Last updated on Jan. 10, 2025</strong></p></li>
</ul>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="abstract-zero-shot-one-image-to-3d-object">
<h1>Abstract : Zero-shot One Image to 3D Object<a class="headerlink" href="#abstract-zero-shot-one-image-to-3d-object" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<ol class="arabic simple">
<li><p><strong>단일  RGB 이미지</strong>를 입력으로  <strong>Object 의 카메라 viewpoint 를 변경</strong>하여 이미지를 합성하는 프레임워크</p></li>
<li><p><strong>제한된 셋팅</strong> 하에서 다중 view 합성 수행을 위해 <strong>large-scale diffusion model</strong> 의 Geometric priors 활용</p>
<ol class="arabic simple">
<li><p>조건부 diffusion 모델은 특정 카메라 시점 변형 하에 동일 객체의 새로운 이미지들을 생성할 수있는 <strong>상대적인 카메라 시점의 조정(control)</strong> 을 학습하기 위해 <strong>3D 합성 데이터셋</strong> 사용</p></li>
<li><p>합성 데이터셋에서 훈련되었음에도 wild images , OOD(out-of-distribution) 데이터 등에 대해 강력한 zero-shot 일반화 능력 보유</p></li>
</ol>
</li>
<li><p>viewpoint-conditioned diffusion 접근법은 단일 이미지로부터의 3D 재구성을 위해서도 사용 가능</p></li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<ol class="arabic">
<li><p><strong>인간의 3D 인지 능력</strong></p>
<ul class="simple">
<li><p>사람은 단일 시점의 이미지에서도 물체의 3D 형태와 외형을 상상 가능</p></li>
<li><p>대칭과 같은 기하학적 prior에 의존하기도 하지만 물리적 제약을 넘는 일반화가 가능</p></li>
<li><p>현실에 존재하지 않는 객체의 3D 형태까지 예측할 수 있는 능력은 평생동안의 시각적 경험에서 축적된 사전지식(prior)를 기반으로 함.</p></li>
</ul>
</li>
<li><p><strong>기존 3D 재구성 접근 방식의 한계</strong></p>
<ul class="simple">
<li><p>대부분의 3D 이미지 재구성 접근 방식은 <strong>closed-world setting</strong>에 제한</p>
<ul>
<li><p>고비용 3D annotation(CAD model) 과 카테고리별 prior에 의존</p></li>
<li><p>CO3D 등 대규모 데이터셋으로 open-world 재구성 연구가 진전되었으나
여전히 stereo view나 카메라 포즈 정보들과 같은 학습을 위한 기하적 정보가 필요</p></li>
<li><p>large-scale diffusion 모델의 성공을 가능하게 한 인터넷 규모의 text-image 데이터셋들과 비교할 때 아직 3D 데이터의 수집 규모는 미미한 상태</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>diffusion 모델의 인터넷 규모 사전 학습과 한계</strong></p>
<ul class="simple">
<li><p>인터넷 규모의 사전학습은 large-scale diffusion model 에 풍부한 semantic prior 를 부여</p></li>
<li><p>하지만, 여전히 기하학적 정보 캡처 능력은 부족</p></li>
</ul>
</li>
<li><p><strong>Zero 1-to-3 를 제안</strong></p>
<ul>
<li><p><strong>Zero 1-to-3 :</strong></p>
<ul class="simple">
<li><p>Stable Diffusion과 같은 large-scale diffusion 모델이 단일 RGB 이미지를 입력으로,</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>zero-shot novel view synthesis</strong></p></li>
<li><p><strong>3D shape reconstruction</strong>
를 수행하기 위해 <strong>카메라 view point(시점)을 조작을 컨트롤하는 매커니즘 학습</strong></p></li>
</ol>
</li>
<li><p>주어진 단일 RGB 이미지 입력하에 이러한 TASK를 수행 하는 것은 매우 많은 제약이 존재
→ Diffusion 모델을 기반으로 다양한 시점에서 방대한 객체 이미지를 생성하여 이용</p>
<ul class="simple">
<li><p>Diffusion 모델은 카메라 포즈 없이 2D 이미지에 대한 학습만 수행되었으므로,
<strong>파인튜닝을 통해 상대적인 카메라 회전 및 이동을 컨트롤</strong>하여 새로운 시점의 이미지를 생성하는 접근법 제시</p></li>
<li><p>파인튜닝을 통해 선택한 다른 카메라 시점에서의 임의의 이미지를 생성 가능</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="related-work">
<h1>2. Related Work<a class="headerlink" href="#related-work" title="Permalink to this heading">#</a></h1>
<ol class="arabic">
<li><p><strong>3D generative models</strong></p>
<ul class="simple">
<li><p>생성 모델의 동향</p>
<ul>
<li><p>대규모 이미지-텍스트 데이터셋과 결합된 생성 모델의 발전으로 고품질의 다양한 장면과 객체를 합성 가능해짐.</p></li>
<li><p>특히 diffusion 모델의 경우 denoising objective를 통해 확장 가능한 이미지 생성 학습에 효과적</p></li>
<li><p>이를 3D 도메인으로 확장하려면 많은 3D annotation 데이터를 필요로 하므로, 이에 대한 대안으로 <strong>대규모 2D diffusion 모델을 3D task 에 전이하는 방식</strong>이 연구되고 있음.</p></li>
</ul>
</li>
<li><p><strong>NeRF과 DreamFields의 역할</strong></p>
<ul>
<li><p><strong>NeRF</strong>는 고품질 장면 인코딩에 탁월하여 단일 장면 재구성에 주로 사용되며, 관찰되지 않은 각도에서 새로운 view 를 예측</p></li>
<li><p><strong>DreamFields</strong>는 NeRF를 3D 생성 시스템의 주요 요소로 활용하여, 텍스트 입력에서 고품질 3D 객체와 장면을 생성할 수 있게 함.</p></li>
</ul>
</li>
<li><p><strong>Zero-1-to-3 의 접근법</strong></p>
<ul>
<li><p>새로운 시점 합성을 <strong>뷰포인트로 조건화된 image-to-image 변환</strong> 작업으로 보고 <strong>diffusion</strong> 모델을 활용</p></li>
<li><p>학습된 모델은 <strong>3D distillation</strong> 과 결합해 단일 이미지로부터 3D Shape reconstruction 수행</p></li>
<li><p>합성 데이터셋으로 시점 조작을 학습하여 wild 이미지에 대해 zero-shot 일반화를  입증</p></li>
</ul>
</li>
<li><p><strong>선행 연구와의 비교</strong></p>
<ul>
<li><p>선행연구는 <strong>Zero-1-to-3</strong> 와 유사한 파이프라인을 채택하였으나, 제로샷 일반화 능력을 입증하지 못함</p></li>
<li><p>이 외에도 다양한 접근방식이 language-guided 와 text-inversion 을 활용하여 image-to-3D 생성 태스크를 위한 유사한 방법론들을 제안</p></li>
<li><p>하지만 본 방법론은 합성 데이터셋을 통해 view point 컨트롤을 학습하고, 제로샷 일반화 능력을 보여줬다는 데에서 차이가 있음.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Single-view object reconstruction</strong></p>
<ul>
<li><p><strong>단일 뷰에서 3D 객체 재구성</strong>:</p>
<ul class="simple">
<li><p>단일 뷰에서 3D 객체를 재구성하는 것은 매우 어려운 문제</p></li>
<li><p>이를 해결하기 위한 강력한 사전 지식(<strong>prior</strong>) 필요</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://velog.io/&#64;dldydldy75/3D-Understanding"><strong>기존 접근 방식</strong>:</a></p>
<ol class="arabic simple">
<li><p><strong>데이터 수집 형태에 따른 전역적인(global) 특성 기반의 조건화 모델</strong></p>
<ul class="simple">
<li><p>기존 연구 중 일부는 3D 데이터를 <strong>메쉬, 복셀, 또는 포인트 클라우드</strong> 형식으로 수집해 이를 기반으로 조건에 대한 이미지 인코사전 지식을 형성</p></li>
<li><p>사용된 3D 데이터의 종류에 큰 제약 존재</p></li>
<li><p>데이터에 따른 조건의 타입에 대한 <strong>global nature</strong> 로 인해 일반화 능력이 부족</p></li>
<li><p>새로운 관점에서 재구성하기 위해 추가적인 포즈 추정 단계가 필요</p></li>
</ul>
</li>
<li><p><strong>국소적으로(locally) 조건화된 모델</strong></p>
<ul class="simple">
<li><p>이미지의 국소적 특성을 직접 사용하여 장면 재구성을 시도</p></li>
<li><p>교차 도메인 일반화 능력이 더 좋지만, 일반적으로 가까운 뷰 재구성에 제한됨.</p></li>
</ul>
</li>
</ol>
  <aside>
<ul class="simple">
<li><p>global nature : 객체의 형태, 기하학적 구조, 데이터 종류, 색상 분포 등</p></li>
<li><p>local nature : 저수준 정보로 색상 픽셀값, 특정 패턴, 텍스쳐 등을 의미</p></li>
</ul>
  </aside>
<ul class="simple">
<li><p><strong>MCC (Multiview Compressive Coding for 3D Reconstruction)</strong></p>
<ul>
<li><p><strong>RGB-D</strong> 뷰를 사용하여 3D 재구성을 위한 general-purpose representation 을 학습하며, 대규모 객체 중심 비디오 데이터셋으로 훈련됨</p></li>
</ul>
</li>
<li><p><strong>Zero-1-to-3</strong></p>
<ul>
<li><p>사전 훈련된 Stable Diffusion 모델에서 직접 풍부한 기하학적 정보를 추출할 수 있음을 보여주며, 이는 <strong>추가적인 깊이 정보 없이</strong>도 가능함을 보임</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="methods">
<h1>3. Methods<a class="headerlink" href="#methods" title="Permalink to this heading">#</a></h1>
<ul>
<li><p><strong>목표 :</strong> 주어진 단일 RGB 이미지 <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{H \times W \times 3}\)</span> 를  입력으로, 다른 카메라 시점에서의 객체 이미지를 합성</p></li>
<li><p><strong>카메라 변환</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R \in \mathbb{R}^{3 \times 3}\)</span> : relative camera rotation</p></li>
<li><p><span class="math notranslate nohighlight">\(T \in \mathbb{R}^{3}\)</span> : relative camera translation</p></li>
</ul>
</li>
<li><p><strong>모델 학습</strong></p>
<ul>
<li><p>함수  <span class="math notranslate nohighlight">\(f\)</span> 를 학습하여 새로운 이미지를 합성</p>
<div class="math notranslate nohighlight">
\[
        \hat{x}_{R,T} = f(x, R, T)
        \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(*x\)</span> : 주어진 단일 RGB 이미지 <span class="math notranslate nohighlight">\((x \in \mathbb{R}^{H \times W \times 3})\)</span>*</p></li>
<li><p><span class="math notranslate nohighlight">\(*\hat{x}_{R,T}\)</span> :*  합성 이미지</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{x}_{R,T}\)</span> 가 실제지만 관측되지 않은 새로운 view <span class="math notranslate nohighlight">\(x_{R,T}\)</span> 와 지각적으로 유사하도록 추정</p></li>
</ul>
</li>
</ul>
</li>
<li><p>단일 RGB 이미지에서 새로운 시점으로의 합성은 많은 제약사항이 존재 (단일 이미지로 많은 정보를 예측해야 하므로)</p></li>
<li><p><strong>large-scale Diffusion(Stable Diffusion)을 활용하여 태스크 수행</strong></p>
<ul class="simple">
<li><p>프롬프트로부터 다양한 이미지를 생성할 때 갖는 뛰어난 zero-shot 능력을 활용</p></li>
</ul>
</li>
<li><p><strong>large-scale Diffusion에서 3D 정보를 추출하는 능력을 저해하는 요인 두가지</strong></p>
<ol class="arabic simple">
<li><p><strong>뷰포인트 간의 명확한 대응 관계의 부족</strong> : 대규모 생성 모델들은 다양한 객체와 시점에서 훈련되었지만, 서로 다른 시점들 간의 연관성을 명시 으로 인코딩하지 않음.</p></li>
<li><p><strong>인터넷 규모 데이터셋에 반영된 뷰포인트 편향</strong> : 생성 모델들은 인터넷에서 반영된 시점 편향을 물려받아, 특정한 자세 및 시점에서의 이미지를 생성하는 경향 존재</p></li>
</ol>
</li>
</ul>
<section id="learning-to-control-camera-viewpoint">
<h2>3.1. Learning to Control Camera Viewpoint<a class="headerlink" href="#learning-to-control-camera-viewpoint" title="Permalink to this heading">#</a></h2>
<ul>
<li><p><strong>목표</strong>: Diffusion 모델을 통해, 촬영된 이미지에서 카메라의 외부 매개변수를 제어할 수 있는 메커니즘을 학습하여 새로운 시점의 이미지를 합성하는 것.</p></li>
<li><p><strong>데이터셋</strong>: 이미지 쌍과 상대적 카메라 외부 매개변수로 구성된 데이터셋 <span class="math notranslate nohighlight">\({ (x, x_{(R,T)}, R, T) }\)</span></p></li>
<li><p><strong>접근법 [Figure3]</strong></p></li>
<li><p>사전 훈련된 diffusion 모델을 미세조정하여 나머지 표현을 손상시키지 않고 카메라 파라미터를 제어하도록 학습.</p></li>
<li><p><strong>Latent Diffusion Architecture 를 이용</strong></p>
<ul class="simple">
<li><p><strong>[참고] LDM</strong></p></li>
</ul>
<ul>
<li><p>Encoder(<span class="math notranslate nohighlight">\(\mathcal{E}\)</span>), Denoiser(U-Net, <span class="math notranslate nohighlight">\(\epsilon_\theta\)</span>), Decoder(<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>)로 구성</p></li>
<li><p><strong>The Objective</strong></p>
<div class="math notranslate nohighlight">
\[
        \text{min}_{\theta} \mathbb{E}_{z \sim \mathcal{E}(x), t, \epsilon \sim \mathcal{N}(0, 1)} \left\| \epsilon - \epsilon_{\theta}(z_t, t, c(x, R, T)) \right\|^2_2
        \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(z \sim \mathcal{E}(x)\)</span>:  <span class="math notranslate nohighlight">\(x\)</span> 에 대한 Encoder의 출력</p></li>
<li><p><span class="math notranslate nohighlight">\(t ∼ [1, 1000]\)</span> : Diffusion time step</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>: 가우시안 노이즈 샘플.</p></li>
<li><p><span class="math notranslate nohighlight">\(c(x, R, T)\)</span> : input view 와 상대적 camera extrinsic 임베딩</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\epsilon_\theta\)</span> <strong>가 훈련된 후, 추론 모델 <span class="math notranslate nohighlight">\(f\)</span>는 <span class="math notranslate nohighlight">\(c(x, R, T)\)</span> 를 조건으로 가우시안 noise image로부터 반복적인 denoising 을 통해 이미지를 생성</strong></p></li>
</ul>
</li>
<li><p><strong>본 방법론의 효과</strong></p>
<ul>
<li><p>사전 훈련모델에 대한 이러한 방식의 파인튜닝을 통해 모델은 카메라 viewpoints 통제를 위한 일반적인 메커니즘을 학습 가능</p>
<p>→ fine tuning 데이터셋에서 보이는 객체 외부의 정보를 외삽</p>
</li>
<li><p>fine tuning 을 통해 제어 기능이 “<strong>부착</strong>”될 수 있고 diffusion의 포토리얼리스틱 이미지 생성 능력을 유지</p></li>
<li><p>이러한 <strong>compositionality</strong>(구성 가능성)는 모델에서 제로샷 기능을 확립하며,
최종모델은 3D assets 이 부족하고 fine-tuning 집합에 존재하지 않는 객체 클래스에 대한 새로운 뷰를 합성 가능</p></li>
</ul>
</li>
</ul>
</section>
<section id="view-conditioned-diffusion">
<h2>3.2. View-Conditioned Diffusion<a class="headerlink" href="#view-conditioned-diffusion" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>단일이미지로부터 3D 객체를 재구성하기 위해 저수준 인식(깊이, 음영, 텍스처 등)과 고수준 이해(형태, 기능, 구조 등)  모두를 필요로 함.</p></li>
</ul>
<p><strong>→ 하이브리드 조건부 메커니즘 채택</strong>:  두 가지 스트림을 결합</p>
<ol class="arabic simple">
<li><p><strong>1st Stream</strong></p>
<ul class="simple">
<li><p>입력 이미지를 CLIP(Contrastive Language–Image Pretraining) 임베딩으로 변환</p></li>
<li><p>이 CLIP 임베딩에 카메라의 상대적인 회전(<span class="math notranslate nohighlight">\(R\)</span>)과 변환(<span class="math notranslate nohighlight">\(T\)</span>)를 연결하여 “posed CLIP” 임베딩인  <span class="math notranslate nohighlight">\(c(x, R, T)\)</span>을 생성</p></li>
<li><p>이 임베딩을 통해 고수준의 의미 정보를 제공하는 <strong>cross-attention</strong>을 U-Net의 디노이징 과정에 적용</p></li>
</ul>
</li>
<li><p><strong>2nd Stream</strong></p>
<ul class="simple">
<li><p>input image 와 denoising 중의 이미지를 channel-concatenated</p></li>
<li><p>모델이 합성되는 중인 객체의 identity 와 details 을 유지하도록 함.</p></li>
</ul>
</li>
</ol>
<ul class="simple">
<li><p><strong>classifier-free guidance</strong> 적용을 위해 이미지와 posed CLIP 임베딩을 무작위로 영(0) 벡터로 설정하고, 추론 중 조건부 정보를 스케일링하는 메커니즘</p></li>
</ul>
</section>
<section id="d-reconstruction">
<h2>3.3 3D Reconstruction<a class="headerlink" href="#d-reconstruction" title="Permalink to this heading">#</a></h2>
<ul>
<li><p>많은 응용에서, 객체의 새로운 뷰를 합성하는 것만으로는 충분하지 않으며, 객체의 외관과 기하 구조를 모두 캡처하는 전체 3D 재구성이 필요</p></li>
<li><p>**Score Jacobian Chaining(SJC)**을 채택하여, 텍스트-이미지 확산 모델의 사전 정보를 사용하여 3D 표현을 최적화</p>
<ul class="simple">
<li><p>diffusion모델의 확률적 특성으로 인해, 그래디언트 업데이트가 매우 불확실하게 변함.</p></li>
<li><p>DreamFusion을 기반으로 한 <strong>SJC</strong>에서 사용된 기술로, <strong>classifier-free guidance 값을 기존보다  상당히 증가 → 각 샘플의 다양성 감소 &amp; reconstruction 의 fiderlity 향상</strong></p></li>
<li><p><strong>SJC</strong> : Dream Fusion 에서 NeRF로 만들어지는 이미지는 pretrained된 Diffusion Model을 학습시킬 때, 없었던 확률 분포이기 때문에 OOD (Out of Distribution) 문제가 발생 <a class="reference external" href="https://arxiv.org/abs/2208.01618">참고</a></p></li>
</ul>
</li>
<li><p><strong>3D reconstruction with Zero-1-to-3 [Figure4]</strong></p>
<ol class="arabic simple">
<li><p><strong>SJC 와 유사하게 임의의 뷰포인트를 샘플링</strong>하고, 볼륨 렌더링을 수행</p></li>
<li><p>볼륨 렌더링으로 생성된 이미지에 가우시안 노이즈를 주입</p></li>
<li><p>input 이미지  <span class="math notranslate nohighlight">\(x\)</span>, CLIP 임베딩  <span class="math notranslate nohighlight">\(c(x, R, T)\)</span> 및 타임스텝 <span class="math notranslate nohighlight">\(t\)</span> 를 조건부로  Denoising U-Net, <span class="math notranslate nohighlight">\(\epsilon_\theta\)</span>을 사용하여  non-noisy input <span class="math notranslate nohighlight">\(x_π\)</span>에 대한 스코어를 근사</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
    \nabla \mathcal{L}_{SJC} = \nabla _{I_{\pi}} \log p_{\sqrt{2}\epsilon}(x_{\pi})
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla \mathcal{L}_{SJC}\)</span> : PAAS 스코어</p></li>
</ul>
</li>
<li><p>입력 뷰와의 <strong>MSE</strong> 손실로 최적화</p></li>
<li><p>NeRF representation 규제를 위한 추가 loss term</p>
<ul class="simple">
<li><p><strong>Depth smoothness loss</strong> to every sampled viewpoint</p></li>
<li><p><strong>near-view consistency loss</strong> : 근접 뷰 간의 외관 변화(appearance change)를 규제, 가까운 뷰 간의 일관성을 유지</p></li>
</ul>
</li>
</ul>
</section>
<section id="dataset">
<h2>3.4. Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Dataset</strong> :  Objaverse, 800K+ 3D models  created by 100K+ artists</p></li>
<li><p><strong>데이터 구성</strong></p>
<ul>
<li><p>ShapeNet과 같이 명시적인 클래스 레이블이 없지만, 다양한 고품질 3D 모델을 포함</p></li>
<li><p>모델은 정교한 기하학적 세부사항과 재질 속성을 갖춘 많은 모델을 포함</p></li>
</ul>
</li>
<li><p><strong>Camera Extrinsics matrices 샘플링</strong></p>
<ul>
<li><p>각 객체에 대해, 객체 중심을 가리키는 <strong>12</strong>개 카메라 외부 매트릭스 <span class="math notranslate nohighlight">\(\mathcal{M}_{\rceil}\)</span>를 무작위로 샘플링</p></li>
<li><p>각 객체에서 12개 뷰를 ray 트레이싱 엔진을 사용해 렌더링합니다.</p></li>
</ul>
</li>
<li><p><strong>Training</strong></p>
<ul>
<li><p>각 객체에 대해 이미지 쌍<span class="math notranslate nohighlight">\((x, x_{R,T})\)</span>을 형성하기 위해 2개 뷰를 샘플링</p></li>
<li><p>이 때 두 시점간의 맵핑을 정의하는 relative viewpoint transformation<span class="math notranslate nohighlight">\((R, T)\)</span> 는 두 시점 각각의 extrinsic matrices 를 통해 쉽게 유도 가능</p></li>
</ul>
</li>
<li><p><strong>카메라 외부 파라미터 (Camera Extrinsics)</strong></p>
<ul>
<li><p><strong>3D 장면 내에서 카메라의 위치와 방향</strong>을 정의하는 요소</p></li>
<li><p>카메라의 시점과 실제 세계 좌표계(<strong>World Coordinate system</strong>)를 연결하여, 특정 객체가 3D 공간에서 정확히 어디에 위치하는지를 파악</p></li>
<li><p><strong>Camera Extrinsics 구성 요소</strong></p>
<ul>
<li><p><strong>회전(Rotation)</strong>: 카메라가 세계 좌표계에 대해 어떤 방향으로 회전되어 있는지를 나타냄</p></li>
<li><p><strong>변환(Translation)</strong>: 카메라가 세계 좌표계의 특정 위치에 어느 좌표로 이동해 있는지를 나타냄</p></li>
<li><p><a class="reference external" href="https://jhtobigs.oopy.io/3dcoordinate">참조 링크</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="experiments">
<h1>4.Experiments<a class="headerlink" href="#experiments" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p>평가 대상 : <strong>model’s performance  zero-shot novel view synthesis &amp; 3D reconstruction</strong></p></li>
<li><p>Objaverse 데이터셋 외의 데이터와 이미지를 사용하였으므로, 제로샷 결과로 간주</p></li>
<li><p>모델 성능을 합성 객체와 장면들을 다양한 복잡함 수준에서 정량적으로 결과 비교</p></li>
<li><p>다양한 자연 이미지(일상적인 객체 사진부터 그림까지)를 사용하여 질적 결과를 보고</p></li>
</ul>
<section id="tasks">
<h2>4.1. Tasks<a class="headerlink" href="#tasks" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>단일 RGB 이미지를 입력으로, 밀접하게 연관된 두가지 태스크를 수행 &amp; zero-shot 적용</p></li>
</ul>
<ol class="arabic simple">
<li><p><strong>Novel View Synthesis</strong></p>
<ul class="simple">
<li><p>단일 뷰에서 객체의 깊이, 텍스처 및 형태를 학습하도록 요구되는 오랜 3D problem</p></li>
<li><p>입력 정보가 극히 제한적일 때, 모델은 prior 를 활용한 새로운 synthesis method 가 필요</p></li>
<li><p>최근 방법들 : CLIP consistency 목적함수를 사용, implicit neural fields 최적화에 의존</p></li>
<li><p><strong>본 연구의 방법론</strong></p>
<ul>
<li><p><strong>3D reconstruction</strong> 과 <strong>Novel View Synthesis</strong> 간에 <strong>orthogonal</strong> (독립적)</p></li>
<li><p><strong>3D reconstruction</strong> 과 <strong>Novel View Synthesis</strong>의 순서를 반대로 전환하여 여전히 입력 이미지에 묘사된 객체의 정체성을 유지</p></li>
<li><p>Self-occlusion로 인한 <strong>aleatoric uncertainty</strong>을 확률적 생성 모델을 사용해 모델링</p></li>
<li><p>대규모 Diffusion 모델로 학습된 semantic, geometric priors  들을 효율적으로 활용</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>3D reconstruction</strong></p>
<ul class="simple">
<li><p><strong>SJC(Symmetric Jacobian Chaining) 및 DreamFusion과 같은 확률적 3D 재구성 프레임워크를 채택하여 최적의 3D 표현을 생성</strong></p></li>
<li><p><strong>parameterize</strong></p>
<ul>
<li><p>3D 표현을 <strong>voxel radiance field</strong>으로 파라미터화</p></li>
<li><p>density 필드에서 <strong>Marching Cubes</strong> 알고리즘을 사용하여 3D 메쉬를 추출합</p>
<ul>
<li><p><strong>Marchinng Cube</strong> 알고리즘 : 3D 스칼라 필드에서 등치선(iso-surface)을 추출하기 위해 사용되는 알고리즘</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>3D reconstruction 에 대한 view-conditioned diffusion model 활용</strong></p>
<ul>
<li><p>diffusion 모델이 학습한 풍부한 2D 외양 prior를 3D 기하학으로 전환 가능한 경로 제공</p></li>
</ul>
</li>
</ul>
</li>
</ol>
</section>
<section id="baselines">
<h2>4.2. Baselines<a class="headerlink" href="#baselines" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>본 방법론이 다루는 범위와 일관되도록 아래 두가지 모두에 해당되는 방법론들만 비교</p>
<ul>
<li><p><strong>zero-shot setting</strong></p></li>
<li><p><strong>input : single-view RGB image</strong></p></li>
</ul>
</li>
</ul>
<ol class="arabic">
<li><p><strong>Novel View Synthesis</strong></p>
<ol class="arabic simple">
<li><p><strong>DietNeRF</strong>: viewpoints 전반에 걸쳐 CLIP mage-to-image consistency loss 로 NeRF 규제</p></li>
<li><p><strong>Image Variations (IV)</strong></p>
<ul class="simple">
<li><p>텍스트 프롬프트가 아닌 이미지 조건을 받기 위해 Stable Diffusion 을 파인튜닝한 모델로, Stable Diffusion 을 활용한 semantic 최근접 이웃 탐색 엔진으로 간주될 수 있음.</p></li>
</ul>
</li>
<li><p><strong>SJC (SJC-I) :</strong></p>
<ul class="simple">
<li><p>diffusion-based text-to-3D 모델인 SJC 를 선택, 이때 텍스트 프롬프트 조건을 이미지 조건으로 대체한 모델을 사용하며 이를 SJC-I 로 명명</p></li>
</ul>
</li>
</ol>
</li>
<li><p><strong>3D Reconstruction</strong></p>
<ul class="simple">
<li><p><strong>Multiview Compressive Coding (MCC)</strong></p>
<ul>
<li><p>신경 필드 기반 접근 방식으로, RGB-D 관측치를 바탕으로 3D 표현을 완성</p></li>
<li><p>CO3Dv2 데이터셋에서 훈련</p></li>
</ul>
</li>
<li><p><strong>Point-E</strong></p>
<ul>
<li><p>색칠된 포인트 클라우드 위에 구축된 Diffusion모델</p></li>
<li><p>OpenAI의 내부 3D 데이터셋에서 훈련되어 더 큰 데이터셋을 사용</p></li>
</ul>
</li>
<li><p><strong>MCC</strong>와 <strong>Point</strong>-<strong>E</strong> 외에도, <strong>SJC</strong>-<strong>I</strong>와 같은 다른 기법들과도 비교</p></li>
<li><p>특정 데이터셋에서 낮은 수준의 정보와 높은 수준의 이해를 요구하는 3D reconstruction 수행</p></li>
</ul>
<hr class="docutils" />
 <aside>
 ☑️ **MCC(다중 뷰 압축 코딩)의 깊이 추정**
 - **MCC**는 입력으로 깊이 정보가 필요, **MiDaS**라는 모델을 사용하여 깊이를 추정
 - **MiDaS** : 상대적인 불일치(disparity) 맵을 생성하여, 이를 절대적인 pseudo-metric 깊이로 변환 (전체 테스트셋에서 합리적으로 보이는 standard scale 과 shift 값을 가정하고 변환)
 </aside>
</li>
</ol>
</section>
<section id="benchmarks-and-metrics">
<h2>4.3. Benchmarks and Metrics<a class="headerlink" href="#benchmarks-and-metrics" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>데이터셋 평가</strong></p>
<ul>
<li><p>Google Scanned Objects (GSO): 고품질 스캔된 가정용 아이템 데이터셋</p></li>
<li><p>RTMV: 20개의 랜덤 객체로 구성된 복잡한 장면 데이터셋</p></li>
<li><p>모든 실험에서 각 데이터셋의 ground truth 3D data 를 사용하여 3D reconstruction을 평가</p></li>
</ul>
</li>
<li><p><strong>Novel view synthesis evaluation metrics</strong></p>
<ul>
<li><p><strong>이미지의 유사성을 평가 → PSNR, SSIM, LPIPS, FID</strong></p></li>
</ul>
</li>
<li><p><strong>3D reconstruction evaluation metrics</strong></p>
<ul>
<li><p><strong>Chamfer Distance, Volumetric IoU (Intersection over Union)</strong></p></li>
</ul>
</li>
</ul>
</section>
<section id="novel-view-synthesis-results">
<h2>4.4. Novel View Synthesis Results<a class="headerlink" href="#novel-view-synthesis-results" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Table 2</strong> : RTMV에서의 새로운 뷰 합성 결과.  RTMV의 장면은 Objaverse 훈련 데이터와 분포가 다르지만, 우리의 모델은 여전히 기준선을 상당한 차이로 능가.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Figure 5 : Novel view synthesis on Google Scanned Object</strong>
왼쪽에 표시된 입력 뷰는 두 개의 무작위 샘플링된 새로운 뷰를 합성하는 데 사용됨. 해당하는 실제 뷰는 오른쪽에 표시되어 있음. 기준 방법들과 비교할 때, 우리가 합성한 새로운 뷰는 실제와 매우 일치하는 풍부한 텍스트 및 기하학적 세부 사항을 포함하고 있으며, 반면 기준 방법들은 고주파 세부 사항의 유의미한 손실을 보임.</p></li>
</ul>
<ul class="simple">
<li><p><strong>Figure 6 Novel view synthesis on RTMV</strong>
왼쪽에 표시된 입력 뷰는 두 개의 무작위로 샘플링된 새로운 뷰를 합성하는 데 사용됨. 오른쪽에는 해당하는 실제 뷰가 표시됨. 우리가 합성한 뷰는 큰 카메라 시점 변화가 있을 때조차도 높은 충실도를 유지하며, 대부분의 다른 방법들은 품질이 급격히 저하됨.</p></li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p><strong>Point-E</strong></p>
<ul>
<li><p>Point-E 모델은 다른 기준선(baselines)들보다 더 뛰어난 성능을 발휘하며, 우수한 제로샷(Zero-shot) 일반화 능력을 보임</p></li>
<li><p>그러나 생성된 포인트 클라우드의 크기가 작아 Point-E가 새로운 뷰 합성(novel view synthesis)에서의 적용 가능성을 제한함.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p><strong>Figure 7 Novel view synthesis on in-the-wild images.</strong>
1, 3, 4 행은 iPhone으로 촬영한 이미지에 대한 결과를 보여주며, 2nd 행은 인터넷에서 다운로드한 이미지에 대한 결과를 제시. 본 방법은 서로 다른 표면 재료와 기하학을 가진 객체에 대해 로버스트함.</p></li>
<li><p><strong>샘플의 다양성</strong></p>
<ul>
<li><p>diffusion 모델이 NeRF보다 이러한 기본적인 불확실성을 포착하는 데 더 적합한 아키텍처</p></li>
<li><p>입력 이미지가 2D이기 때문에 항상 객체의 부분적인 뷰만을 나타내고 많은 부분이 관찰되지 않으므로, diffusion 을 통해 다양한 시점에서 샘플들을 랜덤으로 생성</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Figure 8: 입력 뷰를 고정하고, 다양한 시점에서 새로운 샘플을 랜덤으로 생성하여 시각화
이러한 다양한 결과는 입력 뷰에서 놓친 기하학적 및 외관 정보를 반영</p></li>
</ul>
</section>
<section id="d-reconstruction-results">
<h2>4.5. 3D Reconstruction Results<a class="headerlink" href="#d-reconstruction-results" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>실제 ground truth 와 유사한 고충실도의 3D 메쉬를 reconstruct</p></li>
</ul>
<ul class="simple">
<li><p><strong>MCC (Multiview Compressive Coding)</strong>:</p>
<ul>
<li><p>입력 뷰에서 보이는 표면에 대한 좋은 추정을 제공하지만, 종종 물체 뒷면의 기하학을 올바르게 추론하지 못함.</p></li>
</ul>
</li>
<li><p><strong>SJC-I</strong> : 의미 있는 기하학을 재구성하는 데 실패</p></li>
<li><p><strong>Point-E</strong>: 인상적인 제로샷 일반화 능력을 보여주며, 물체 기하학에 대한 합리적인 추정을 예측</p>
<ul>
<li><p>그러나, 4,096 포인트로 구성된 비균일한 희소 포인트 클라우드만 생성 가능하고, 이로 인해 재구성된 표면에 구멍이 생김.</p></li>
<li><p>좋은 CD(Chamfer Distance) 점수를 얻지만, 부피 IoU(Intersection over Union)에서는 부족한 평가척도 결과를 보임.</p></li>
</ul>
</li>
<li><p><strong>제안된 방법</strong>:</p>
<ul>
<li><p>학습된 다중 보기 우선 순위를 활용하고 NeRF 스타일 표현의 장점을 결합하여 CD와 부피 IoU 모두에서 이전 작업들보다 개선됨(표 3 및 4에서 확인).</p></li>
</ul>
</li>
</ul>
</section>
<section id="text-to-image-to-3d">
<h2>4.6. Text to Image to 3D-<a class="headerlink" href="#text-to-image-to-3d" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>실제 환경에서 촬영된 이미지 외에도, Dall-E-2와 같은 txt2img 모델이 생성한 이미지에 대해서도 테스트</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="discussion">
<h1>5. Discussion<a class="headerlink" href="#discussion" title="Permalink to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Zero-1-to-3</strong> 방식은 단일 이미지에서 새로운 시점을 생성하고 3D 재구성을 수행하는 제로샷 방식</p></li>
<li><p>사전 학습된 Stable Diffusion 모델의 <strong>풍부한 의미적, 기하적 prior 를</strong> 활용하여 우수한 성능을 달성</p>
<ul>
<li><p>이러한 정보 추출을 위해 <strong>Stable Diffusion 모델이</strong> <strong>카메라 시점 제어를 학습</strong>하도록 미세 조정하여, 벤치마크에서 최첨단 성능을 입증</p></li>
</ul>
</li>
</ul>
<section id="future-work">
<h2>5.1. Future Work<a class="headerlink" href="#future-work" title="Permalink to this heading">#</a></h2>
<section id="from-objects-to-scenes">
<h3>From objects to scenes<a class="headerlink" href="#from-objects-to-scenes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>본 접근법은 평범한 배경의 단일 객체 데이터셋으로 훈련</p></li>
<li><p>RTMV 데이터셋에서 여러 객체가 있는 장면에 대한 강한 일반화를 입증했지만, GSO에서의 분포 내 샘플에 비해 품질이 여전히 저하</p></li>
<li><p>복잡한 배경이 있는 장면으로의 일반화는 앞으로의 주요 과제</p></li>
</ul>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="appendix">
<h1>6. Appendix<a class="headerlink" href="#appendix" title="Permalink to this heading">#</a></h1>
<section id="c-finetuning-stable-diffusion">
<h2>C. Finetuning Stable Diffusion<a class="headerlink" href="#c-finetuning-stable-diffusion" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>렌더링된 데이터셋을 사용하여 새로운 뷰 합성을 수행하기 위해 사전 훈련된 스테이블 디퓨전 모델을 미세 조정</p></li>
<li><p>원래의 스테이블 디퓨전 네트워크는 다중 모드 텍스트 임베딩에 대해 조건화되지 않기 때문에, 조건 정보를 이미지에서 사용할 수 있도록 원래의 스테이블 디퓨전 아키텍처를 조정하고 미세 조정해야함.</p></li>
<li><p>이미지 CLIP 임베딩(차원 768)과 포즈 벡터(차원 4)를 연결하고, 디퓨전 모델 아키텍처와의 호환성을 확보하기 위해 또 다른 완전 연결 층(772 → 768)을 초기화</p></li>
<li><p>이 층의 학습률은 다른 층보다 10배 크게 조정</p></li>
<li><p>나머지 네트워크 아키텍처는 원래 스테이블 디퓨전과 동일하게 유지</p></li>
</ul>
</section>
<section id="c-1">
<h2>C.1 훈련 세부사항<a class="headerlink" href="#c-1" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>8×A100-80GB 머신</strong>에서 7일 동안 모델을 미세 조정</p></li>
</ul>
</section>
<section id="c-2">
<h2>C.2 추론 세부사항<a class="headerlink" href="#c-2" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>새로운 뷰를 생성하기 위해 Zero-1-to-3는 RTX A6000 GPU에서 단 2초 소요</p></li>
<li><p>이전 작업에서는 일반적으로 NeRF를 훈련하여 새로운 뷰를 렌더링하는 데 상당한 시간 소요</p></li>
<li><p>비교적으로, 본 접근 방식은 3D 재구성과 새로운 뷰 합성의 순서를 반전시켜 새로운 뷰 합성 과정을 신속하고 불확실성 하에서 다양성을 포함하도록 함</p></li>
</ul>
</section>
<section id="d-3d-reconstruction">
<h2>D. 3D Reconstruction<a class="headerlink" href="#d-3d-reconstruction" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>이미지에서 전체 3D 재구성을 실행하는 데 RTX A6000 GPU에서 약 30분이 소요</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs\review"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="DreamBooth3D.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dream Booth 3D</p>
      </div>
    </a>
    <a class="right-next"
       href="zero123plus.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Zero123++</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">zero 123</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract-zero-shot-one-image-to-3d-object">Abstract : Zero-shot One Image to 3D Object</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">1. Introduction</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#related-work">2. Related Work</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#methods">3. Methods</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-to-control-camera-viewpoint">3.1. Learning to Control Camera Viewpoint</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#view-conditioned-diffusion">3.2. View-Conditioned Diffusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-reconstruction">3.3 3D Reconstruction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">3.4. Dataset</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">4.Experiments</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tasks">4.1. Tasks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines">4.2. Baselines</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarks-and-metrics">4.3. Benchmarks and Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#novel-view-synthesis-results">4.4. Novel View Synthesis Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-reconstruction-results">4.5. 3D Reconstruction Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-to-image-to-3d">4.6. Text to Image to 3D-</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">5. Discussion</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-work">5.1. Future Work</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-objects-to-scenes">From objects to scenes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">6. Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-finetuning-stable-diffusion">C. Finetuning Stable Diffusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-1">C.1 훈련 세부사항</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-2">C.2 추론 세부사항</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-3d-reconstruction">D. 3D Reconstruction</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PseudoLab
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>